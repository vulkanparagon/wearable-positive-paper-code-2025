{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c745d9aa-2a8a-460a-b2e0-136c44dbf00a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 12:59:58.483393: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-28 12:59:58.522146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-28 12:59:58.522170: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-28 12:59:58.522191: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-28 12:59:58.529904: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-28 12:59:59.286295: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Global Max Sequence Length: 74\n",
      "âœ… StandardScaler saved!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Dense, Dropout, LayerNormalization, MultiHeadAttention, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import os\n",
    "\n",
    "# Number of Folds\n",
    "NUM_FOLDS = 5\n",
    "fold_files = [f\"train_2d_fold{i}.pkl\" for i in range(1, 6)]\n",
    "\n",
    "# Directory to Save Models & Scaler\n",
    "os.makedirs(\"2Ddata_Transformer\", exist_ok=True)\n",
    "\n",
    "# Label Columns\n",
    "label_columns = [\"Positive_Emotions\", \"Negative_Emotions\", \"Self_Esteem\", \"Meaning_in_Life\", \"Social_Support\"]\n",
    "\n",
    "# ===== Step 1: Compute Global Max Sequence Length Across All Folds ===== #\n",
    "global_max_seq_length = 0\n",
    "\n",
    "for file_name in fold_files:\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        fold_dict = pickle.load(f)\n",
    "\n",
    "    max_length = max(len(seq) for seq in fold_dict[\"data\"])\n",
    "    global_max_seq_length = max(global_max_seq_length, max_length)\n",
    "\n",
    "print(f\"\\nâœ… Global Max Sequence Length: {global_max_seq_length}\")\n",
    "\n",
    "# ===== Step 2: Standardize Features Using a Single Scaler ===== #\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit scaler on the first fold's training data\n",
    "with open(fold_files[0], \"rb\") as f:\n",
    "    first_fold_dict = pickle.load(f)\n",
    "\n",
    "columns = first_fold_dict[\"columns\"]\n",
    "label_indices = [columns.index(col) for col in label_columns]\n",
    "feature_indices = [i for i in range(len(columns)) if i not in label_indices and columns[i] != \"participant_id\"]\n",
    "\n",
    "X_raw_first_fold = [np.array(seq[:, feature_indices], dtype=np.float32) for seq in first_fold_dict[\"data\"]]\n",
    "X_flattened = np.vstack(X_raw_first_fold)\n",
    "scaler.fit(X_flattened)\n",
    "\n",
    "# Save Scaler\n",
    "with open(\"2Ddata_Transformer/scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"âœ… StandardScaler saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5252c9a-f610-4b77-8f69-3ef33e348cdb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŸ¢ Processing Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 13:00:18.591326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 701 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:3b:00.0, compute capability: 7.5\n",
      "2025-02-28 13:00:18.593821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13664 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:d8:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 13:00:21.854671: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fac691c2310 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-02-28 13:00:21.854710: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-02-28 13:00:21.854721: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n",
      "2025-02-28 13:00:21.900532: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-02-28 13:00:21.979380: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n",
      "2025-02-28 13:00:22.210489: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 6s 38ms/step - loss: 6.9667 - mae: 4.6374 - val_loss: 6.0270 - val_mae: 3.7175\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 6.0414 - mae: 3.7458 - val_loss: 5.4678 - val_mae: 3.1899\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 5.3984 - mae: 3.1334 - val_loss: 5.0042 - val_mae: 2.7560\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 4.7728 - mae: 2.5371 - val_loss: 4.5550 - val_mae: 2.3358\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 4.2978 - mae: 2.0914 - val_loss: 4.1678 - val_mae: 1.9781\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.8976 - mae: 1.7206 - val_loss: 4.1201 - val_mae: 1.9603\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.6184 - mae: 1.4728 - val_loss: 3.9326 - val_mae: 1.8070\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.5605 - mae: 1.4504 - val_loss: 3.9393 - val_mae: 1.8491\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.4105 - mae: 1.3358 - val_loss: 3.8541 - val_mae: 1.7996\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.3294 - mae: 1.2900 - val_loss: 3.7623 - val_mae: 1.7423\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.2731 - mae: 1.2683 - val_loss: 3.7065 - val_mae: 1.7216\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 3.2466 - mae: 1.2766 - val_loss: 3.7489 - val_mae: 1.7985\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.2074 - mae: 1.2722 - val_loss: 3.7173 - val_mae: 1.8018\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.1214 - mae: 1.2207 - val_loss: 3.6118 - val_mae: 1.7303\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.1161 - mae: 1.2490 - val_loss: 3.5549 - val_mae: 1.7071\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.9988 - mae: 1.1654 - val_loss: 3.5091 - val_mae: 1.6950\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.9533 - mae: 1.1536 - val_loss: 3.5170 - val_mae: 1.7356\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.9427 - mae: 1.1748 - val_loss: 3.5067 - val_mae: 1.7573\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.8959 - mae: 1.1604 - val_loss: 3.4079 - val_mae: 1.6903\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.8111 - mae: 1.1072 - val_loss: 3.3971 - val_mae: 1.7107\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.8042 - mae: 1.1309 - val_loss: 3.3832 - val_mae: 1.7272\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.7759 - mae: 1.1335 - val_loss: 3.2291 - val_mae: 1.6038\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.6792 - mae: 1.0667 - val_loss: 3.2909 - val_mae: 1.6950\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.6232 - mae: 1.0402 - val_loss: 3.2350 - val_mae: 1.6688\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.6220 - mae: 1.0684 - val_loss: 3.1788 - val_mae: 1.6412\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.5249 - mae: 0.9993 - val_loss: 3.1822 - val_mae: 1.6725\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.5522 - mae: 1.0548 - val_loss: 3.1379 - val_mae: 1.6563\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.5108 - mae: 1.0410 - val_loss: 3.1277 - val_mae: 1.6731\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.5173 - mae: 1.0736 - val_loss: 3.1645 - val_mae: 1.7354\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 2.4198 - mae: 1.0020 - val_loss: 3.0732 - val_mae: 1.6702\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.3538 - mae: 0.9617 - val_loss: 2.9775 - val_mae: 1.5997\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.3833 - mae: 1.0167 - val_loss: 2.9718 - val_mae: 1.6190\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.3266 - mae: 0.9843 - val_loss: 2.9814 - val_mae: 1.6530\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.2908 - mae: 0.9729 - val_loss: 2.8710 - val_mae: 1.5667\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.2770 - mae: 0.9827 - val_loss: 2.9533 - val_mae: 1.6720\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.2720 - mae: 1.0008 - val_loss: 2.7995 - val_mae: 1.5420\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.2354 - mae: 0.9879 - val_loss: 2.7724 - val_mae: 1.5375\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.1647 - mae: 0.9388 - val_loss: 2.9552 - val_mae: 1.7411\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.1549 - mae: 0.9505 - val_loss: 2.7727 - val_mae: 1.5811\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.1092 - mae: 0.9267 - val_loss: 2.7624 - val_mae: 1.5913\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.0694 - mae: 0.9067 - val_loss: 2.8488 - val_mae: 1.6974\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.0877 - mae: 0.9454 - val_loss: 2.7918 - val_mae: 1.6608\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.0152 - mae: 0.8927 - val_loss: 2.8492 - val_mae: 1.7374\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.0382 - mae: 0.9349 - val_loss: 2.6874 - val_mae: 1.5952\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.0009 - mae: 0.9166 - val_loss: 2.8024 - val_mae: 1.7283\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.9732 - mae: 0.9075 - val_loss: 2.6644 - val_mae: 1.6092\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.9471 - mae: 0.8994 - val_loss: 2.7593 - val_mae: 1.7214\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.9104 - mae: 0.8803 - val_loss: 2.6206 - val_mae: 1.6010\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.9315 - mae: 0.9190 - val_loss: 2.7049 - val_mae: 1.7012\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.9021 - mae: 0.9055 - val_loss: 2.6116 - val_mae: 1.6246\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.8387 - mae: 0.8585 - val_loss: 2.5498 - val_mae: 1.5789\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 1.8231 - mae: 0.8593 - val_loss: 2.5964 - val_mae: 1.6415\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.7974 - mae: 0.8489 - val_loss: 2.6506 - val_mae: 1.7109\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 1.8108 - mae: 0.8774 - val_loss: 2.5576 - val_mae: 1.6330\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.7673 - mae: 0.8491 - val_loss: 2.6398 - val_mae: 1.7295\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.7939 - mae: 0.8897 - val_loss: 2.6086 - val_mae: 1.7132\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.7500 - mae: 0.8608 - val_loss: 2.6096 - val_mae: 1.7280\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 1.7080 - mae: 0.8320 - val_loss: 2.5542 - val_mae: 1.6866\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.7097 - mae: 0.8476 - val_loss: 2.7379 - val_mae: 1.8825\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.6956 - mae: 0.8460 - val_loss: 2.5943 - val_mae: 1.7526\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.6971 - mae: 0.8611 - val_loss: 2.4729 - val_mae: 1.6443\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.6749 - mae: 0.8515 - val_loss: 2.4255 - val_mae: 1.6092\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.6452 - mae: 0.8347 - val_loss: 2.4656 - val_mae: 1.6619\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.6434 - mae: 0.8443 - val_loss: 2.6476 - val_mae: 1.8551\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.5611 - mae: 0.7739 - val_loss: 2.5579 - val_mae: 1.7775\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.5986 - mae: 0.8228 - val_loss: 2.5805 - val_mae: 1.8105\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.5615 - mae: 0.7967 - val_loss: 2.4198 - val_mae: 1.6615\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.5607 - mae: 0.8068 - val_loss: 2.4718 - val_mae: 1.7238\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.5681 - mae: 0.8249 - val_loss: 2.3979 - val_mae: 1.6611\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 1.5517 - mae: 0.8191 - val_loss: 2.4903 - val_mae: 1.7633\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.5175 - mae: 0.7947 - val_loss: 2.5377 - val_mae: 1.8205\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.5036 - mae: 0.7909 - val_loss: 2.4278 - val_mae: 1.7210\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.4982 - mae: 0.7956 - val_loss: 2.4376 - val_mae: 1.7406\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.4766 - mae: 0.7837 - val_loss: 2.3146 - val_mae: 1.6276\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 1.4994 - mae: 0.8159 - val_loss: 2.4434 - val_mae: 1.7645\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.4752 - mae: 0.8003 - val_loss: 2.3613 - val_mae: 1.6919\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.4726 - mae: 0.8070 - val_loss: 2.4222 - val_mae: 1.7615\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.4282 - mae: 0.7711 - val_loss: 2.4208 - val_mae: 1.7684\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.4238 - mae: 0.7747 - val_loss: 2.4254 - val_mae: 1.7810\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.4229 - mae: 0.7824 - val_loss: 2.3671 - val_mae: 1.7315\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3762 - mae: 0.7438 - val_loss: 2.4571 - val_mae: 1.8289\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3783 - mae: 0.7538 - val_loss: 2.3824 - val_mae: 1.7624\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.4035 - mae: 0.7869 - val_loss: 2.3467 - val_mae: 1.7346\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3570 - mae: 0.7481 - val_loss: 2.3255 - val_mae: 1.7208\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3255 - mae: 0.7240 - val_loss: 2.2539 - val_mae: 1.6567\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3471 - mae: 0.7528 - val_loss: 2.3699 - val_mae: 1.7795\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3746 - mae: 0.7873 - val_loss: 2.3997 - val_mae: 1.8166\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.2835 - mae: 0.7041 - val_loss: 2.3088 - val_mae: 1.7337\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.2887 - mae: 0.7158 - val_loss: 2.4744 - val_mae: 1.9046\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3394 - mae: 0.7728 - val_loss: 2.2578 - val_mae: 1.6957\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3384 - mae: 0.7782 - val_loss: 2.4160 - val_mae: 1.8591\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3176 - mae: 0.7641 - val_loss: 2.2583 - val_mae: 1.7085\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.2625 - mae: 0.7154 - val_loss: 2.2806 - val_mae: 1.7370\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.2791 - mae: 0.7384 - val_loss: 2.2641 - val_mae: 1.7268\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.2799 - mae: 0.7448 - val_loss: 2.3647 - val_mae: 1.8327\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.2538 - mae: 0.7245 - val_loss: 2.2784 - val_mae: 1.7526\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.2453 - mae: 0.7220 - val_loss: 2.3390 - val_mae: 1.8187\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.2598 - mae: 0.7422 - val_loss: 2.2507 - val_mae: 1.7370\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.2550 - mae: 0.7436 - val_loss: 2.2999 - val_mae: 1.7912\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.2292 - mae: 0.7233 - val_loss: 2.2420 - val_mae: 1.7393\n",
      "7/7 [==============================] - 0s 5ms/step\n",
      "ðŸ”¹ Fold 1 - Train Loss: 1.2292, Val Loss: 2.2420, Train MAE: 0.7233, Val MAE: 1.7393, Residual Error: -0.5105 Â± 1.2579\n",
      "\n",
      "ðŸŸ¢ Processing Fold 2/5\n",
      "Epoch 1/100\n",
      "11/11 [==============================] - 3s 35ms/step - loss: 6.2255 - mae: 3.9231 - val_loss: 5.6183 - val_mae: 3.3350\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 5.3116 - mae: 3.0412 - val_loss: 4.7738 - val_mae: 2.5199\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 4.5529 - mae: 2.3108 - val_loss: 4.1880 - val_mae: 1.9610\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.9091 - mae: 1.6932 - val_loss: 3.8905 - val_mae: 1.6894\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.6738 - mae: 1.4849 - val_loss: 3.7916 - val_mae: 1.6194\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.5196 - mae: 1.3610 - val_loss: 3.7143 - val_mae: 1.5735\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.4628 - mae: 1.3356 - val_loss: 3.5558 - val_mae: 1.4471\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.4192 - mae: 1.3243 - val_loss: 3.4690 - val_mae: 1.3916\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.2789 - mae: 1.2149 - val_loss: 3.4419 - val_mae: 1.3957\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.2454 - mae: 1.2126 - val_loss: 3.4363 - val_mae: 1.4209\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.1889 - mae: 1.1867 - val_loss: 3.3982 - val_mae: 1.4135\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.1073 - mae: 1.1359 - val_loss: 3.3633 - val_mae: 1.4092\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.1198 - mae: 1.1787 - val_loss: 3.2736 - val_mae: 1.3496\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 3.0477 - mae: 1.1366 - val_loss: 3.2999 - val_mae: 1.4052\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.9749 - mae: 1.0931 - val_loss: 3.2831 - val_mae: 1.4181\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.0055 - mae: 1.1527 - val_loss: 3.2899 - val_mae: 1.4531\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.9106 - mae: 1.0861 - val_loss: 3.1546 - val_mae: 1.3463\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.8081 - mae: 1.0120 - val_loss: 3.1975 - val_mae: 1.4170\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.8371 - mae: 1.0683 - val_loss: 3.1039 - val_mae: 1.3510\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.7742 - mae: 1.0331 - val_loss: 3.1309 - val_mae: 1.4046\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.7241 - mae: 1.0091 - val_loss: 3.2088 - val_mae: 1.5085\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.6993 - mae: 1.0104 - val_loss: 3.1612 - val_mae: 1.4874\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.6495 - mae: 0.9869 - val_loss: 3.1801 - val_mae: 1.5316\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.6640 - mae: 1.0264 - val_loss: 3.1357 - val_mae: 1.5128\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.6153 - mae: 1.0031 - val_loss: 3.1164 - val_mae: 1.5181\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.5499 - mae: 0.9617 - val_loss: 3.0885 - val_mae: 1.5141\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.4712 - mae: 0.9073 - val_loss: 3.0147 - val_mae: 1.4639\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.5088 - mae: 0.9679 - val_loss: 3.0177 - val_mae: 1.4899\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.4977 - mae: 0.9801 - val_loss: 2.9820 - val_mae: 1.4777\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.4208 - mae: 0.9262 - val_loss: 2.9418 - val_mae: 1.4599\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.3920 - mae: 0.9196 - val_loss: 2.9131 - val_mae: 1.4531\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.3574 - mae: 0.9067 - val_loss: 2.8744 - val_mae: 1.4359\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.3635 - mae: 0.9345 - val_loss: 2.8782 - val_mae: 1.4613\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.2783 - mae: 0.8701 - val_loss: 2.9035 - val_mae: 1.5071\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.2698 - mae: 0.8829 - val_loss: 2.8714 - val_mae: 1.4962\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.3073 - mae: 0.9404 - val_loss: 2.9860 - val_mae: 1.6301\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.2441 - mae: 0.8973 - val_loss: 2.8204 - val_mae: 1.4856\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.1881 - mae: 0.8617 - val_loss: 2.8215 - val_mae: 1.5058\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.1743 - mae: 0.8669 - val_loss: 2.7599 - val_mae: 1.4637\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.1769 - mae: 0.8889 - val_loss: 2.7855 - val_mae: 1.5081\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.1254 - mae: 0.8557 - val_loss: 2.8398 - val_mae: 1.5806\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.0871 - mae: 0.8358 - val_loss: 2.7692 - val_mae: 1.5281\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.0921 - mae: 0.8584 - val_loss: 2.7135 - val_mae: 1.4897\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.0663 - mae: 0.8502 - val_loss: 2.7003 - val_mae: 1.4938\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.0582 - mae: 0.8596 - val_loss: 2.6613 - val_mae: 1.4724\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.0086 - mae: 0.8265 - val_loss: 2.7227 - val_mae: 1.5499\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.0354 - mae: 0.8698 - val_loss: 2.6569 - val_mae: 1.5006\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.9906 - mae: 0.8410 - val_loss: 2.6436 - val_mae: 1.5028\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.9492 - mae: 0.8152 - val_loss: 2.6274 - val_mae: 1.5022\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.9535 - mae: 0.8350 - val_loss: 2.5829 - val_mae: 1.4730\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.9182 - mae: 0.8148 - val_loss: 2.6396 - val_mae: 1.5448\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 1.8833 - mae: 0.7953 - val_loss: 2.5670 - val_mae: 1.4879\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.8934 - mae: 0.8205 - val_loss: 2.6003 - val_mae: 1.5351\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.8844 - mae: 0.8254 - val_loss: 2.5924 - val_mae: 1.5414\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.8860 - mae: 0.8410 - val_loss: 2.5194 - val_mae: 1.4822\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.8241 - mae: 0.7926 - val_loss: 2.5381 - val_mae: 1.5140\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.7915 - mae: 0.7727 - val_loss: 2.5549 - val_mae: 1.5435\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.7628 - mae: 0.7572 - val_loss: 2.5234 - val_mae: 1.5249\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.7750 - mae: 0.7819 - val_loss: 2.5260 - val_mae: 1.5405\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.8025 - mae: 0.8227 - val_loss: 2.4936 - val_mae: 1.5207\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.7491 - mae: 0.7817 - val_loss: 2.4342 - val_mae: 1.4741\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.7209 - mae: 0.7662 - val_loss: 2.5187 - val_mae: 1.5701\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.7450 - mae: 0.8012 - val_loss: 2.4540 - val_mae: 1.5175\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.7110 - mae: 0.7796 - val_loss: 2.4341 - val_mae: 1.5089\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.6639 - mae: 0.7430 - val_loss: 2.4186 - val_mae: 1.5040\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.6575 - mae: 0.7479 - val_loss: 2.4225 - val_mae: 1.5192\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.6485 - mae: 0.7498 - val_loss: 2.4252 - val_mae: 1.5332\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.6506 - mae: 0.7635 - val_loss: 2.4059 - val_mae: 1.5243\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.6861 - mae: 0.8086 - val_loss: 2.3420 - val_mae: 1.4704\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.6157 - mae: 0.7490 - val_loss: 2.2793 - val_mae: 1.4187\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.6285 - mae: 0.7726 - val_loss: 2.3579 - val_mae: 1.5077\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.5886 - mae: 0.7425 - val_loss: 2.3203 - val_mae: 1.4800\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.5673 - mae: 0.7312 - val_loss: 2.3390 - val_mae: 1.5081\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.5599 - mae: 0.7328 - val_loss: 2.3311 - val_mae: 1.5095\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.5263 - mae: 0.7087 - val_loss: 2.3271 - val_mae: 1.5146\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.5365 - mae: 0.7279 - val_loss: 2.3171 - val_mae: 1.5139\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.4959 - mae: 0.6963 - val_loss: 2.3049 - val_mae: 1.5098\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.5135 - mae: 0.7226 - val_loss: 2.2313 - val_mae: 1.4454\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.4980 - mae: 0.7157 - val_loss: 2.3080 - val_mae: 1.5304\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.5075 - mae: 0.7341 - val_loss: 2.1793 - val_mae: 1.4109\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.4719 - mae: 0.7068 - val_loss: 2.2220 - val_mae: 1.4616\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.4760 - mae: 0.7194 - val_loss: 2.2923 - val_mae: 1.5400\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.4288 - mae: 0.6804 - val_loss: 2.1798 - val_mae: 1.4368\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.4854 - mae: 0.7454 - val_loss: 2.3156 - val_mae: 1.5793\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.4482 - mae: 0.7152 - val_loss: 2.2412 - val_mae: 1.5136\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.4816 - mae: 0.7577 - val_loss: 2.2768 - val_mae: 1.5563\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.4063 - mae: 0.6889 - val_loss: 2.2233 - val_mae: 1.5106\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3991 - mae: 0.6897 - val_loss: 2.1890 - val_mae: 1.4837\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3725 - mae: 0.6701 - val_loss: 2.2071 - val_mae: 1.5086\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3900 - mae: 0.6944 - val_loss: 2.1613 - val_mae: 1.4701\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3697 - mae: 0.6813 - val_loss: 2.2163 - val_mae: 1.5321\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3661 - mae: 0.6852 - val_loss: 2.1115 - val_mae: 1.4344\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.3688 - mae: 0.6941 - val_loss: 2.2272 - val_mae: 1.5558\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3573 - mae: 0.6895 - val_loss: 2.0800 - val_mae: 1.4166\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.3167 - mae: 0.6557 - val_loss: 2.1692 - val_mae: 1.5117\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3540 - mae: 0.6992 - val_loss: 2.1700 - val_mae: 1.5189\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3381 - mae: 0.6897 - val_loss: 2.1572 - val_mae: 1.5125\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3072 - mae: 0.6656 - val_loss: 2.2065 - val_mae: 1.5682\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3202 - mae: 0.6844 - val_loss: 2.2415 - val_mae: 1.6092\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3049 - mae: 0.6754 - val_loss: 2.2320 - val_mae: 1.6059\n",
      "7/7 [==============================] - 0s 4ms/step\n",
      "ðŸ”¹ Fold 2 - Train Loss: 1.3049, Val Loss: 2.2320, Train MAE: 0.6754, Val MAE: 1.6059, Residual Error: -0.4474 Â± 1.1921\n",
      "\n",
      "ðŸŸ¢ Processing Fold 3/5\n",
      "Epoch 1/100\n",
      "11/11 [==============================] - 3s 35ms/step - loss: 5.9798 - mae: 3.6560 - val_loss: 5.0486 - val_mae: 2.7414\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 4.7996 - mae: 2.5036 - val_loss: 4.3035 - val_mae: 2.0216\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 4.1873 - mae: 1.9160 - val_loss: 4.0081 - val_mae: 1.7511\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 3.9254 - mae: 1.6796 - val_loss: 3.8149 - val_mae: 1.5846\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 3.8160 - mae: 1.5985 - val_loss: 3.6299 - val_mae: 1.4294\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.5700 - mae: 1.3823 - val_loss: 3.6413 - val_mae: 1.4701\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.4757 - mae: 1.3169 - val_loss: 3.6072 - val_mae: 1.4649\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.4840 - mae: 1.3541 - val_loss: 3.4882 - val_mae: 1.3752\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.3602 - mae: 1.2598 - val_loss: 3.4583 - val_mae: 1.3742\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.2993 - mae: 1.2274 - val_loss: 3.4762 - val_mae: 1.4204\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.2426 - mae: 1.1992 - val_loss: 3.4826 - val_mae: 1.4553\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.1891 - mae: 1.1742 - val_loss: 3.3910 - val_mae: 1.3925\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.1797 - mae: 1.1937 - val_loss: 3.3607 - val_mae: 1.3909\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.1307 - mae: 1.1726 - val_loss: 3.3556 - val_mae: 1.4128\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.0410 - mae: 1.1103 - val_loss: 3.2449 - val_mae: 1.3297\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 3.0643 - mae: 1.1600 - val_loss: 3.2935 - val_mae: 1.4040\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.9808 - mae: 1.1029 - val_loss: 3.2669 - val_mae: 1.4038\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.9377 - mae: 1.0859 - val_loss: 3.2467 - val_mae: 1.4095\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.8998 - mae: 1.0736 - val_loss: 3.2785 - val_mae: 1.4667\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.8824 - mae: 1.0821 - val_loss: 3.2009 - val_mae: 1.4149\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.8667 - mae: 1.0913 - val_loss: 3.2293 - val_mae: 1.4681\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.8323 - mae: 1.0818 - val_loss: 3.2146 - val_mae: 1.4778\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.7381 - mae: 1.0119 - val_loss: 3.1423 - val_mae: 1.4303\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.7183 - mae: 1.0164 - val_loss: 3.1796 - val_mae: 1.4909\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.7126 - mae: 1.0345 - val_loss: 3.0764 - val_mae: 1.4122\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.6653 - mae: 1.0115 - val_loss: 3.0569 - val_mae: 1.4165\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.5944 - mae: 0.9633 - val_loss: 3.0817 - val_mae: 1.4631\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.5718 - mae: 0.9631 - val_loss: 2.9848 - val_mae: 1.3882\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.5607 - mae: 0.9732 - val_loss: 2.9388 - val_mae: 1.3639\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.5255 - mae: 0.9598 - val_loss: 2.9333 - val_mae: 1.3795\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.5179 - mae: 0.9729 - val_loss: 2.9962 - val_mae: 1.4627\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.4266 - mae: 0.9018 - val_loss: 2.9157 - val_mae: 1.4030\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.4036 - mae: 0.9000 - val_loss: 2.9500 - val_mae: 1.4577\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.4099 - mae: 0.9261 - val_loss: 2.9375 - val_mae: 1.4655\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.3737 - mae: 0.9108 - val_loss: 2.8665 - val_mae: 1.4153\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.4185 - mae: 0.9754 - val_loss: 2.8798 - val_mae: 1.4476\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.3525 - mae: 0.9286 - val_loss: 2.7969 - val_mae: 1.3839\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.2680 - mae: 0.8631 - val_loss: 2.8188 - val_mae: 1.4243\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.2335 - mae: 0.8470 - val_loss: 2.8277 - val_mae: 1.4515\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.2399 - mae: 0.8713 - val_loss: 2.8159 - val_mae: 1.4576\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.2737 - mae: 0.9233 - val_loss: 2.7229 - val_mae: 1.3829\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 2.2261 - mae: 0.8939 - val_loss: 2.7599 - val_mae: 1.4374\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.2186 - mae: 0.9032 - val_loss: 2.7252 - val_mae: 1.4196\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.1508 - mae: 0.8532 - val_loss: 2.6181 - val_mae: 1.3305\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 2.1031 - mae: 0.8225 - val_loss: 2.6238 - val_mae: 1.3524\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 2.1082 - mae: 0.8437 - val_loss: 2.6264 - val_mae: 1.3707\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.0706 - mae: 0.8218 - val_loss: 2.6462 - val_mae: 1.4064\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.0521 - mae: 0.8195 - val_loss: 2.6557 - val_mae: 1.4318\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.0422 - mae: 0.8245 - val_loss: 2.6639 - val_mae: 1.4553\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.0315 - mae: 0.8296 - val_loss: 2.6664 - val_mae: 1.4729\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.0581 - mae: 0.8709 - val_loss: 2.5872 - val_mae: 1.4089\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.0178 - mae: 0.8457 - val_loss: 2.5962 - val_mae: 1.4321\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.0079 - mae: 0.8501 - val_loss: 2.5709 - val_mae: 1.4214\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.9696 - mae: 0.8262 - val_loss: 2.5290 - val_mae: 1.3939\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.9139 - mae: 0.7847 - val_loss: 2.5536 - val_mae: 1.4319\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.9098 - mae: 0.7938 - val_loss: 2.5210 - val_mae: 1.4130\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.9264 - mae: 0.8243 - val_loss: 2.4942 - val_mae: 1.3998\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.8922 - mae: 0.8033 - val_loss: 2.5372 - val_mae: 1.4560\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.8632 - mae: 0.7879 - val_loss: 2.4780 - val_mae: 1.4104\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.8511 - mae: 0.7888 - val_loss: 2.5041 - val_mae: 1.4486\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.8077 - mae: 0.7574 - val_loss: 2.4830 - val_mae: 1.4402\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.8236 - mae: 0.7861 - val_loss: 2.5472 - val_mae: 1.5159\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.8135 - mae: 0.7875 - val_loss: 2.5087 - val_mae: 1.4900\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.7929 - mae: 0.7796 - val_loss: 2.5256 - val_mae: 1.5183\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.7810 - mae: 0.7789 - val_loss: 2.4383 - val_mae: 1.4433\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.7342 - mae: 0.7439 - val_loss: 2.5057 - val_mae: 1.5208\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.7384 - mae: 0.7589 - val_loss: 2.3675 - val_mae: 1.3952\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.7415 - mae: 0.7735 - val_loss: 2.4774 - val_mae: 1.5149\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.7322 - mae: 0.7741 - val_loss: 2.4272 - val_mae: 1.4757\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.6769 - mae: 0.7307 - val_loss: 2.3751 - val_mae: 1.4349\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.6776 - mae: 0.7421 - val_loss: 2.3658 - val_mae: 1.4366\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.6775 - mae: 0.7530 - val_loss: 2.3150 - val_mae: 1.3965\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.6887 - mae: 0.7742 - val_loss: 2.3209 - val_mae: 1.4115\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.6580 - mae: 0.7528 - val_loss: 2.3120 - val_mae: 1.4124\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.6326 - mae: 0.7371 - val_loss: 2.3246 - val_mae: 1.4343\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.6002 - mae: 0.7143 - val_loss: 2.2551 - val_mae: 1.3746\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.6316 - mae: 0.7552 - val_loss: 2.2416 - val_mae: 1.3708\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.6431 - mae: 0.7761 - val_loss: 2.2682 - val_mae: 1.4060\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.5649 - mae: 0.7068 - val_loss: 2.2646 - val_mae: 1.4120\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.5962 - mae: 0.7476 - val_loss: 2.3042 - val_mae: 1.4607\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.5776 - mae: 0.7378 - val_loss: 2.3191 - val_mae: 1.4848\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.5366 - mae: 0.7061 - val_loss: 2.2914 - val_mae: 1.4656\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.4952 - mae: 0.6729 - val_loss: 2.2786 - val_mae: 1.4613\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.5422 - mae: 0.7287 - val_loss: 2.2394 - val_mae: 1.4310\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.5170 - mae: 0.7120 - val_loss: 2.2997 - val_mae: 1.4987\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.5090 - mae: 0.7114 - val_loss: 2.2556 - val_mae: 1.4628\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 1.5232 - mae: 0.7343 - val_loss: 2.2180 - val_mae: 1.4336\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.4823 - mae: 0.7015 - val_loss: 2.1765 - val_mae: 1.4006\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.4853 - mae: 0.7125 - val_loss: 2.2084 - val_mae: 1.4393\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.4861 - mae: 0.7203 - val_loss: 2.2450 - val_mae: 1.4838\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 1.4844 - mae: 0.7264 - val_loss: 2.2668 - val_mae: 1.5128\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.4352 - mae: 0.6849 - val_loss: 2.1269 - val_mae: 1.3816\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.4354 - mae: 0.6930 - val_loss: 2.2501 - val_mae: 1.5111\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.4149 - mae: 0.6795 - val_loss: 2.1150 - val_mae: 1.3846\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.4342 - mae: 0.7064 - val_loss: 2.2174 - val_mae: 1.4931\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.4380 - mae: 0.7167 - val_loss: 2.1477 - val_mae: 1.4304\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.4166 - mae: 0.7030 - val_loss: 2.0748 - val_mae: 1.3655\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3926 - mae: 0.6860 - val_loss: 2.1083 - val_mae: 1.4054\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.4145 - mae: 0.7143 - val_loss: 2.1324 - val_mae: 1.4361\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.3363 - mae: 0.6428 - val_loss: 2.1381 - val_mae: 1.4485\n",
      "7/7 [==============================] - 0s 5ms/step\n",
      "ðŸ”¹ Fold 3 - Train Loss: 1.3363, Val Loss: 2.1381, Train MAE: 0.6428, Val MAE: 1.4485, Residual Error: -0.3860 Â± 1.0431\n",
      "\n",
      "ðŸŸ¢ Processing Fold 4/5\n",
      "Epoch 1/100\n",
      "11/11 [==============================] - 3s 37ms/step - loss: 5.8545 - mae: 3.5429 - val_loss: 5.0221 - val_mae: 2.7273\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 4.6598 - mae: 2.3762 - val_loss: 4.2796 - val_mae: 2.0102\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 3.9992 - mae: 1.7409 - val_loss: 3.9853 - val_mae: 1.7424\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 3.6716 - mae: 1.4415 - val_loss: 3.7855 - val_mae: 1.5729\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.5242 - mae: 1.3252 - val_loss: 3.6772 - val_mae: 1.4963\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.5032 - mae: 1.3364 - val_loss: 3.6175 - val_mae: 1.4687\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.3725 - mae: 1.2372 - val_loss: 3.5551 - val_mae: 1.4377\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.2534 - mae: 1.1493 - val_loss: 3.4840 - val_mae: 1.3972\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.3039 - mae: 1.2310 - val_loss: 3.3359 - val_mae: 1.2810\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.2047 - mae: 1.1629 - val_loss: 3.4252 - val_mae: 1.4003\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.1737 - mae: 1.1618 - val_loss: 3.3197 - val_mae: 1.3256\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.1080 - mae: 1.1275 - val_loss: 3.2629 - val_mae: 1.2997\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.0774 - mae: 1.1266 - val_loss: 3.3607 - val_mae: 1.4262\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.0126 - mae: 1.0903 - val_loss: 3.3059 - val_mae: 1.4002\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.0016 - mae: 1.1086 - val_loss: 3.2712 - val_mae: 1.3944\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.9660 - mae: 1.1017 - val_loss: 3.1836 - val_mae: 1.3358\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.9233 - mae: 1.0875 - val_loss: 3.2332 - val_mae: 1.4128\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.8187 - mae: 1.0102 - val_loss: 3.2208 - val_mae: 1.4283\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.8564 - mae: 1.0763 - val_loss: 3.0937 - val_mae: 1.3294\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.8604 - mae: 1.1076 - val_loss: 3.1382 - val_mae: 1.4003\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.7748 - mae: 1.0481 - val_loss: 3.1010 - val_mae: 1.3893\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.7122 - mae: 1.0120 - val_loss: 3.0724 - val_mae: 1.3866\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.7117 - mae: 1.0364 - val_loss: 3.1076 - val_mae: 1.4468\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.6680 - mae: 1.0182 - val_loss: 3.0023 - val_mae: 1.3669\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.6118 - mae: 0.9870 - val_loss: 3.0252 - val_mae: 1.4143\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.6026 - mae: 1.0019 - val_loss: 3.1122 - val_mae: 1.5254\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.5549 - mae: 0.9785 - val_loss: 3.0408 - val_mae: 1.4780\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.5074 - mae: 0.9546 - val_loss: 2.9912 - val_mae: 1.4516\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.4975 - mae: 0.9678 - val_loss: 2.9906 - val_mae: 1.4735\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.4535 - mae: 0.9462 - val_loss: 2.8936 - val_mae: 1.3995\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.4782 - mae: 0.9938 - val_loss: 2.8768 - val_mae: 1.4047\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.3605 - mae: 0.8973 - val_loss: 2.9264 - val_mae: 1.4751\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.3704 - mae: 0.9287 - val_loss: 2.8335 - val_mae: 1.4047\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.3441 - mae: 0.9245 - val_loss: 2.7954 - val_mae: 1.3874\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.3192 - mae: 0.9202 - val_loss: 2.7731 - val_mae: 1.3861\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 2.2620 - mae: 0.8835 - val_loss: 2.7878 - val_mae: 1.4204\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.2716 - mae: 0.9130 - val_loss: 2.7651 - val_mae: 1.4184\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.2668 - mae: 0.9289 - val_loss: 2.7040 - val_mae: 1.3774\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.2279 - mae: 0.9096 - val_loss: 2.6757 - val_mae: 1.3678\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.1814 - mae: 0.8814 - val_loss: 2.6889 - val_mae: 1.3993\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.1510 - mae: 0.8695 - val_loss: 2.6976 - val_mae: 1.4269\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.1518 - mae: 0.8892 - val_loss: 2.6530 - val_mae: 1.4004\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.1097 - mae: 0.8641 - val_loss: 2.7162 - val_mae: 1.4800\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.1520 - mae: 0.9234 - val_loss: 2.5979 - val_mae: 1.3799\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.0343 - mae: 0.8239 - val_loss: 2.5600 - val_mae: 1.3592\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.0759 - mae: 0.8822 - val_loss: 2.5452 - val_mae: 1.3612\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.9832 - mae: 0.8065 - val_loss: 2.5357 - val_mae: 1.3683\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.9911 - mae: 0.8305 - val_loss: 2.5539 - val_mae: 1.4022\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.0108 - mae: 0.8664 - val_loss: 2.5054 - val_mae: 1.3702\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.9972 - mae: 0.8690 - val_loss: 2.4930 - val_mae: 1.3737\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.9097 - mae: 0.7968 - val_loss: 2.4484 - val_mae: 1.3441\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.9452 - mae: 0.8474 - val_loss: 2.4361 - val_mae: 1.3464\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.8863 - mae: 0.8027 - val_loss: 2.5070 - val_mae: 1.4310\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.8630 - mae: 0.7932 - val_loss: 2.4498 - val_mae: 1.3885\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.8665 - mae: 0.8117 - val_loss: 2.3952 - val_mae: 1.3486\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.8260 - mae: 0.7853 - val_loss: 2.4032 - val_mae: 1.3698\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.8439 - mae: 0.8163 - val_loss: 2.4183 - val_mae: 1.3988\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.8315 - mae: 0.8181 - val_loss: 2.3917 - val_mae: 1.3857\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.7723 - mae: 0.7719 - val_loss: 2.3968 - val_mae: 1.4036\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.7861 - mae: 0.7986 - val_loss: 2.3608 - val_mae: 1.3808\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.7691 - mae: 0.7944 - val_loss: 2.3552 - val_mae: 1.3881\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.7437 - mae: 0.7822 - val_loss: 2.3291 - val_mae: 1.3746\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.7313 - mae: 0.7818 - val_loss: 2.3539 - val_mae: 1.4111\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.6908 - mae: 0.7534 - val_loss: 2.2896 - val_mae: 1.3589\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.7112 - mae: 0.7851 - val_loss: 2.3495 - val_mae: 1.4301\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.6700 - mae: 0.7561 - val_loss: 2.2454 - val_mae: 1.3383\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.6585 - mae: 0.7560 - val_loss: 2.2788 - val_mae: 1.3822\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.6560 - mae: 0.7642 - val_loss: 2.2598 - val_mae: 1.3745\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.6243 - mae: 0.7437 - val_loss: 2.2805 - val_mae: 1.4059\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.6384 - mae: 0.7687 - val_loss: 2.2400 - val_mae: 1.3763\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.6104 - mae: 0.7511 - val_loss: 2.2470 - val_mae: 1.3933\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.5452 - mae: 0.6964 - val_loss: 2.2026 - val_mae: 1.3601\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.6068 - mae: 0.7682 - val_loss: 2.3290 - val_mae: 1.4956\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.5762 - mae: 0.7476 - val_loss: 2.1883 - val_mae: 1.3657\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.5711 - mae: 0.7522 - val_loss: 2.1860 - val_mae: 1.3726\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.5615 - mae: 0.7522 - val_loss: 2.2184 - val_mae: 1.4142\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.5480 - mae: 0.7480 - val_loss: 2.1997 - val_mae: 1.4058\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.5245 - mae: 0.7348 - val_loss: 2.2034 - val_mae: 1.4187\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.5196 - mae: 0.7387 - val_loss: 2.2148 - val_mae: 1.4388\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.5081 - mae: 0.7362 - val_loss: 2.1492 - val_mae: 1.3826\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.4937 - mae: 0.7306 - val_loss: 2.0906 - val_mae: 1.3324\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.4867 - mae: 0.7321 - val_loss: 2.1459 - val_mae: 1.3954\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 1.4848 - mae: 0.7381 - val_loss: 2.0669 - val_mae: 1.3255\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.4864 - mae: 0.7486 - val_loss: 2.1038 - val_mae: 1.3705\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.4519 - mae: 0.7223 - val_loss: 2.0359 - val_mae: 1.3117\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.4495 - mae: 0.7284 - val_loss: 2.0887 - val_mae: 1.3712\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.4348 - mae: 0.7211 - val_loss: 2.0669 - val_mae: 1.3582\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.4294 - mae: 0.7236 - val_loss: 2.1602 - val_mae: 1.4584\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3859 - mae: 0.6875 - val_loss: 2.0514 - val_mae: 1.3577\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.4418 - mae: 0.7510 - val_loss: 2.0556 - val_mae: 1.3687\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.4175 - mae: 0.7341 - val_loss: 2.0133 - val_mae: 1.3345\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3835 - mae: 0.7074 - val_loss: 2.1462 - val_mae: 1.4737\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3628 - mae: 0.6938 - val_loss: 2.0501 - val_mae: 1.3856\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3823 - mae: 0.7210 - val_loss: 2.1095 - val_mae: 1.4513\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3666 - mae: 0.7120 - val_loss: 2.0234 - val_mae: 1.3729\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3570 - mae: 0.7090 - val_loss: 2.1398 - val_mae: 1.4949\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.3523 - mae: 0.7099 - val_loss: 2.1004 - val_mae: 1.4620\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.3202 - mae: 0.6846 - val_loss: 2.1350 - val_mae: 1.5028\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 1.3198 - mae: 0.6912 - val_loss: 2.0295 - val_mae: 1.4053\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.3110 - mae: 0.6897 - val_loss: 2.0526 - val_mae: 1.4343\n",
      "7/7 [==============================] - 0s 5ms/step\n",
      "ðŸ”¹ Fold 4 - Train Loss: 1.3110, Val Loss: 2.0526, Train MAE: 0.6897, Val MAE: 1.4343, Residual Error: -0.5142 Â± 1.1318\n",
      "\n",
      "ðŸŸ¢ Processing Fold 5/5\n",
      "Epoch 1/100\n",
      "11/11 [==============================] - 3s 37ms/step - loss: 6.2442 - mae: 3.9450 - val_loss: 5.2163 - val_mae: 2.9338\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 5.1090 - mae: 2.8378 - val_loss: 4.5044 - val_mae: 2.2476\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 4.4787 - mae: 2.2327 - val_loss: 4.0273 - val_mae: 1.7959\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 4.0531 - mae: 1.8330 - val_loss: 3.7795 - val_mae: 1.5744\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.7255 - mae: 1.5324 - val_loss: 3.6752 - val_mae: 1.4988\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.5793 - mae: 1.4160 - val_loss: 3.6209 - val_mae: 1.4750\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.4518 - mae: 1.3197 - val_loss: 3.4344 - val_mae: 1.3201\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.3896 - mae: 1.2891 - val_loss: 3.4033 - val_mae: 1.3205\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.3221 - mae: 1.2527 - val_loss: 3.4924 - val_mae: 1.4404\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.2352 - mae: 1.1965 - val_loss: 3.4211 - val_mae: 1.3997\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 3.2272 - mae: 1.2189 - val_loss: 3.2976 - val_mae: 1.3065\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 3.1981 - mae: 1.2200 - val_loss: 3.3032 - val_mae: 1.3417\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 3.1012 - mae: 1.1527 - val_loss: 3.2836 - val_mae: 1.3523\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 3.0502 - mae: 1.1319 - val_loss: 3.2752 - val_mae: 1.3736\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 3.0579 - mae: 1.1693 - val_loss: 3.2026 - val_mae: 1.3307\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.9835 - mae: 1.1241 - val_loss: 3.2600 - val_mae: 1.4165\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.9066 - mae: 1.0754 - val_loss: 3.2048 - val_mae: 1.3899\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.8654 - mae: 1.0630 - val_loss: 3.0722 - val_mae: 1.2860\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.8468 - mae: 1.0725 - val_loss: 3.1423 - val_mae: 1.3832\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.7509 - mae: 1.0039 - val_loss: 3.0729 - val_mae: 1.3417\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.7640 - mae: 1.0447 - val_loss: 2.9871 - val_mae: 1.2832\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.6921 - mae: 0.9996 - val_loss: 2.9284 - val_mae: 1.2512\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.6957 - mae: 1.0296 - val_loss: 3.0489 - val_mae: 1.3971\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.6517 - mae: 1.0109 - val_loss: 2.9612 - val_mae: 1.3353\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.6403 - mae: 1.0254 - val_loss: 3.0160 - val_mae: 1.4149\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.6177 - mae: 1.0274 - val_loss: 2.9419 - val_mae: 1.3661\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.5520 - mae: 0.9867 - val_loss: 2.9380 - val_mae: 1.3861\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.5082 - mae: 0.9667 - val_loss: 2.8331 - val_mae: 1.3056\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.4635 - mae: 0.9464 - val_loss: 2.8729 - val_mae: 1.3688\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.4380 - mae: 0.9439 - val_loss: 2.9002 - val_mae: 1.4196\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.3748 - mae: 0.9046 - val_loss: 2.7771 - val_mae: 1.3199\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.3820 - mae: 0.9338 - val_loss: 2.9352 - val_mae: 1.4988\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.3555 - mae: 0.9291 - val_loss: 2.7885 - val_mae: 1.3752\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.3680 - mae: 0.9637 - val_loss: 2.8034 - val_mae: 1.4114\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.2958 - mae: 0.9133 - val_loss: 2.6803 - val_mae: 1.3106\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.2450 - mae: 0.8842 - val_loss: 2.7388 - val_mae: 1.3895\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.2351 - mae: 0.8945 - val_loss: 2.7279 - val_mae: 1.3987\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.2081 - mae: 0.8871 - val_loss: 2.7274 - val_mae: 1.4177\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.1817 - mae: 0.8805 - val_loss: 2.7587 - val_mae: 1.4680\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.1622 - mae: 0.8795 - val_loss: 2.6616 - val_mae: 1.3900\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.1010 - mae: 0.8376 - val_loss: 2.6357 - val_mae: 1.3826\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.1226 - mae: 0.8778 - val_loss: 2.6763 - val_mae: 1.4421\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 2.1250 - mae: 0.8982 - val_loss: 2.7436 - val_mae: 1.5269\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.0724 - mae: 0.8635 - val_loss: 2.6114 - val_mae: 1.4128\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.0749 - mae: 0.8841 - val_loss: 2.5061 - val_mae: 1.3254\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.0474 - mae: 0.8737 - val_loss: 2.6207 - val_mae: 1.4561\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.9877 - mae: 0.8307 - val_loss: 2.4120 - val_mae: 1.2651\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.9536 - mae: 0.8132 - val_loss: 2.5591 - val_mae: 1.4271\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.9844 - mae: 0.8595 - val_loss: 2.4446 - val_mae: 1.3287\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.9440 - mae: 0.8344 - val_loss: 2.4718 - val_mae: 1.3704\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.9114 - mae: 0.8168 - val_loss: 2.4038 - val_mae: 1.3182\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.9240 - mae: 0.8444 - val_loss: 2.5319 - val_mae: 1.4605\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.8577 - mae: 0.7932 - val_loss: 2.3972 - val_mae: 1.3415\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.8426 - mae: 0.7928 - val_loss: 2.4350 - val_mae: 1.3934\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 1.8225 - mae: 0.7868 - val_loss: 2.4425 - val_mae: 1.4152\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.8131 - mae: 0.7921 - val_loss: 2.4664 - val_mae: 1.4529\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.7910 - mae: 0.7833 - val_loss: 2.3560 - val_mae: 1.3562\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.7564 - mae: 0.7618 - val_loss: 2.4157 - val_mae: 1.4281\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.7586 - mae: 0.7769 - val_loss: 2.2913 - val_mae: 1.3172\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.7690 - mae: 0.8001 - val_loss: 2.4142 - val_mae: 1.4516\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.7616 - mae: 0.8046 - val_loss: 2.3949 - val_mae: 1.4455\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.7448 - mae: 0.8002 - val_loss: 2.4110 - val_mae: 1.4730\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.7507 - mae: 0.8184 - val_loss: 2.2525 - val_mae: 1.3270\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.6683 - mae: 0.7479 - val_loss: 2.3215 - val_mae: 1.4077\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.7034 - mae: 0.7946 - val_loss: 2.2626 - val_mae: 1.3606\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.6760 - mae: 0.7792 - val_loss: 2.2336 - val_mae: 1.3431\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.6376 - mae: 0.7518 - val_loss: 2.2974 - val_mae: 1.4173\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.6130 - mae: 0.7370 - val_loss: 2.3385 - val_mae: 1.4681\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.6053 - mae: 0.7403 - val_loss: 2.1944 - val_mae: 1.3357\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.6205 - mae: 0.7655 - val_loss: 2.3002 - val_mae: 1.4513\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.5857 - mae: 0.7413 - val_loss: 2.2737 - val_mae: 1.4349\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.5714 - mae: 0.7368 - val_loss: 2.2356 - val_mae: 1.4069\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.5701 - mae: 0.7457 - val_loss: 2.2011 - val_mae: 1.3823\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.5671 - mae: 0.7523 - val_loss: 2.2633 - val_mae: 1.4539\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.5256 - mae: 0.7203 - val_loss: 2.1788 - val_mae: 1.3790\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.5190 - mae: 0.7230 - val_loss: 2.3064 - val_mae: 1.5149\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.5304 - mae: 0.7428 - val_loss: 2.1553 - val_mae: 1.3733\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.4964 - mae: 0.7185 - val_loss: 2.1624 - val_mae: 1.3894\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.5283 - mae: 0.7592 - val_loss: 2.2083 - val_mae: 1.4438\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.4638 - mae: 0.7029 - val_loss: 2.2129 - val_mae: 1.4572\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.4519 - mae: 0.6999 - val_loss: 2.1612 - val_mae: 1.4141\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 1.4868 - mae: 0.7437 - val_loss: 2.0928 - val_mae: 1.3541\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.4445 - mae: 0.7093 - val_loss: 2.0744 - val_mae: 1.3444\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.4665 - mae: 0.7396 - val_loss: 2.1708 - val_mae: 1.4480\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.4241 - mae: 0.7047 - val_loss: 2.1419 - val_mae: 1.4267\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.4199 - mae: 0.7078 - val_loss: 2.2284 - val_mae: 1.5204\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.4322 - mae: 0.7273 - val_loss: 2.2196 - val_mae: 1.5191\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.3984 - mae: 0.7016 - val_loss: 2.1085 - val_mae: 1.4163\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.3702 - mae: 0.6810 - val_loss: 2.1322 - val_mae: 1.4472\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.3589 - mae: 0.6776 - val_loss: 2.0468 - val_mae: 1.3700\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3763 - mae: 0.7022 - val_loss: 2.1964 - val_mae: 1.5256\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3896 - mae: 0.7222 - val_loss: 2.0811 - val_mae: 1.4187\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3581 - mae: 0.6985 - val_loss: 2.1908 - val_mae: 1.5348\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3637 - mae: 0.7111 - val_loss: 2.0279 - val_mae: 1.3795\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3331 - mae: 0.6870 - val_loss: 2.1565 - val_mae: 1.5145\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3515 - mae: 0.7127 - val_loss: 2.1036 - val_mae: 1.4683\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3404 - mae: 0.7076 - val_loss: 2.0761 - val_mae: 1.4467\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.2849 - mae: 0.6584 - val_loss: 2.0431 - val_mae: 1.4204\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.2903 - mae: 0.6705 - val_loss: 2.0281 - val_mae: 1.4112\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.3041 - mae: 0.6892 - val_loss: 2.0317 - val_mae: 1.4205\n",
      "7/7 [==============================] - 0s 5ms/step\n",
      "ðŸ”¹ Fold 5 - Train Loss: 1.3041, Val Loss: 2.0317, Train MAE: 0.6892, Val MAE: 1.4205, Residual Error: -0.5353 Â± 1.0758\n",
      "\n",
      "âœ… All Transformer models trained and saved with consistent padding and standardization!\n"
     ]
    }
   ],
   "source": [
    "# ===== Step 3: Train Transformer Model Across Folds ===== #\n",
    "cv_results = {\"train_loss\": [], \"val_loss\": [], \"train_mae\": [], \"val_mae\": [], \"residual_error\": []}\n",
    "\n",
    "for fold, file_name in enumerate(fold_files):\n",
    "    print(f\"\\nðŸŸ¢ Processing Fold {fold+1}/{NUM_FOLDS}\")\n",
    "\n",
    "    # Load fold data\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        fold_dict = pickle.load(f)\n",
    "\n",
    "    # Extract features and labels\n",
    "    X_raw = [np.array(seq[:, feature_indices], dtype=np.float32) for seq in fold_dict[\"data\"]]\n",
    "    y_raw = [np.array(seq[:, label_indices], dtype=np.float32) for seq in fold_dict[\"data\"]]\n",
    "\n",
    "    # Standardize Features using the pre-saved scaler\n",
    "    X_flat = np.vstack(X_raw)\n",
    "    X_scaled = scaler.transform(X_flat)\n",
    "\n",
    "    # Restore sequence structure\n",
    "    X_fixed = []\n",
    "    start = 0\n",
    "    for seq in X_raw:\n",
    "        length = len(seq)\n",
    "        X_fixed.append(X_scaled[start:start+length])\n",
    "        start += length\n",
    "\n",
    "    # Convert lists to NumPy arrays\n",
    "    X_fixed = np.array(X_fixed, dtype=object)\n",
    "\n",
    "    # âœ… Apply the global max sequence length for consistent padding\n",
    "    X_padded = tf.keras.preprocessing.sequence.pad_sequences(X_fixed, maxlen=global_max_seq_length, dtype=\"float32\", padding=\"post\")\n",
    "\n",
    "    # Only take the last timestep of labels\n",
    "    y_final = np.array([seq[-1] for seq in y_raw], dtype=np.float32)\n",
    "\n",
    "    # ===== Define Transformer Model ===== #\n",
    "    def build_transformer_model(input_shape):\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        # Self-Attention Layer\n",
    "        attn_output = MultiHeadAttention(num_heads=4, key_dim=64)(inputs, inputs)\n",
    "        attn_output = Dropout(0.3)(attn_output)\n",
    "        attn_output = LayerNormalization(epsilon=1e-6)(attn_output + inputs)  # Residual Connection\n",
    "\n",
    "        # Feed-Forward Network\n",
    "        ffn = Dense(128, activation=\"relu\", kernel_regularizer=l2(0.01))(attn_output)\n",
    "        ffn = Dropout(0.3)(ffn)\n",
    "        ffn = Dense(64, activation=\"relu\", kernel_regularizer=l2(0.01))(ffn)\n",
    "\n",
    "        # Output Layer (Regression)\n",
    "        outputs = Dense(5, activation=\"linear\")(ffn[:, -1, :])  # Take last timestep output\n",
    "\n",
    "        model = Model(inputs, outputs)\n",
    "        model.compile(optimizer=Adam(learning_rate=0.0003), loss=\"mae\", metrics=[\"mae\"])\n",
    "        return model\n",
    "\n",
    "    model = build_transformer_model((X_padded.shape[1], X_padded.shape[2]))\n",
    "\n",
    "    # Train Model\n",
    "    history = model.fit(X_padded, y_final, epochs=100, batch_size=16, validation_split=0.2, verbose=1)\n",
    "\n",
    "    # Save Model\n",
    "    model.save(f\"2Ddata_Transformer/transformer_model_fold{fold+1}.keras\")\n",
    "\n",
    "    # Store Results\n",
    "    train_loss, train_mae = history.history[\"loss\"][-1], history.history[\"mae\"][-1]\n",
    "    val_loss, val_mae = history.history[\"val_loss\"][-1], history.history[\"val_mae\"][-1]\n",
    "\n",
    "    # Predict on train set for residual analysis\n",
    "    y_pred = model.predict(X_padded)\n",
    "    residuals = y_final - y_pred\n",
    "    mean_residual = np.mean(residuals)\n",
    "    std_residual = np.std(residuals)\n",
    "\n",
    "    # Append results\n",
    "    cv_results[\"train_loss\"].append(train_loss)\n",
    "    cv_results[\"val_loss\"].append(val_loss)\n",
    "    cv_results[\"train_mae\"].append(train_mae)\n",
    "    cv_results[\"val_mae\"].append(val_mae)\n",
    "    cv_results[\"residual_error\"].append(mean_residual)\n",
    "\n",
    "    # Save Metrics\n",
    "    fold_metrics = {\n",
    "        \"loss\": history.history[\"loss\"],\n",
    "        \"val_loss\": history.history[\"val_loss\"],\n",
    "        \"mae\": history.history[\"mae\"],\n",
    "        \"val_mae\": history.history[\"val_mae\"]\n",
    "    }\n",
    "    with open(f\"2Ddata_Transformer/training_metrics_fold{fold+1}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(fold_metrics, f)\n",
    "\n",
    "    # Save Residuals\n",
    "    residual_data = {\"mean_residual\": mean_residual, \"std_residual\": std_residual}\n",
    "    with open(f\"2Ddata_Transformer/residuals_fold{fold+1}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(residual_data, f)\n",
    "\n",
    "    print(f\"ðŸ”¹ Fold {fold+1} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train MAE: {train_mae:.4f}, Val MAE: {val_mae:.4f}, Residual Error: {mean_residual:.4f} Â± {std_residual:.4f}\")\n",
    "\n",
    "# âœ… Final Summary\n",
    "print(\"\\nâœ… All Transformer models trained and saved with consistent padding and standardization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243a8fad-f7ed-4add-9453-6245357c5562",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"First 5 training samples:\\n{X_padded[:5]}\\n\")\n",
    "print(f\"First 5 validation samples:\\n{X_padded[int(0.8 * len(X_padded)):int(0.85 * len(X_padded))]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8b4041f-f6fb-4f1f-bc78-9ac84e521bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Global Max Sequence Length: 74\n",
      "âœ… StandardScaler saved!\n",
      "\n",
      "âœ… Test set preprocessed! Test samples: 52\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Number of Folds\n",
    "NUM_FOLDS = 5\n",
    "fold_files = [f\"train_2d_fold{i}.pkl\" for i in range(1, 6)]\n",
    "test_file = \"test_2d_fold5.pkl\"  # Ensure this is your actual test file name\n",
    "\n",
    "# Directory to Save Models & Scaler\n",
    "os.makedirs(\"2Ddata_Transformer\", exist_ok=True)\n",
    "\n",
    "# Label Columns\n",
    "label_columns = [\"Positive_Emotions\", \"Negative_Emotions\", \"Self_Esteem\", \"Meaning_in_Life\", \"Social_Support\"]\n",
    "\n",
    "# ===== Step 1: Compute Global Max Sequence Length Across All Folds ===== #\n",
    "global_max_seq_length = 0\n",
    "\n",
    "for file_name in fold_files:\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        fold_dict = pickle.load(f)\n",
    "\n",
    "    max_length = max(len(seq) for seq in fold_dict[\"data\"])\n",
    "    global_max_seq_length = max(global_max_seq_length, max_length)\n",
    "\n",
    "print(f\"\\nâœ… Global Max Sequence Length: {global_max_seq_length}\")\n",
    "\n",
    "# ===== Step 2: Standardize Features Using Training Data Only ===== #\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit scaler on all training folds\n",
    "all_train_features = []\n",
    "\n",
    "for file_name in fold_files:\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        fold_dict = pickle.load(f)\n",
    "\n",
    "    columns = fold_dict[\"columns\"]\n",
    "    label_indices = [columns.index(col) for col in label_columns]\n",
    "    feature_indices = [i for i in range(len(columns)) if i not in label_indices and columns[i] != \"participant_id\"]\n",
    "\n",
    "    X_raw = [np.array(seq[:, feature_indices], dtype=np.float32) for seq in fold_dict[\"data\"]]\n",
    "    X_flattened = np.vstack(X_raw)  # Flatten sequences for fitting scaler\n",
    "    all_train_features.append(X_flattened)\n",
    "\n",
    "# Fit scaler on the entire training set\n",
    "scaler.fit(np.vstack(all_train_features))\n",
    "\n",
    "# Save Scaler\n",
    "with open(\"2Ddata_Transformer/scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"âœ… StandardScaler saved!\")\n",
    "\n",
    "# ===== Step 3: Preprocess the Test Set ===== #\n",
    "with open(test_file, \"rb\") as f:\n",
    "    test_dict = pickle.load(f)\n",
    "\n",
    "columns = test_dict[\"columns\"]\n",
    "label_indices = [columns.index(col) for col in label_columns]\n",
    "feature_indices = [i for i in range(len(columns)) if i not in label_indices and columns[i] != \"participant_id\"]\n",
    "\n",
    "X_test_raw = [np.array(seq[:, feature_indices], dtype=np.float32) for seq in test_dict[\"data\"]]\n",
    "y_test_raw = [np.array(seq[:, label_indices], dtype=np.float32) for seq in test_dict[\"data\"]]\n",
    "\n",
    "# Standardize Test Features Using Pre-Saved Scaler\n",
    "X_test_flat = np.vstack(X_test_raw)\n",
    "X_test_scaled = scaler.transform(X_test_flat)\n",
    "\n",
    "# Restore sequence structure\n",
    "X_test_fixed = []\n",
    "start = 0\n",
    "for seq in X_test_raw:\n",
    "    length = len(seq)\n",
    "    X_test_fixed.append(X_test_scaled[start:start+length])\n",
    "    start += length\n",
    "\n",
    "# Apply Padding\n",
    "X_test_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    X_test_fixed, maxlen=global_max_seq_length, dtype=\"float32\", padding=\"post\"\n",
    ")\n",
    "\n",
    "# Extract last timestep labels\n",
    "y_test_final = np.array([seq[-1] for seq in y_test_raw], dtype=np.float32)\n",
    "\n",
    "print(f\"\\nâœ… Test set preprocessed! Test samples: {len(X_test_padded)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a19e8444-820e-4853-9103-762f16d3ab00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 8ms/step - loss: 1.8922 - mae: 1.2810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.8922131061553955, 1.280975580215454]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_padded, y_test_final, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce6bc0a0-f3cd-4952-9c1c-f3a876f2f3bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Total Training Samples: 1052\n"
     ]
    }
   ],
   "source": [
    "train_samples = 0\n",
    "\n",
    "for file_name in fold_files:\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        fold_dict = pickle.load(f)\n",
    "    \n",
    "    train_samples += len(fold_dict[\"data\"])  # Each sequence is one sample\n",
    "\n",
    "print(f\"âœ… Total Training Samples: {train_samples}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8f7450e-3ad3-4176-ac10-d113f9d369e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Test Samples: 52\n"
     ]
    }
   ],
   "source": [
    "with open(\"test_2d_fold5.pkl\", \"rb\") as f:\n",
    "    test_dict = pickle.load(f)\n",
    "\n",
    "test_samples = len(test_dict[\"data\"])\n",
    "print(f\"âœ… Test Samples: {test_samples}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d91f4b64-14aa-4bb9-97fd-fdd1a932257f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Validation Samples per Fold: 42\n"
     ]
    }
   ],
   "source": [
    "val_samples = int(train_samples * 0.2 / NUM_FOLDS)  # Per fold\n",
    "print(f\"âœ… Validation Samples per Fold: {val_samples}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c558196b-a683-4a19-bbe1-300c7cd8ffdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 20:48:52.826750: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-05 20:48:52.864264: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-05 20:48:52.864288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-05 20:48:52.864325: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-05 20:48:52.873111: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-05 20:48:53.659207: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Global Max Sequence Length: 74\n",
      "âœ… StandardScaler saved!\n",
      "\n",
      "âœ… Test set preprocessed! Total test samples: 263\n"
     ]
    }
   ],
   "source": [
    "# wrong?\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Dense, Dropout, LayerNormalization, MultiHeadAttention, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Number of Folds\n",
    "NUM_FOLDS = 5\n",
    "fold_files = [f\"train_2d_fold{i}.pkl\" for i in range(1, 6)]\n",
    "test_files = [f\"test_2d_fold{i}.pkl\" for i in range(1, 6)]\n",
    "\n",
    "# Directory to Save Models & Scaler\n",
    "os.makedirs(\"2Ddata_Transformer\", exist_ok=True)\n",
    "\n",
    "# Label Columns\n",
    "label_columns = [\"Positive_Emotions\", \"Negative_Emotions\", \"Self_Esteem\", \"Meaning_in_Life\", \"Social_Support\"]\n",
    "\n",
    "# ===== Step 1: Compute Global Max Sequence Length Across All Folds ===== #\n",
    "global_max_seq_length = 0\n",
    "for file_name in fold_files:\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        fold_dict = pickle.load(f)\n",
    "    max_length = max(len(seq) for seq in fold_dict[\"data\"])\n",
    "    global_max_seq_length = max(global_max_seq_length, max_length)\n",
    "\n",
    "print(f\"\\nâœ… Global Max Sequence Length: {global_max_seq_length}\")\n",
    "\n",
    "# ===== Step 2: Standardize Features Using All Training Data ===== #\n",
    "scaler = StandardScaler()\n",
    "all_train_features = []\n",
    "\n",
    "for file_name in fold_files:\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        fold_dict = pickle.load(f)\n",
    "    columns = fold_dict[\"columns\"]\n",
    "    label_indices = [columns.index(col) for col in label_columns]\n",
    "    feature_indices = [i for i in range(len(columns)) if i not in label_indices and columns[i] != \"participant_id\"]\n",
    "    X_raw = [np.array(seq[:, feature_indices], dtype=np.float32) for seq in fold_dict[\"data\"]]\n",
    "    X_flattened = np.vstack(X_raw)\n",
    "    all_train_features.append(X_flattened)\n",
    "\n",
    "scaler.fit(np.vstack(all_train_features))\n",
    "\n",
    "# Save Scaler\n",
    "with open(\"2Ddata_Transformer/scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"âœ… StandardScaler saved!\")\n",
    "\n",
    "# ===== Step 3: Preprocess the Test Set ===== #\n",
    "test_samples = 0\n",
    "test_data = []\n",
    "y_test = []\n",
    "\n",
    "for test_file in test_files:\n",
    "    with open(test_file, \"rb\") as f:\n",
    "        test_dict = pickle.load(f)\n",
    "    columns = test_dict[\"columns\"]\n",
    "    label_indices = [columns.index(col) for col in label_columns]\n",
    "    feature_indices = [i for i in range(len(columns)) if i not in label_indices and columns[i] != \"participant_id\"]\n",
    "    X_test_raw = [np.array(seq[:, feature_indices], dtype=np.float32) for seq in test_dict[\"data\"]]\n",
    "    y_test_raw = [np.array(seq[:, label_indices], dtype=np.float32) for seq in test_dict[\"data\"]]\n",
    "    X_test_flat = np.vstack(X_test_raw)\n",
    "    X_test_scaled = scaler.transform(X_test_flat)\n",
    "    X_test_fixed = []\n",
    "    start = 0\n",
    "    for seq in X_test_raw:\n",
    "        length = len(seq)\n",
    "        X_test_fixed.append(X_test_scaled[start:start+length])\n",
    "        start += length\n",
    "    X_test_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        X_test_fixed, maxlen=global_max_seq_length, dtype=\"float32\", padding=\"post\"\n",
    "    )\n",
    "    test_data.append(X_test_padded)\n",
    "    y_test.extend([seq[-1] for seq in y_test_raw])\n",
    "    test_samples += len(X_test_padded)\n",
    "\n",
    "y_test_final = np.array(y_test, dtype=np.float32)\n",
    "print(f\"\\nâœ… Test set preprocessed! Total test samples: {test_samples}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecd3d497-1c2a-4cc7-85b2-6d80380278a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "27/27 [==============================] - 2s 23ms/step - loss: 6.7350 - mae: 4.0375 - val_loss: 6.5011 - val_mae: 3.8203\n",
      "Epoch 2/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 6.2891 - mae: 3.6211 - val_loss: 5.9936 - val_mae: 3.3394\n",
      "Epoch 3/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 5.9733 - mae: 3.3308 - val_loss: 5.5818 - val_mae: 2.9525\n",
      "Epoch 4/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 5.6353 - mae: 3.0172 - val_loss: 5.1996 - val_mae: 2.5941\n",
      "Epoch 5/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 5.3304 - mae: 2.7358 - val_loss: 4.8664 - val_mae: 2.2844\n",
      "Epoch 6/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 5.1555 - mae: 2.5844 - val_loss: 4.6355 - val_mae: 2.0770\n",
      "Epoch 7/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 4.9844 - mae: 2.4370 - val_loss: 4.4630 - val_mae: 1.9278\n",
      "Epoch 8/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 4.7633 - mae: 2.2390 - val_loss: 4.3300 - val_mae: 1.8182\n",
      "Epoch 9/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 4.5772 - mae: 2.0764 - val_loss: 4.2018 - val_mae: 1.7137\n",
      "Epoch 10/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 4.4571 - mae: 1.9808 - val_loss: 4.0828 - val_mae: 1.6201\n",
      "Epoch 11/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 4.3432 - mae: 1.8921 - val_loss: 4.0278 - val_mae: 1.5900\n",
      "Epoch 12/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 4.2625 - mae: 1.8369 - val_loss: 3.9675 - val_mae: 1.5554\n",
      "Epoch 13/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 4.1425 - mae: 1.7424 - val_loss: 3.8796 - val_mae: 1.4934\n",
      "Epoch 14/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 4.0821 - mae: 1.7077 - val_loss: 3.8085 - val_mae: 1.4472\n",
      "Epoch 15/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 4.0958 - mae: 1.7464 - val_loss: 3.7789 - val_mae: 1.4428\n",
      "Epoch 16/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 4.0131 - mae: 1.6882 - val_loss: 3.7610 - val_mae: 1.4492\n",
      "Epoch 17/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 3.9459 - mae: 1.6458 - val_loss: 3.6863 - val_mae: 1.3999\n",
      "Epoch 18/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 3.9344 - mae: 1.6598 - val_loss: 3.6107 - val_mae: 1.3497\n",
      "Epoch 19/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 3.8024 - mae: 1.5534 - val_loss: 3.5321 - val_mae: 1.2965\n",
      "Epoch 20/50\n",
      "27/27 [==============================] - 0s 15ms/step - loss: 3.8737 - mae: 1.6491 - val_loss: 3.5414 - val_mae: 1.3296\n",
      "Epoch 21/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 3.7359 - mae: 1.5359 - val_loss: 3.4947 - val_mae: 1.3080\n",
      "Epoch 22/50\n",
      "27/27 [==============================] - 0s 15ms/step - loss: 3.7716 - mae: 1.5965 - val_loss: 3.4737 - val_mae: 1.3114\n",
      "Epoch 23/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 3.6896 - mae: 1.5383 - val_loss: 3.4735 - val_mae: 1.3347\n",
      "Epoch 24/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 3.7145 - mae: 1.5869 - val_loss: 3.4360 - val_mae: 1.3216\n",
      "Epoch 25/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 3.6510 - mae: 1.5488 - val_loss: 3.3442 - val_mae: 1.2554\n",
      "Epoch 26/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 3.6184 - mae: 1.5409 - val_loss: 3.3289 - val_mae: 1.2637\n",
      "Epoch 27/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 3.6201 - mae: 1.5656 - val_loss: 3.3774 - val_mae: 1.3351\n",
      "Epoch 28/50\n",
      "27/27 [==============================] - 0s 15ms/step - loss: 3.4906 - mae: 1.4599 - val_loss: 3.2903 - val_mae: 1.2726\n",
      "Epoch 29/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 3.5284 - mae: 1.5217 - val_loss: 3.2557 - val_mae: 1.2614\n",
      "Epoch 30/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 3.4861 - mae: 1.5028 - val_loss: 3.2209 - val_mae: 1.2498\n",
      "Epoch 31/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 3.4140 - mae: 1.4535 - val_loss: 3.2196 - val_mae: 1.2712\n",
      "Epoch 32/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 3.3764 - mae: 1.4385 - val_loss: 3.1913 - val_mae: 1.2655\n",
      "Epoch 33/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 3.4163 - mae: 1.5012 - val_loss: 3.1909 - val_mae: 1.2879\n",
      "Epoch 34/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 3.3528 - mae: 1.4601 - val_loss: 3.1704 - val_mae: 1.2901\n",
      "Epoch 35/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 3.3634 - mae: 1.4937 - val_loss: 3.1310 - val_mae: 1.2733\n",
      "Epoch 36/50\n",
      "27/27 [==============================] - 0s 15ms/step - loss: 3.2978 - mae: 1.4505 - val_loss: 3.0987 - val_mae: 1.2631\n",
      "Epoch 37/50\n",
      "27/27 [==============================] - 0s 15ms/step - loss: 3.3512 - mae: 1.5261 - val_loss: 3.0653 - val_mae: 1.2519\n",
      "Epoch 38/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 3.1968 - mae: 1.3939 - val_loss: 3.0533 - val_mae: 1.2617\n",
      "Epoch 39/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 3.2233 - mae: 1.4412 - val_loss: 3.0661 - val_mae: 1.2954\n",
      "Epoch 40/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 3.2063 - mae: 1.4455 - val_loss: 3.0443 - val_mae: 1.2950\n",
      "Epoch 41/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 3.1084 - mae: 1.3693 - val_loss: 2.9983 - val_mae: 1.2704\n",
      "Epoch 42/50\n",
      "27/27 [==============================] - 0s 15ms/step - loss: 3.1272 - mae: 1.4091 - val_loss: 2.9899 - val_mae: 1.2829\n",
      "Epoch 43/50\n",
      "27/27 [==============================] - 0s 15ms/step - loss: 3.0812 - mae: 1.3837 - val_loss: 2.9705 - val_mae: 1.2842\n",
      "Epoch 44/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 3.1186 - mae: 1.4420 - val_loss: 2.9166 - val_mae: 1.2512\n",
      "Epoch 45/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 3.0521 - mae: 1.3965 - val_loss: 2.8810 - val_mae: 1.2365\n",
      "Epoch 46/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 3.0561 - mae: 1.4209 - val_loss: 2.8765 - val_mae: 1.2520\n",
      "Epoch 47/50\n",
      "27/27 [==============================] - 0s 15ms/step - loss: 3.0379 - mae: 1.4225 - val_loss: 2.8815 - val_mae: 1.2765\n",
      "Epoch 48/50\n",
      "27/27 [==============================] - 0s 15ms/step - loss: 3.0030 - mae: 1.4073 - val_loss: 2.8739 - val_mae: 1.2885\n",
      "Epoch 49/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 2.9686 - mae: 1.3919 - val_loss: 2.8870 - val_mae: 1.3207\n",
      "Epoch 50/50\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 2.9265 - mae: 1.3696 - val_loss: 2.8099 - val_mae: 1.2635\n",
      "âœ… Final model saved!\n"
     ]
    }
   ],
   "source": [
    "# ===== Step 4: Define Transformer Model ===== #\n",
    "def build_transformer_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    attn_output = MultiHeadAttention(num_heads=2, key_dim=32)(inputs, inputs)\n",
    "    attn_output = Dropout(0.5)(attn_output)\n",
    "    attn_output = LayerNormalization(epsilon=1e-6)(attn_output + inputs)\n",
    "    ffn = Dense(64, activation=\"relu\", kernel_regularizer=l2(0.02))(attn_output)\n",
    "    ffn = Dropout(0.5)(ffn)\n",
    "    ffn = Dense(32, activation=\"relu\", kernel_regularizer=l2(0.02))(ffn)\n",
    "    outputs = Dense(5, activation=\"linear\")(ffn[:, -1, :])\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss=\"mae\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "# ===== Step 5: Train and Evaluate the Model ===== #\n",
    "model = build_transformer_model((global_max_seq_length, len(feature_indices)))\n",
    "model.fit(\n",
    "    np.vstack(test_data), y_test_final, epochs=50, batch_size=8, validation_split=0.2, verbose=1\n",
    ")\n",
    "\n",
    "# Save the Model\n",
    "model.save(\"2Ddata_Transformer/final_transformer_model.keras\")\n",
    "print(\"âœ… Final model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f9fdace-854f-4452-bbbd-22b3153e4a48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 5ms/step - loss: 2.8670 - mae: 1.2531\n",
      "\n",
      "âœ… Final Test Loss: 2.8670, Test MAE: 1.2531\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on Test Set\n",
    "test_loss, test_mae = model.evaluate(np.vstack(test_data), y_test_final, verbose=1)\n",
    "print(f\"\\nâœ… Final Test Loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7556b208-b8aa-47c2-b841-349178c30e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
