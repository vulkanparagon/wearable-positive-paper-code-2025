{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "676d57bb-c36a-4fe3-888f-7b8be37d8913",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 23:58:59.250758: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-27 23:58:59.298998: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-27 23:58:59.299034: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-27 23:58:59.299072: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-27 23:58:59.309235: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-27 23:59:00.318595: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Global Max Sequence Length: 74\n",
      "âœ… StandardScaler saved!\n",
      "\n",
      "ðŸŸ¢ Processing Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 23:59:03.622649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20854 MB memory:  -> device: 0, name: NVIDIA A10, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2025-02-27 23:59:03.624776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 20854 MB memory:  -> device: 1, name: NVIDIA A10, pci bus id: 0000:ca:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 23:59:08.586454: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n",
      "2025-02-27 23:59:09.467375: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fb65844d430 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-02-27 23:59:09.467399: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A10, Compute Capability 8.6\n",
      "2025-02-27 23:59:09.467406: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA A10, Compute Capability 8.6\n",
      "2025-02-27 23:59:09.476955: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-02-27 23:59:09.614502: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 7s 122ms/step - loss: 5.6391 - mae: 4.5235 - val_loss: 5.4322 - val_mae: 4.3200\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 5.4580 - mae: 4.3482 - val_loss: 5.3059 - val_mae: 4.1994\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 5.2904 - mae: 4.1862 - val_loss: 5.1819 - val_mae: 4.0808\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 5.1870 - mae: 4.0882 - val_loss: 5.0535 - val_mae: 3.9577\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 5.0751 - mae: 3.9817 - val_loss: 4.9291 - val_mae: 3.8386\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 4.9199 - mae: 3.8316 - val_loss: 4.8094 - val_mae: 3.7240\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 4.8241 - mae: 3.7409 - val_loss: 4.6912 - val_mae: 3.6109\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 4.7017 - mae: 3.6235 - val_loss: 4.5711 - val_mae: 3.4958\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 4.4604 - mae: 3.3873 - val_loss: 4.4553 - val_mae: 3.3849\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 4.4922 - mae: 3.4240 - val_loss: 4.3449 - val_mae: 3.2794\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 4.2527 - mae: 3.1893 - val_loss: 4.2378 - val_mae: 3.1772\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 4.1000 - mae: 3.0414 - val_loss: 4.1285 - val_mae: 3.0725\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 4.0435 - mae: 2.9896 - val_loss: 4.0246 - val_mae: 2.9733\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.9434 - mae: 2.8942 - val_loss: 3.9276 - val_mae: 2.8810\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 3.9909 - mae: 2.9462 - val_loss: 3.8412 - val_mae: 2.7992\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 3.8882 - mae: 2.8481 - val_loss: 3.7515 - val_mae: 2.7141\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 3.8885 - mae: 2.8530 - val_loss: 3.6682 - val_mae: 2.6353\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 3.8019 - mae: 2.7709 - val_loss: 3.5931 - val_mae: 2.5647\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 3.5777 - mae: 2.5512 - val_loss: 3.5245 - val_mae: 2.5006\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 3.6141 - mae: 2.5921 - val_loss: 3.4665 - val_mae: 2.4471\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 3.5575 - mae: 2.5401 - val_loss: 3.4136 - val_mae: 2.3987\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 3.4435 - mae: 2.4306 - val_loss: 3.3628 - val_mae: 2.3523\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.4936 - mae: 2.4850 - val_loss: 3.3158 - val_mae: 2.3097\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.5057 - mae: 2.5014 - val_loss: 3.2735 - val_mae: 2.2717\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 3.4519 - mae: 2.4520 - val_loss: 3.2442 - val_mae: 2.2468\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 3.3967 - mae: 2.4012 - val_loss: 3.2128 - val_mae: 2.2197\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.3890 - mae: 2.3978 - val_loss: 3.1783 - val_mae: 2.1895\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 3.3905 - mae: 2.4035 - val_loss: 3.1440 - val_mae: 2.1594\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.3482 - mae: 2.3654 - val_loss: 3.1121 - val_mae: 2.1318\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.3235 - mae: 2.3449 - val_loss: 3.0800 - val_mae: 2.1037\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.2854 - mae: 2.3109 - val_loss: 3.0488 - val_mae: 2.0766\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.2189 - mae: 2.2484 - val_loss: 3.0193 - val_mae: 2.0512\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.2214 - mae: 2.2550 - val_loss: 2.9887 - val_mae: 2.0246\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 3.1880 - mae: 2.2257 - val_loss: 2.9628 - val_mae: 2.0028\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.2539 - mae: 2.2956 - val_loss: 2.9376 - val_mae: 1.9816\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.0723 - mae: 2.1181 - val_loss: 2.9156 - val_mae: 1.9636\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.1154 - mae: 2.1652 - val_loss: 2.8931 - val_mae: 1.9451\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.1219 - mae: 2.1756 - val_loss: 2.8718 - val_mae: 1.9277\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.1184 - mae: 2.1761 - val_loss: 2.8513 - val_mae: 1.9112\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.1561 - mae: 2.2176 - val_loss: 2.8315 - val_mae: 1.8952\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 3.0230 - mae: 2.0884 - val_loss: 2.8095 - val_mae: 1.8769\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.0891 - mae: 2.1582 - val_loss: 2.7873 - val_mae: 1.8585\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.0955 - mae: 2.1683 - val_loss: 2.7682 - val_mae: 1.8431\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.9984 - mae: 2.0749 - val_loss: 2.7499 - val_mae: 1.8284\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.0411 - mae: 2.1212 - val_loss: 2.7330 - val_mae: 1.8151\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 2.9799 - mae: 2.0636 - val_loss: 2.7207 - val_mae: 1.8064\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.8846 - mae: 1.9719 - val_loss: 2.6997 - val_mae: 1.7890\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.8891 - mae: 1.9799 - val_loss: 2.6782 - val_mae: 1.7710\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.9651 - mae: 2.0594 - val_loss: 2.6581 - val_mae: 1.7544\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.9577 - mae: 2.0555 - val_loss: 2.6443 - val_mae: 1.7440\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.0024 - mae: 2.1037 - val_loss: 2.6350 - val_mae: 1.7383\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.9386 - mae: 2.0433 - val_loss: 2.6279 - val_mae: 1.7346\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.9146 - mae: 2.0228 - val_loss: 2.6215 - val_mae: 1.7316\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.8934 - mae: 2.0049 - val_loss: 2.6093 - val_mae: 1.7228\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.8646 - mae: 1.9795 - val_loss: 2.5990 - val_mae: 1.7157\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.8473 - mae: 1.9655 - val_loss: 2.5839 - val_mae: 1.7040\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.7687 - mae: 1.8901 - val_loss: 2.5683 - val_mae: 1.6915\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 2.8474 - mae: 1.9720 - val_loss: 2.5566 - val_mae: 1.6830\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.8873 - mae: 2.0151 - val_loss: 2.5481 - val_mae: 1.6777\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 2.7502 - mae: 1.8812 - val_loss: 2.5398 - val_mae: 1.6726\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.8967 - mae: 2.0309 - val_loss: 2.5375 - val_mae: 1.6735\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.8126 - mae: 1.9500 - val_loss: 2.5305 - val_mae: 1.6696\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.7560 - mae: 1.8964 - val_loss: 2.5222 - val_mae: 1.6645\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.6865 - mae: 1.8301 - val_loss: 2.5181 - val_mae: 1.6634\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 2.8161 - mae: 1.9628 - val_loss: 2.5067 - val_mae: 1.6551\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 2.7772 - mae: 1.9269 - val_loss: 2.4982 - val_mae: 1.6496\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.7644 - mae: 1.9171 - val_loss: 2.4865 - val_mae: 1.6409\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.6680 - mae: 1.8236 - val_loss: 2.4829 - val_mae: 1.6403\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.6805 - mae: 1.8391 - val_loss: 2.4776 - val_mae: 1.6379\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.6890 - mae: 1.8506 - val_loss: 2.4766 - val_mae: 1.6398\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 2.7914 - mae: 1.9559 - val_loss: 2.4687 - val_mae: 1.6349\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.6888 - mae: 1.8563 - val_loss: 2.4616 - val_mae: 1.6307\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.5318 - mae: 1.7021 - val_loss: 2.4616 - val_mae: 1.6336\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 2.7533 - mae: 1.9265 - val_loss: 2.4628 - val_mae: 1.6376\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 2.6576 - mae: 1.8337 - val_loss: 2.4636 - val_mae: 1.6413\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.7345 - mae: 1.9135 - val_loss: 2.4662 - val_mae: 1.6469\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 2.7078 - mae: 1.8896 - val_loss: 2.4595 - val_mae: 1.6429\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 2.7401 - mae: 1.9248 - val_loss: 2.4590 - val_mae: 1.6452\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.6223 - mae: 1.8097 - val_loss: 2.4477 - val_mae: 1.6368\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 2.6172 - mae: 1.8073 - val_loss: 2.4392 - val_mae: 1.6309\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.6118 - mae: 1.8046 - val_loss: 2.4305 - val_mae: 1.6248\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.6499 - mae: 1.8454 - val_loss: 2.4126 - val_mae: 1.6095\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.6226 - mae: 1.8207 - val_loss: 2.4006 - val_mae: 1.6002\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.6364 - mae: 1.8372 - val_loss: 2.3897 - val_mae: 1.5920\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.7329 - mae: 1.9363 - val_loss: 2.3835 - val_mae: 1.5884\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 2.5822 - mae: 1.7882 - val_loss: 2.3821 - val_mae: 1.5896\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.5906 - mae: 1.7992 - val_loss: 2.3809 - val_mae: 1.5910\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.6685 - mae: 1.8797 - val_loss: 2.3772 - val_mae: 1.5898\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.6794 - mae: 1.8931 - val_loss: 2.3738 - val_mae: 1.5890\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.6735 - mae: 1.8898 - val_loss: 2.3678 - val_mae: 1.5855\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 2.5294 - mae: 1.7483 - val_loss: 2.3604 - val_mae: 1.5807\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.5513 - mae: 1.7726 - val_loss: 2.3675 - val_mae: 1.5903\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 2.6323 - mae: 1.8562 - val_loss: 2.3692 - val_mae: 1.5945\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.4958 - mae: 1.7222 - val_loss: 2.3635 - val_mae: 1.5914\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.5629 - mae: 1.7918 - val_loss: 2.3480 - val_mae: 1.5783\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.6775 - mae: 1.9088 - val_loss: 2.3279 - val_mae: 1.5605\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.5912 - mae: 1.8248 - val_loss: 2.3223 - val_mae: 1.5573\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.5769 - mae: 1.8129 - val_loss: 2.3250 - val_mae: 1.5624\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.6198 - mae: 1.8583 - val_loss: 2.3232 - val_mae: 1.5630\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.5009 - mae: 1.7418 - val_loss: 2.3217 - val_mae: 1.5639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 10ms/step\n",
      "ðŸ”¹ Fold 1 - Train Loss: 2.5009, Val Loss: 2.3217, Train MAE: 1.7418, Val MAE: 1.5639, Residual Error: 0.6851 Â± 1.5421\n",
      "\n",
      "ðŸŸ¢ Processing Fold 2/5\n",
      "Epoch 1/100\n",
      "11/11 [==============================] - 6s 150ms/step - loss: 5.4028 - mae: 4.2869 - val_loss: 5.3079 - val_mae: 4.1959\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 5.3397 - mae: 4.2305 - val_loss: 5.2196 - val_mae: 4.1141\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 5.2299 - mae: 4.1272 - val_loss: 5.1321 - val_mae: 4.0329\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 5.1017 - mae: 4.0052 - val_loss: 5.0475 - val_mae: 3.9546\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 5.0761 - mae: 3.9858 - val_loss: 4.9662 - val_mae: 3.8793\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 4.9447 - mae: 3.8604 - val_loss: 4.8884 - val_mae: 3.8075\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 4.8065 - mae: 3.7281 - val_loss: 4.8155 - val_mae: 3.7404\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 4.7815 - mae: 3.7090 - val_loss: 4.7508 - val_mae: 3.6817\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 4.6953 - mae: 3.6287 - val_loss: 4.6858 - val_mae: 3.6225\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 4.5686 - mae: 3.5079 - val_loss: 4.6209 - val_mae: 3.5633\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 4.5302 - mae: 3.4751 - val_loss: 4.5514 - val_mae: 3.4995\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 4.4755 - mae: 3.4260 - val_loss: 4.4828 - val_mae: 3.4365\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 4.4154 - mae: 3.3715 - val_loss: 4.4176 - val_mae: 3.3768\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 4.2991 - mae: 3.2606 - val_loss: 4.3515 - val_mae: 3.3161\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 4.2676 - mae: 3.2345 - val_loss: 4.2845 - val_mae: 3.2544\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 4.3012 - mae: 3.2734 - val_loss: 4.2132 - val_mae: 3.1883\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 4.1085 - mae: 3.0859 - val_loss: 4.1416 - val_mae: 3.1219\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 4.0798 - mae: 3.0623 - val_loss: 4.0759 - val_mae: 3.0613\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 4.0503 - mae: 3.0378 - val_loss: 4.0112 - val_mae: 3.0016\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 4.0475 - mae: 3.0401 - val_loss: 3.9482 - val_mae: 2.9436\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 4.0173 - mae: 3.0148 - val_loss: 3.8911 - val_mae: 2.8914\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.7886 - mae: 2.7911 - val_loss: 3.8327 - val_mae: 2.8379\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.8042 - mae: 2.8115 - val_loss: 3.7687 - val_mae: 2.7787\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.8109 - mae: 2.8229 - val_loss: 3.7081 - val_mae: 2.7228\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.7714 - mae: 2.7881 - val_loss: 3.6466 - val_mae: 2.6660\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.7041 - mae: 2.7255 - val_loss: 3.5909 - val_mae: 2.6149\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.5892 - mae: 2.6153 - val_loss: 3.5312 - val_mae: 2.5598\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 3.6331 - mae: 2.6637 - val_loss: 3.4728 - val_mae: 2.5059\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 3.6121 - mae: 2.6471 - val_loss: 3.4247 - val_mae: 2.4623\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 3.5681 - mae: 2.6076 - val_loss: 3.3731 - val_mae: 2.4150\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.5498 - mae: 2.5936 - val_loss: 3.3269 - val_mae: 2.3731\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.5557 - mae: 2.6038 - val_loss: 3.2807 - val_mae: 2.3313\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.4939 - mae: 2.5462 - val_loss: 3.2357 - val_mae: 2.2905\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 3.4364 - mae: 2.4930 - val_loss: 3.1911 - val_mae: 2.2501\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 3.4323 - mae: 2.4930 - val_loss: 3.1513 - val_mae: 2.2144\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 3.4151 - mae: 2.4800 - val_loss: 3.1112 - val_mae: 2.1784\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.3071 - mae: 2.3761 - val_loss: 3.0759 - val_mae: 2.1473\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.2548 - mae: 2.3279 - val_loss: 3.0415 - val_mae: 2.1168\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 3.3279 - mae: 2.4050 - val_loss: 3.0084 - val_mae: 2.0877\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.2059 - mae: 2.2869 - val_loss: 2.9820 - val_mae: 2.0653\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 3.3203 - mae: 2.4053 - val_loss: 2.9569 - val_mae: 2.0441\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.1967 - mae: 2.2856 - val_loss: 2.9292 - val_mae: 2.0203\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.2957 - mae: 2.3885 - val_loss: 2.9001 - val_mae: 1.9950\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.2538 - mae: 2.3504 - val_loss: 2.8721 - val_mae: 1.9708\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 3.1701 - mae: 2.2704 - val_loss: 2.8501 - val_mae: 1.9525\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.0860 - mae: 2.1900 - val_loss: 2.8284 - val_mae: 1.9346\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.2157 - mae: 2.3236 - val_loss: 2.8059 - val_mae: 1.9159\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.1423 - mae: 2.2538 - val_loss: 2.7862 - val_mae: 1.8999\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.1143 - mae: 2.2294 - val_loss: 2.7652 - val_mae: 1.8824\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.9620 - mae: 2.0808 - val_loss: 2.7361 - val_mae: 1.8568\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.0337 - mae: 2.1559 - val_loss: 2.7090 - val_mae: 1.8332\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 3.0242 - mae: 2.1498 - val_loss: 2.6894 - val_mae: 1.8171\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 3.0128 - mae: 2.1420 - val_loss: 2.6722 - val_mae: 1.8033\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 2.9963 - mae: 2.1289 - val_loss: 2.6542 - val_mae: 1.7887\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.9878 - mae: 2.1239 - val_loss: 2.6357 - val_mae: 1.7737\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.9769 - mae: 2.1164 - val_loss: 2.6218 - val_mae: 1.7630\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 2.9715 - mae: 2.1142 - val_loss: 2.6089 - val_mae: 1.7535\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.0047 - mae: 2.1507 - val_loss: 2.5925 - val_mae: 1.7404\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.9794 - mae: 2.1286 - val_loss: 2.5795 - val_mae: 1.7306\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.0144 - mae: 2.1669 - val_loss: 2.5697 - val_mae: 1.7241\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.9642 - mae: 2.1199 - val_loss: 2.5551 - val_mae: 1.7127\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.9104 - mae: 2.0693 - val_loss: 2.5368 - val_mae: 1.6975\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.9195 - mae: 2.0815 - val_loss: 2.5150 - val_mae: 1.6787\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.8159 - mae: 1.9809 - val_loss: 2.4988 - val_mae: 1.6656\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.9051 - mae: 2.0732 - val_loss: 2.4844 - val_mae: 1.6543\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.9002 - mae: 2.0713 - val_loss: 2.4750 - val_mae: 1.6478\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.7672 - mae: 1.9413 - val_loss: 2.4680 - val_mae: 1.6438\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.9070 - mae: 2.0841 - val_loss: 2.4561 - val_mae: 1.6348\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.8535 - mae: 2.0335 - val_loss: 2.4488 - val_mae: 1.6306\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.7817 - mae: 1.9648 - val_loss: 2.4404 - val_mae: 1.6251\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 2.8347 - mae: 2.0207 - val_loss: 2.4352 - val_mae: 1.6228\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 2.8295 - mae: 2.0183 - val_loss: 2.4224 - val_mae: 1.6128\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.7701 - mae: 1.9617 - val_loss: 2.4076 - val_mae: 1.6008\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 2.7992 - mae: 1.9936 - val_loss: 2.3999 - val_mae: 1.5959\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.7204 - mae: 1.9176 - val_loss: 2.3916 - val_mae: 1.5903\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.7936 - mae: 1.9936 - val_loss: 2.3833 - val_mae: 1.5848\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.6509 - mae: 1.8537 - val_loss: 2.3823 - val_mae: 1.5866\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.7823 - mae: 1.9878 - val_loss: 2.3755 - val_mae: 1.5825\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.7143 - mae: 1.9225 - val_loss: 2.3709 - val_mae: 1.5806\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.7963 - mae: 2.0071 - val_loss: 2.3618 - val_mae: 1.5740\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.7985 - mae: 2.0119 - val_loss: 2.3477 - val_mae: 1.5626\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.7443 - mae: 1.9603 - val_loss: 2.3347 - val_mae: 1.5522\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.7357 - mae: 1.9543 - val_loss: 2.3286 - val_mae: 1.5487\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.6693 - mae: 1.8905 - val_loss: 2.3239 - val_mae: 1.5466\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.6551 - mae: 1.8789 - val_loss: 2.3160 - val_mae: 1.5412\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 2.7433 - mae: 1.9696 - val_loss: 2.3040 - val_mae: 1.5317\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.6906 - mae: 1.9194 - val_loss: 2.2933 - val_mae: 1.5235\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.6775 - mae: 1.9088 - val_loss: 2.2881 - val_mae: 1.5208\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.6950 - mae: 1.9288 - val_loss: 2.2819 - val_mae: 1.5170\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.6371 - mae: 1.8733 - val_loss: 2.2770 - val_mae: 1.5146\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.6314 - mae: 1.8701 - val_loss: 2.2732 - val_mae: 1.5132\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.6453 - mae: 1.8864 - val_loss: 2.2653 - val_mae: 1.5077\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.5915 - mae: 1.8349 - val_loss: 2.2606 - val_mae: 1.5053\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 2.5858 - mae: 1.8316 - val_loss: 2.2563 - val_mae: 1.5033\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.6404 - mae: 1.8884 - val_loss: 2.2482 - val_mae: 1.4976\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.6002 - mae: 1.8506 - val_loss: 2.2381 - val_mae: 1.4898\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.6174 - mae: 1.8702 - val_loss: 2.2374 - val_mae: 1.4915\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.5790 - mae: 1.8342 - val_loss: 2.2376 - val_mae: 1.4942\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.5473 - mae: 1.8049 - val_loss: 2.2405 - val_mae: 1.4994\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.6453 - mae: 1.9051 - val_loss: 2.2334 - val_mae: 1.4946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 10ms/step\n",
      "ðŸ”¹ Fold 2 - Train Loss: 2.6453, Val Loss: 2.2334, Train MAE: 1.9051, Val MAE: 1.4946, Residual Error: 0.7860 Â± 1.4498\n",
      "\n",
      "ðŸŸ¢ Processing Fold 3/5\n",
      "Epoch 1/100\n",
      "11/11 [==============================] - 6s 145ms/step - loss: 5.7075 - mae: 4.5968 - val_loss: 5.6130 - val_mae: 4.5059\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 5.5049 - mae: 4.4004 - val_loss: 5.5313 - val_mae: 4.4303\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 5.4652 - mae: 4.3668 - val_loss: 5.4471 - val_mae: 4.3521\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 5.3097 - mae: 4.2172 - val_loss: 5.3645 - val_mae: 4.2754\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 5.3016 - mae: 4.2150 - val_loss: 5.2806 - val_mae: 4.1973\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 5.0750 - mae: 3.9943 - val_loss: 5.1992 - val_mae: 4.1218\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 5.0751 - mae: 4.0001 - val_loss: 5.1213 - val_mae: 4.0496\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 5.0274 - mae: 3.9582 - val_loss: 5.0466 - val_mae: 3.9806\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 4.8655 - mae: 3.8018 - val_loss: 4.9671 - val_mae: 3.9066\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 4.8069 - mae: 3.7488 - val_loss: 4.8913 - val_mae: 3.8363\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 4.7783 - mae: 3.7257 - val_loss: 4.8202 - val_mae: 3.7706\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 4.6528 - mae: 3.6056 - val_loss: 4.7516 - val_mae: 3.7074\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 4.5854 - mae: 3.5435 - val_loss: 4.6814 - val_mae: 3.6425\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 4.5756 - mae: 3.5389 - val_loss: 4.6120 - val_mae: 3.5783\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 4.4498 - mae: 3.4183 - val_loss: 4.5411 - val_mae: 3.5126\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 4.4295 - mae: 3.4032 - val_loss: 4.4690 - val_mae: 3.4456\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 4.3509 - mae: 3.3297 - val_loss: 4.3996 - val_mae: 3.3813\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 4.3143 - mae: 3.2981 - val_loss: 4.3327 - val_mae: 3.3193\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 4.1590 - mae: 3.1478 - val_loss: 4.2706 - val_mae: 3.2622\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 4.1756 - mae: 3.1693 - val_loss: 4.2052 - val_mae: 3.2016\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 4.0482 - mae: 3.0467 - val_loss: 4.1402 - val_mae: 3.1413\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 4.1043 - mae: 3.1075 - val_loss: 4.0747 - val_mae: 3.0806\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 4.0000 - mae: 3.0079 - val_loss: 4.0164 - val_mae: 3.0269\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.8782 - mae: 2.8907 - val_loss: 3.9572 - val_mae: 2.9723\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 3.8334 - mae: 2.8504 - val_loss: 3.8972 - val_mae: 2.9169\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.8125 - mae: 2.8341 - val_loss: 3.8360 - val_mae: 2.8601\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.6740 - mae: 2.7000 - val_loss: 3.7734 - val_mae: 2.8020\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.7723 - mae: 2.8027 - val_loss: 3.7137 - val_mae: 2.7466\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.7158 - mae: 2.7506 - val_loss: 3.6569 - val_mae: 2.6942\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.6903 - mae: 2.7295 - val_loss: 3.6089 - val_mae: 2.6505\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.4716 - mae: 2.5151 - val_loss: 3.5591 - val_mae: 2.6050\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.5746 - mae: 2.6223 - val_loss: 3.4956 - val_mae: 2.5457\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.5399 - mae: 2.5918 - val_loss: 3.4249 - val_mae: 2.4791\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.4545 - mae: 2.5104 - val_loss: 3.3725 - val_mae: 2.4308\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.5282 - mae: 2.5883 - val_loss: 3.3250 - val_mae: 2.3874\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.4669 - mae: 2.5310 - val_loss: 3.2821 - val_mae: 2.3485\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.4125 - mae: 2.4806 - val_loss: 3.2424 - val_mae: 2.3129\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.3845 - mae: 2.4568 - val_loss: 3.2102 - val_mae: 2.2847\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 3.2900 - mae: 2.3662 - val_loss: 3.1757 - val_mae: 2.2542\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.2978 - mae: 2.3779 - val_loss: 3.1441 - val_mae: 2.2265\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.2516 - mae: 2.3357 - val_loss: 3.1120 - val_mae: 2.1982\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.1875 - mae: 2.2754 - val_loss: 3.0778 - val_mae: 2.1678\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.0856 - mae: 2.1773 - val_loss: 3.0498 - val_mae: 2.1437\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.2143 - mae: 2.3097 - val_loss: 3.0155 - val_mae: 2.1130\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.1908 - mae: 2.2900 - val_loss: 2.9830 - val_mae: 2.0843\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.2172 - mae: 2.3200 - val_loss: 2.9619 - val_mae: 2.0668\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.1367 - mae: 2.2432 - val_loss: 2.9439 - val_mae: 2.0525\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 3.1226 - mae: 2.2328 - val_loss: 2.9198 - val_mae: 2.0321\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.0118 - mae: 2.1255 - val_loss: 2.8837 - val_mae: 1.9994\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.0690 - mae: 2.1862 - val_loss: 2.8632 - val_mae: 1.9825\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.0394 - mae: 2.1602 - val_loss: 2.8365 - val_mae: 1.9592\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.9989 - mae: 2.1232 - val_loss: 2.8122 - val_mae: 1.9384\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.9240 - mae: 2.0518 - val_loss: 2.7872 - val_mae: 1.9168\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.9461 - mae: 2.0773 - val_loss: 2.7611 - val_mae: 1.8942\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 2.9522 - mae: 2.0866 - val_loss: 2.7497 - val_mae: 1.8861\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.9064 - mae: 2.0442 - val_loss: 2.7416 - val_mae: 1.8814\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.8717 - mae: 2.0129 - val_loss: 2.7418 - val_mae: 1.8849\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.9595 - mae: 2.1040 - val_loss: 2.7297 - val_mae: 1.8761\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.9897 - mae: 2.1374 - val_loss: 2.7077 - val_mae: 1.8573\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.9504 - mae: 2.1014 - val_loss: 2.6885 - val_mae: 1.8413\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.8964 - mae: 2.0506 - val_loss: 2.6737 - val_mae: 1.8296\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.9869 - mae: 2.1442 - val_loss: 2.6622 - val_mae: 1.8214\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.9568 - mae: 2.1174 - val_loss: 2.6432 - val_mae: 1.8056\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.8869 - mae: 2.0505 - val_loss: 2.6277 - val_mae: 1.7932\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.8373 - mae: 2.0041 - val_loss: 2.6206 - val_mae: 1.7892\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.9019 - mae: 2.0719 - val_loss: 2.6050 - val_mae: 1.7767\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.7417 - mae: 1.9147 - val_loss: 2.5833 - val_mae: 1.7580\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.8189 - mae: 1.9949 - val_loss: 2.5651 - val_mae: 1.7428\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.7993 - mae: 1.9783 - val_loss: 2.5618 - val_mae: 1.7424\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.7758 - mae: 1.9577 - val_loss: 2.5621 - val_mae: 1.7457\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 2.8862 - mae: 2.0711 - val_loss: 2.5487 - val_mae: 1.7352\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.8207 - mae: 2.0085 - val_loss: 2.5379 - val_mae: 1.7273\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.6618 - mae: 1.8525 - val_loss: 2.5268 - val_mae: 1.7191\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.6759 - mae: 1.8695 - val_loss: 2.5269 - val_mae: 1.7222\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.7609 - mae: 1.9575 - val_loss: 2.5238 - val_mae: 1.7220\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.7715 - mae: 1.9709 - val_loss: 2.5221 - val_mae: 1.7232\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.7787 - mae: 1.9810 - val_loss: 2.5081 - val_mae: 1.7119\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.6999 - mae: 1.9050 - val_loss: 2.5080 - val_mae: 1.7147\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.7325 - mae: 1.9405 - val_loss: 2.5127 - val_mae: 1.7222\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.6951 - mae: 1.9058 - val_loss: 2.5085 - val_mae: 1.7208\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.6156 - mae: 1.8291 - val_loss: 2.5125 - val_mae: 1.7276\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.7378 - mae: 1.9541 - val_loss: 2.5119 - val_mae: 1.7297\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.6324 - mae: 1.8514 - val_loss: 2.5019 - val_mae: 1.7224\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.6571 - mae: 1.8787 - val_loss: 2.4915 - val_mae: 1.7147\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.6931 - mae: 1.9174 - val_loss: 2.4845 - val_mae: 1.7104\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.7094 - mae: 1.9365 - val_loss: 2.4777 - val_mae: 1.7063\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.6218 - mae: 1.8515 - val_loss: 2.4668 - val_mae: 1.6981\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.6243 - mae: 1.8566 - val_loss: 2.4552 - val_mae: 1.6890\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.5974 - mae: 1.8322 - val_loss: 2.4479 - val_mae: 1.6843\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.5935 - mae: 1.8310 - val_loss: 2.4595 - val_mae: 1.6984\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.7148 - mae: 1.9549 - val_loss: 2.4657 - val_mae: 1.7073\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.6123 - mae: 1.8549 - val_loss: 2.4498 - val_mae: 1.6938\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.7810 - mae: 2.0261 - val_loss: 2.4393 - val_mae: 1.6858\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.5588 - mae: 1.8064 - val_loss: 2.4506 - val_mae: 1.6997\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.6547 - mae: 1.9049 - val_loss: 2.4448 - val_mae: 1.6964\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.5806 - mae: 1.8332 - val_loss: 2.4359 - val_mae: 1.6899\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.5249 - mae: 1.7799 - val_loss: 2.4211 - val_mae: 1.6774\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.6020 - mae: 1.8593 - val_loss: 2.4095 - val_mae: 1.6682\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.5700 - mae: 1.8297 - val_loss: 2.3972 - val_mae: 1.6582\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.5958 - mae: 1.8579 - val_loss: 2.4081 - val_mae: 1.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 2s 11ms/step\n",
      "ðŸ”¹ Fold 3 - Train Loss: 2.5958, Val Loss: 2.4081, Train MAE: 1.8579, Val MAE: 1.6716, Residual Error: 0.7706 Â± 1.5586\n",
      "\n",
      "ðŸŸ¢ Processing Fold 4/5\n",
      "Epoch 1/100\n",
      "11/11 [==============================] - 6s 137ms/step - loss: 5.7782 - mae: 4.6625 - val_loss: 5.6852 - val_mae: 4.5737\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 5.6475 - mae: 4.5391 - val_loss: 5.5938 - val_mae: 4.4894\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 5.5499 - mae: 4.4484 - val_loss: 5.5036 - val_mae: 4.4060\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 5.4487 - mae: 4.3541 - val_loss: 5.4194 - val_mae: 4.3286\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 5.3848 - mae: 4.2969 - val_loss: 5.3366 - val_mae: 4.2525\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 5.3015 - mae: 4.2202 - val_loss: 5.2577 - val_mae: 4.1802\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 5.1628 - mae: 4.0882 - val_loss: 5.1830 - val_mae: 4.1121\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 5.1208 - mae: 4.0526 - val_loss: 5.1081 - val_mae: 4.0436\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 5.0030 - mae: 3.9412 - val_loss: 5.0336 - val_mae: 3.9754\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 4.9208 - mae: 3.8653 - val_loss: 4.9543 - val_mae: 3.9023\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 4.8578 - mae: 3.8084 - val_loss: 4.8741 - val_mae: 3.8282\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 4.7057 - mae: 3.6624 - val_loss: 4.7914 - val_mae: 3.7514\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 4.6285 - mae: 3.5910 - val_loss: 4.7095 - val_mae: 3.6754\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 4.5865 - mae: 3.5548 - val_loss: 4.6301 - val_mae: 3.6017\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 4.5522 - mae: 3.5263 - val_loss: 4.5526 - val_mae: 3.5299\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 4.4651 - mae: 3.4449 - val_loss: 4.4753 - val_mae: 3.4584\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 4.4434 - mae: 3.4289 - val_loss: 4.3996 - val_mae: 3.3882\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 4.3064 - mae: 3.2974 - val_loss: 4.3309 - val_mae: 3.3250\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 4.1937 - mae: 3.1901 - val_loss: 4.2610 - val_mae: 3.2604\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 4.2538 - mae: 3.2556 - val_loss: 4.1932 - val_mae: 3.1980\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 4.2559 - mae: 3.2630 - val_loss: 4.1261 - val_mae: 3.1362\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 4.1597 - mae: 3.1720 - val_loss: 4.0656 - val_mae: 3.0809\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 4.1699 - mae: 3.1875 - val_loss: 4.0125 - val_mae: 3.0330\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 4.0080 - mae: 3.0308 - val_loss: 3.9553 - val_mae: 2.9809\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.8976 - mae: 2.9254 - val_loss: 3.8967 - val_mae: 2.9273\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.9178 - mae: 2.9505 - val_loss: 3.8370 - val_mae: 2.8725\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.8578 - mae: 2.8954 - val_loss: 3.7782 - val_mae: 2.8185\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.7774 - mae: 2.8197 - val_loss: 3.7216 - val_mae: 2.7667\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.8146 - mae: 2.8617 - val_loss: 3.6679 - val_mae: 2.7177\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.8801 - mae: 2.9319 - val_loss: 3.6124 - val_mae: 2.6669\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.6832 - mae: 2.7397 - val_loss: 3.5577 - val_mae: 2.6168\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.6225 - mae: 2.6836 - val_loss: 3.5082 - val_mae: 2.5719\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 3.5557 - mae: 2.6213 - val_loss: 3.4609 - val_mae: 2.5290\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.6190 - mae: 2.6891 - val_loss: 3.4185 - val_mae: 2.4910\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.5995 - mae: 2.6740 - val_loss: 3.3816 - val_mae: 2.4586\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.5280 - mae: 2.6069 - val_loss: 3.3428 - val_mae: 2.4242\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.5083 - mae: 2.5914 - val_loss: 3.3008 - val_mae: 2.3864\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.6043 - mae: 2.6917 - val_loss: 3.2693 - val_mae: 2.3591\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.3333 - mae: 2.4250 - val_loss: 3.2329 - val_mae: 2.3269\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.3440 - mae: 2.4399 - val_loss: 3.1877 - val_mae: 2.2858\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.3144 - mae: 2.4142 - val_loss: 3.1400 - val_mae: 2.2421\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.3118 - mae: 2.4157 - val_loss: 3.1082 - val_mae: 2.2144\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.3407 - mae: 2.4486 - val_loss: 3.0821 - val_mae: 2.1923\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.2838 - mae: 2.3958 - val_loss: 3.0619 - val_mae: 2.1762\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.2128 - mae: 2.3288 - val_loss: 3.0312 - val_mae: 2.1494\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.3431 - mae: 2.4631 - val_loss: 3.0077 - val_mae: 2.1299\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.2686 - mae: 2.3925 - val_loss: 2.9860 - val_mae: 2.1121\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 3.2494 - mae: 2.3771 - val_loss: 2.9536 - val_mae: 2.0834\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.2032 - mae: 2.3347 - val_loss: 2.9232 - val_mae: 2.0567\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.1967 - mae: 2.3319 - val_loss: 2.9007 - val_mae: 2.0380\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.1259 - mae: 2.2649 - val_loss: 2.8880 - val_mae: 2.0291\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.2096 - mae: 2.3523 - val_loss: 2.8668 - val_mae: 2.0116\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.2665 - mae: 2.4130 - val_loss: 2.8491 - val_mae: 1.9976\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.0243 - mae: 2.1743 - val_loss: 2.8299 - val_mae: 1.9820\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.0210 - mae: 2.1747 - val_loss: 2.8209 - val_mae: 1.9766\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.0813 - mae: 2.2384 - val_loss: 2.8052 - val_mae: 1.9643\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.1485 - mae: 2.3091 - val_loss: 2.7838 - val_mae: 1.9464\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.0455 - mae: 2.2096 - val_loss: 2.7611 - val_mae: 1.9272\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.0666 - mae: 2.2342 - val_loss: 2.7423 - val_mae: 1.9118\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.2385 - mae: 2.4095 - val_loss: 2.7268 - val_mae: 1.8997\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.1003 - mae: 2.2747 - val_loss: 2.7093 - val_mae: 1.8856\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.9899 - mae: 2.1676 - val_loss: 2.6916 - val_mae: 1.8712\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 2.9159 - mae: 2.0969 - val_loss: 2.6673 - val_mae: 1.8501\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.1386 - mae: 2.3228 - val_loss: 2.6475 - val_mae: 1.8336\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.1030 - mae: 2.2904 - val_loss: 2.6136 - val_mae: 1.8028\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.9687 - mae: 2.1592 - val_loss: 2.5877 - val_mae: 1.7801\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 3.0301 - mae: 2.2239 - val_loss: 2.5860 - val_mae: 1.7816\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.9563 - mae: 2.1534 - val_loss: 2.5932 - val_mae: 1.7921\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.9309 - mae: 2.1312 - val_loss: 2.5791 - val_mae: 1.7812\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.9313 - mae: 2.1348 - val_loss: 2.5647 - val_mae: 1.7699\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.0377 - mae: 2.2443 - val_loss: 2.5552 - val_mae: 1.7636\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.0157 - mae: 2.2254 - val_loss: 2.5390 - val_mae: 1.7504\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 2.9815 - mae: 2.1941 - val_loss: 2.5237 - val_mae: 1.7380\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.9506 - mae: 2.1662 - val_loss: 2.5043 - val_mae: 1.7216\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.0398 - mae: 2.2583 - val_loss: 2.4943 - val_mae: 1.7145\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.8699 - mae: 2.0914 - val_loss: 2.4718 - val_mae: 1.6949\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.9503 - mae: 2.1747 - val_loss: 2.4608 - val_mae: 1.6868\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.8188 - mae: 2.0461 - val_loss: 2.4560 - val_mae: 1.6849\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.9324 - mae: 2.1625 - val_loss: 2.4536 - val_mae: 1.6854\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.9539 - mae: 2.1870 - val_loss: 2.4545 - val_mae: 1.6892\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.7893 - mae: 2.0252 - val_loss: 2.4512 - val_mae: 1.6887\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.8363 - mae: 2.0752 - val_loss: 2.4438 - val_mae: 1.6842\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.7231 - mae: 1.9647 - val_loss: 2.4418 - val_mae: 1.6850\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.8313 - mae: 2.0757 - val_loss: 2.4288 - val_mae: 1.6747\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.7848 - mae: 2.0319 - val_loss: 2.4144 - val_mae: 1.6630\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.8807 - mae: 2.1306 - val_loss: 2.4014 - val_mae: 1.6528\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.8062 - mae: 2.0587 - val_loss: 2.3971 - val_mae: 1.6513\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.7518 - mae: 2.0071 - val_loss: 2.3891 - val_mae: 1.6460\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.7013 - mae: 1.9593 - val_loss: 2.3773 - val_mae: 1.6368\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.8746 - mae: 2.1352 - val_loss: 2.3730 - val_mae: 1.6351\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.7693 - mae: 2.0326 - val_loss: 2.3796 - val_mae: 1.6444\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.6608 - mae: 1.9268 - val_loss: 2.3828 - val_mae: 1.6502\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.7929 - mae: 2.0614 - val_loss: 2.3768 - val_mae: 1.6468\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.7525 - mae: 2.0236 - val_loss: 2.3757 - val_mae: 1.6483\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.8090 - mae: 2.0827 - val_loss: 2.3652 - val_mae: 1.6403\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.7420 - mae: 2.0181 - val_loss: 2.3611 - val_mae: 1.6387\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.6919 - mae: 1.9706 - val_loss: 2.3619 - val_mae: 1.6420\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.8572 - mae: 2.1384 - val_loss: 2.3595 - val_mae: 1.6421\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.6767 - mae: 1.9603 - val_loss: 2.3539 - val_mae: 1.6389\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.7989 - mae: 2.0850 - val_loss: 2.3392 - val_mae: 1.6267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 10ms/step\n",
      "ðŸ”¹ Fold 4 - Train Loss: 2.7989, Val Loss: 2.3392, Train MAE: 2.0850, Val MAE: 1.6267, Residual Error: 0.9325 Â± 1.5759\n",
      "\n",
      "ðŸŸ¢ Processing Fold 5/5\n",
      "Epoch 1/100\n",
      "11/11 [==============================] - 6s 141ms/step - loss: 5.2106 - mae: 4.0889 - val_loss: 5.1760 - val_mae: 4.0578\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 5.2574 - mae: 4.1417 - val_loss: 5.0543 - val_mae: 3.9418\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 4.9818 - mae: 3.8717 - val_loss: 4.9285 - val_mae: 3.8215\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 4.8727 - mae: 3.7680 - val_loss: 4.8057 - val_mae: 3.7041\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 4.7964 - mae: 3.6971 - val_loss: 4.6864 - val_mae: 3.5901\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 4.5634 - mae: 3.4694 - val_loss: 4.5604 - val_mae: 3.4694\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 4.5943 - mae: 3.5055 - val_loss: 4.4418 - val_mae: 3.3559\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 4.4265 - mae: 3.3428 - val_loss: 4.3244 - val_mae: 3.2436\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 4.2706 - mae: 3.1919 - val_loss: 4.2146 - val_mae: 3.1388\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 4.2217 - mae: 3.1480 - val_loss: 4.0954 - val_mae: 3.0246\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 4.1001 - mae: 3.0313 - val_loss: 3.9849 - val_mae: 2.9189\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 4.0855 - mae: 3.0216 - val_loss: 3.8861 - val_mae: 2.8249\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.8140 - mae: 2.7549 - val_loss: 3.7955 - val_mae: 2.7392\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.9161 - mae: 2.8618 - val_loss: 3.7117 - val_mae: 2.6601\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 3.8716 - mae: 2.8221 - val_loss: 3.6268 - val_mae: 2.5799\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.6621 - mae: 2.6172 - val_loss: 3.5482 - val_mae: 2.5060\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.7520 - mae: 2.7118 - val_loss: 3.4800 - val_mae: 2.4424\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.5650 - mae: 2.5294 - val_loss: 3.4172 - val_mae: 2.3842\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.4716 - mae: 2.4405 - val_loss: 3.3633 - val_mae: 2.3349\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.6292 - mae: 2.6027 - val_loss: 3.3124 - val_mae: 2.2885\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.4364 - mae: 2.4145 - val_loss: 3.2638 - val_mae: 2.2444\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.3866 - mae: 2.3691 - val_loss: 3.2154 - val_mae: 2.2004\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 3.3719 - mae: 2.3588 - val_loss: 3.1680 - val_mae: 2.1574\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 3.3168 - mae: 2.3081 - val_loss: 3.1288 - val_mae: 2.1226\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.4332 - mae: 2.4289 - val_loss: 3.0833 - val_mae: 2.0814\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.3084 - mae: 2.3084 - val_loss: 3.0412 - val_mae: 2.0436\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.1549 - mae: 2.1591 - val_loss: 3.0049 - val_mae: 2.0115\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 3.1584 - mae: 2.1669 - val_loss: 2.9724 - val_mae: 1.9833\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.2835 - mae: 2.2961 - val_loss: 2.9445 - val_mae: 1.9596\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.2032 - mae: 2.2202 - val_loss: 2.9201 - val_mae: 1.9394\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.0839 - mae: 2.1050 - val_loss: 2.8957 - val_mae: 1.9191\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.0836 - mae: 2.1088 - val_loss: 2.8643 - val_mae: 1.8918\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.0256 - mae: 2.0548 - val_loss: 2.8331 - val_mae: 1.8646\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.1559 - mae: 2.1892 - val_loss: 2.8046 - val_mae: 1.8401\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 3.0275 - mae: 2.0648 - val_loss: 2.7777 - val_mae: 1.8172\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.9864 - mae: 2.0276 - val_loss: 2.7591 - val_mae: 1.8026\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 3.0680 - mae: 2.1132 - val_loss: 2.7440 - val_mae: 1.7914\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.0061 - mae: 2.0552 - val_loss: 2.7310 - val_mae: 1.7823\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.9935 - mae: 2.0465 - val_loss: 2.7172 - val_mae: 1.7724\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.0445 - mae: 2.1013 - val_loss: 2.7032 - val_mae: 1.7622\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.8977 - mae: 1.9584 - val_loss: 2.6897 - val_mae: 1.7525\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 3.0094 - mae: 2.0739 - val_loss: 2.6725 - val_mae: 1.7391\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.9604 - mae: 2.0286 - val_loss: 2.6518 - val_mae: 1.7221\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.8419 - mae: 1.9138 - val_loss: 2.6378 - val_mae: 1.7118\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.8934 - mae: 1.9690 - val_loss: 2.6182 - val_mae: 1.6959\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 2.8597 - mae: 1.9389 - val_loss: 2.6000 - val_mae: 1.6813\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.9120 - mae: 1.9949 - val_loss: 2.5886 - val_mae: 1.6735\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.8167 - mae: 1.9031 - val_loss: 2.5814 - val_mae: 1.6699\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.8424 - mae: 1.9325 - val_loss: 2.5666 - val_mae: 1.6587\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.9532 - mae: 2.0468 - val_loss: 2.5545 - val_mae: 1.6501\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.9057 - mae: 2.0028 - val_loss: 2.5402 - val_mae: 1.6392\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.7875 - mae: 1.8879 - val_loss: 2.5261 - val_mae: 1.6285\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.8437 - mae: 1.9475 - val_loss: 2.5209 - val_mae: 1.6267\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.7821 - mae: 1.8894 - val_loss: 2.5146 - val_mae: 1.6238\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 2.8403 - mae: 1.9510 - val_loss: 2.5049 - val_mae: 1.6175\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.8757 - mae: 1.9897 - val_loss: 2.4953 - val_mae: 1.6111\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.8243 - mae: 1.9416 - val_loss: 2.4851 - val_mae: 1.6042\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.8271 - mae: 1.9477 - val_loss: 2.4775 - val_mae: 1.5999\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.8083 - mae: 1.9322 - val_loss: 2.4725 - val_mae: 1.5981\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.7558 - mae: 1.8829 - val_loss: 2.4639 - val_mae: 1.5928\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.6952 - mae: 1.8255 - val_loss: 2.4587 - val_mae: 1.5908\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.8336 - mae: 1.9671 - val_loss: 2.4491 - val_mae: 1.5844\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.7390 - mae: 1.8756 - val_loss: 2.4295 - val_mae: 1.5679\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.7908 - mae: 1.9305 - val_loss: 2.4173 - val_mae: 1.5588\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.7826 - mae: 1.9254 - val_loss: 2.4087 - val_mae: 1.5533\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.6908 - mae: 1.8367 - val_loss: 2.3954 - val_mae: 1.5430\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.6924 - mae: 1.8414 - val_loss: 2.3841 - val_mae: 1.5347\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.6625 - mae: 1.8145 - val_loss: 2.3705 - val_mae: 1.5241\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.6620 - mae: 1.8169 - val_loss: 2.3569 - val_mae: 1.5135\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.7308 - mae: 1.8887 - val_loss: 2.3448 - val_mae: 1.5043\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.5774 - mae: 1.7381 - val_loss: 2.3360 - val_mae: 1.4984\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 2.6139 - mae: 1.7776 - val_loss: 2.3306 - val_mae: 1.4960\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 2.6328 - mae: 1.7994 - val_loss: 2.3345 - val_mae: 1.5028\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 2.5754 - mae: 1.7450 - val_loss: 2.3265 - val_mae: 1.4978\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 2.6819 - mae: 1.8544 - val_loss: 2.3116 - val_mae: 1.4856\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.6223 - mae: 1.7975 - val_loss: 2.3001 - val_mae: 1.4768\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.5599 - mae: 1.7378 - val_loss: 2.2895 - val_mae: 1.4690\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.5904 - mae: 1.7710 - val_loss: 2.2778 - val_mae: 1.4600\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.5693 - mae: 1.7527 - val_loss: 2.2692 - val_mae: 1.4541\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.5605 - mae: 1.7466 - val_loss: 2.2635 - val_mae: 1.4512\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.6197 - mae: 1.8085 - val_loss: 2.2610 - val_mae: 1.4514\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.6404 - mae: 1.8320 - val_loss: 2.2528 - val_mae: 1.4459\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.5835 - mae: 1.7778 - val_loss: 2.2437 - val_mae: 1.4395\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.4988 - mae: 1.6957 - val_loss: 2.2411 - val_mae: 1.4395\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.4910 - mae: 1.6906 - val_loss: 2.2384 - val_mae: 1.4394\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.5409 - mae: 1.7432 - val_loss: 2.2363 - val_mae: 1.4400\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.6136 - mae: 1.8185 - val_loss: 2.2302 - val_mae: 1.4366\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.5383 - mae: 1.7458 - val_loss: 2.2297 - val_mae: 1.4387\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.6132 - mae: 1.8234 - val_loss: 2.2325 - val_mae: 1.4441\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 2.5416 - mae: 1.7544 - val_loss: 2.2350 - val_mae: 1.4492\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.5042 - mae: 1.7195 - val_loss: 2.2360 - val_mae: 1.4527\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.5438 - mae: 1.7617 - val_loss: 2.2421 - val_mae: 1.4613\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.5363 - mae: 1.7566 - val_loss: 2.2263 - val_mae: 1.4479\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.4951 - mae: 1.7178 - val_loss: 2.2171 - val_mae: 1.4412\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 2.5752 - mae: 1.8004 - val_loss: 2.2066 - val_mae: 1.4331\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.4890 - mae: 1.7165 - val_loss: 2.1995 - val_mae: 1.4284\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.5722 - mae: 1.8022 - val_loss: 2.1972 - val_mae: 1.4286\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 2.4390 - mae: 1.6714 - val_loss: 2.1929 - val_mae: 1.4266\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 2.4418 - mae: 1.6766 - val_loss: 2.1897 - val_mae: 1.4258\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 2.4880 - mae: 1.7251 - val_loss: 2.1845 - val_mae: 1.4230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 10ms/step\n",
      "ðŸ”¹ Fold 5 - Train Loss: 2.4880, Val Loss: 2.1845, Train MAE: 1.7251, Val MAE: 1.4230, Residual Error: 0.6926 Â± 1.4481\n",
      "\n",
      "âœ… All LSTM models trained and saved with consistent padding and standardization!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import os\n",
    "\n",
    "# Define label column names\n",
    "LABEL_COLUMNS = [\"Positive_Emotions\", \"Negative_Emotions\", \"Self_Esteem\", \"Meaning_in_Life\", \"Social_Support\"]\n",
    "\n",
    "# Number of Folds\n",
    "NUM_FOLDS = 5\n",
    "fold_files = [f\"train_2d_fold{i}.pkl\" for i in range(1, NUM_FOLDS + 1)]\n",
    "\n",
    "# Create directory for saving models\n",
    "os.makedirs(\"2Ddata_LSTM\", exist_ok=True)\n",
    "\n",
    "# ===== Step 1: Compute Global Max Sequence Length Across All Folds ===== #\n",
    "global_max_seq_length = 0\n",
    "\n",
    "for file_name in fold_files:\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        fold_dict = pickle.load(f)\n",
    "    max_length = max(len(seq) for seq in fold_dict[\"data\"])\n",
    "    global_max_seq_length = max(global_max_seq_length, max_length)\n",
    "\n",
    "print(f\"\\nâœ… Global Max Sequence Length: {global_max_seq_length}\")\n",
    "\n",
    "# ===== Step 2: Standardize Features Using a Single Scaler ===== #\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit scaler on the first fold's training data\n",
    "with open(fold_files[0], \"rb\") as f:\n",
    "    first_fold_dict = pickle.load(f)\n",
    "\n",
    "columns = first_fold_dict[\"columns\"]\n",
    "label_indices = [columns.index(col) for col in LABEL_COLUMNS]\n",
    "feature_indices = [i for i in range(len(columns)) if i not in label_indices and columns[i] != \"participant_id\"]\n",
    "\n",
    "X_raw_first_fold = [np.array(seq[:, feature_indices], dtype=np.float32) for seq in first_fold_dict[\"data\"]]\n",
    "X_flattened = np.vstack(X_raw_first_fold)\n",
    "scaler.fit(X_flattened)\n",
    "\n",
    "# Save Scaler\n",
    "with open(\"2Ddata_LSTM/scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"âœ… StandardScaler saved!\")\n",
    "\n",
    "# ===== Step 3: Train LSTM Model Across Folds ===== #\n",
    "cv_results = {\"train_loss\": [], \"val_loss\": [], \"train_mae\": [], \"val_mae\": [], \"residual_error\": []}\n",
    "\n",
    "def build_model(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Masking(mask_value=0.0, input_shape=input_shape),\n",
    "        tf.keras.layers.GRU(64, return_sequences=False, kernel_regularizer=l2(0.005)),  # Faster than LSTM\n",
    "        tf.keras.layers.LayerNormalization(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(32, activation=\"relu\", kernel_regularizer=l2(0.005)),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Dense(len(LABEL_COLUMNS), activation=\"linear\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss=\"mae\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "for fold, file_name in enumerate(fold_files):\n",
    "    print(f\"\\nðŸŸ¢ Processing Fold {fold+1}/{NUM_FOLDS}\")\n",
    "\n",
    "    # Load fold data\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        fold_dict = pickle.load(f)\n",
    "\n",
    "    # Extract features and labels\n",
    "    X_raw = [np.array(seq[:, feature_indices], dtype=np.float32) for seq in fold_dict[\"data\"]]\n",
    "    y_raw = [np.array(seq[:, label_indices], dtype=np.float32) for seq in fold_dict[\"data\"]]\n",
    "\n",
    "    # Standardize Features using the pre-saved scaler\n",
    "    X_flat = np.vstack(X_raw)\n",
    "    X_scaled = scaler.transform(X_flat)\n",
    "\n",
    "    # Restore sequence structure\n",
    "    X_fixed = []\n",
    "    start = 0\n",
    "    for seq in X_raw:\n",
    "        length = len(seq)\n",
    "        X_fixed.append(X_scaled[start:start+length])\n",
    "        start += length\n",
    "\n",
    "    # Convert lists to NumPy arrays\n",
    "    X_fixed = np.array(X_fixed, dtype=object)\n",
    "\n",
    "    # âœ… Apply the global max sequence length for consistent padding\n",
    "    X_padded = tf.keras.preprocessing.sequence.pad_sequences(X_fixed, maxlen=global_max_seq_length, dtype=\"float32\", padding=\"post\")\n",
    "\n",
    "    # Use only the last timestep of labels\n",
    "    y_final = np.array([seq[-1] for seq in y_raw], dtype=np.float32)\n",
    "\n",
    "    # Build model\n",
    "    model = build_model(input_shape=(X_padded.shape[1], X_padded.shape[2]))\n",
    "\n",
    "    # Train model\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True)\n",
    "    history = model.fit(X_padded, y_final, epochs=100, batch_size=16, validation_split=0.2, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "    # Save Model\n",
    "    model.save(f\"2Ddata_LSTM/lstm_model_fold{fold+1}.h5\")\n",
    "\n",
    "    # Store Results\n",
    "    train_loss, train_mae = history.history[\"loss\"][-1], history.history[\"mae\"][-1]\n",
    "    val_loss, val_mae = history.history[\"val_loss\"][-1], history.history[\"val_mae\"][-1]\n",
    "\n",
    "    # Predict on train set for residual analysis\n",
    "    y_pred = np.clip(model.predict(X_padded), 0, 10)  # Clip predictions to [0, 10]\n",
    "    residuals = y_final - y_pred\n",
    "    mean_residual = np.mean(residuals)\n",
    "    std_residual = np.std(residuals)\n",
    "\n",
    "    # Append results\n",
    "    cv_results[\"train_loss\"].append(train_loss)\n",
    "    cv_results[\"val_loss\"].append(val_loss)\n",
    "    cv_results[\"train_mae\"].append(train_mae)\n",
    "    cv_results[\"val_mae\"].append(val_mae)\n",
    "    cv_results[\"residual_error\"].append(mean_residual)\n",
    "\n",
    "    # Save Metrics\n",
    "    fold_metrics = {\n",
    "        \"loss\": history.history[\"loss\"],\n",
    "        \"val_loss\": history.history[\"val_loss\"],\n",
    "        \"mae\": history.history[\"mae\"],\n",
    "        \"val_mae\": history.history[\"val_mae\"]\n",
    "    }\n",
    "    with open(f\"2Ddata_LSTM/training_metrics_fold{fold+1}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(fold_metrics, f)\n",
    "\n",
    "    # Save Residuals\n",
    "    residual_data = {\"mean_residual\": mean_residual, \"std_residual\": std_residual}\n",
    "    with open(f\"2Ddata_LSTM/residuals_fold{fold+1}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(residual_data, f)\n",
    "\n",
    "    print(f\"ðŸ”¹ Fold {fold+1} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train MAE: {train_mae:.4f}, Val MAE: {val_mae:.4f}, Residual Error: {mean_residual:.4f} Â± {std_residual:.4f}\")\n",
    "\n",
    "# âœ… Final Summary\n",
    "print(\"\\nâœ… All LSTM models trained and saved with consistent padding and standardization!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9424548-c364-4f3b-b960-9be0cc6d9530",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š **Overall Cross-Validation Results**\n",
      "Train Loss: 2.6058 Â± 0.1130\n",
      "Validation Loss: 2.2974 Â± 0.0793\n",
      "Train MAE: 1.8630 Â± 0.1302\n",
      "Validation MAE: 1.5560 Â± 0.0893\n",
      "Residual Error: 0.7734 Â± 0.0892\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Cross-validation results aggregated and saved!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Number of folds\n",
    "NUM_FOLDS = 5\n",
    "\n",
    "# Store results\n",
    "all_train_losses = []\n",
    "all_val_losses = []\n",
    "all_train_maes = []\n",
    "all_val_maes = []\n",
    "all_residuals = []\n",
    "\n",
    "# Load results from all folds\n",
    "for fold in range(1, NUM_FOLDS + 1):\n",
    "    # Load training metrics\n",
    "    with open(f\"2Ddata_LSTM/training_metrics_fold{fold}.pkl\", \"rb\") as f:\n",
    "        metrics = pickle.load(f)\n",
    "\n",
    "    # Load residuals\n",
    "    with open(f\"2Ddata_LSTM/residuals_fold{fold}.pkl\", \"rb\") as f:\n",
    "        residuals = pickle.load(f)\n",
    "\n",
    "    # Store results\n",
    "    all_train_losses.append(metrics[\"loss\"][-1])  # Last epoch loss\n",
    "    all_val_losses.append(metrics[\"val_loss\"][-1])  # Last epoch validation loss\n",
    "    all_train_maes.append(metrics[\"mae\"][-1])  # Last epoch MAE\n",
    "    all_val_maes.append(metrics[\"val_mae\"][-1])  # Last epoch validation MAE\n",
    "    all_residuals.append(residuals[\"mean_residual\"])  # Mean residual per fold\n",
    "\n",
    "# Compute mean and standard deviation\n",
    "def mean_std(arr):\n",
    "    return np.mean(arr), np.std(arr)\n",
    "\n",
    "train_loss_mean, train_loss_std = mean_std(all_train_losses)\n",
    "val_loss_mean, val_loss_std = mean_std(all_val_losses)\n",
    "train_mae_mean, train_mae_std = mean_std(all_train_maes)\n",
    "val_mae_mean, val_mae_std = mean_std(all_val_maes)\n",
    "residual_mean, residual_std = mean_std(all_residuals)\n",
    "\n",
    "# Print overall performance with standard deviation\n",
    "print(\"\\nðŸ“Š **Overall Cross-Validation Results**\")\n",
    "print(f\"Train Loss: {train_loss_mean:.4f} Â± {train_loss_std:.4f}\")\n",
    "print(f\"Validation Loss: {val_loss_mean:.4f} Â± {val_loss_std:.4f}\")\n",
    "print(f\"Train MAE: {train_mae_mean:.4f} Â± {train_mae_std:.4f}\")\n",
    "print(f\"Validation MAE: {val_mae_mean:.4f} Â± {val_mae_std:.4f}\")\n",
    "print(f\"Residual Error: {residual_mean:.4f} Â± {residual_std:.4f}\")\n",
    "\n",
    "# Plot Loss Trends\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.errorbar(range(1, NUM_FOLDS + 1), all_train_losses, yerr=train_loss_std, fmt='o', label=\"Train Loss\")\n",
    "plt.errorbar(range(1, NUM_FOLDS + 1), all_val_losses, yerr=val_loss_std, fmt='s', label=\"Validation Loss\")\n",
    "plt.xlabel(\"Fold\")\n",
    "plt.ylabel(\"Loss (MAE)\")\n",
    "plt.legend()\n",
    "plt.title(\"Cross-Validation Loss per Fold for LSTM\")\n",
    "plt.savefig(\"2Ddata_LSTM/cross_validation_loss.png\")\n",
    "plt.show()\n",
    "\n",
    "# Save Aggregated Results\n",
    "aggregated_results = {\n",
    "    \"train_loss_mean\": train_loss_mean,\n",
    "    \"train_loss_std\": train_loss_std,\n",
    "    \"val_loss_mean\": val_loss_mean,\n",
    "    \"val_loss_std\": val_loss_std,\n",
    "    \"train_mae_mean\": train_mae_mean,\n",
    "    \"train_mae_std\": train_mae_std,\n",
    "    \"val_mae_mean\": val_mae_mean,\n",
    "    \"val_mae_std\": val_mae_std,\n",
    "    \"residual_mean\": residual_mean,\n",
    "    \"residual_std\": residual_std,\n",
    "    \"all_train_losses\": all_train_losses,\n",
    "    \"all_val_losses\": all_val_losses,\n",
    "    \"all_train_maes\": all_train_maes,\n",
    "    \"all_val_maes\": all_val_maes,\n",
    "    \"all_residuals\": all_residuals\n",
    "}\n",
    "\n",
    "with open(\"2Ddata_LSTM/aggregated_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(aggregated_results, f)\n",
    "\n",
    "print(\"\\nâœ… Cross-validation results aggregated and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e697558a-59f6-45da-bfff-74f4745a4729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 13:27:53.972960: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-28 13:27:54.012111: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-28 13:27:54.012131: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-28 13:27:54.012156: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-28 13:27:54.020005: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-28 13:27:54.892627: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Global Max Sequence Length: 74\n",
      "âœ… StandardScaler saved!\n",
      "\n",
      "âœ… Test set preprocessed! Total test samples: 263\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Define label column names\n",
    "LABEL_COLUMNS = [\"Positive_Emotions\", \"Negative_Emotions\", \"Self_Esteem\", \"Meaning_in_Life\", \"Social_Support\"]\n",
    "\n",
    "# Number of Folds\n",
    "NUM_FOLDS = 5\n",
    "fold_files = [f\"train_2d_fold{i}.pkl\" for i in range(1, NUM_FOLDS + 1)]\n",
    "test_files = [f\"test_2d_fold{i}.pkl\" for i in range(1, NUM_FOLDS + 1)]\n",
    "\n",
    "# Create directory for saving models\n",
    "os.makedirs(\"2Ddata_LSTM\", exist_ok=True)\n",
    "\n",
    "# ===== Step 1: Compute Global Max Sequence Length Across All Folds ===== #\n",
    "global_max_seq_length = 0\n",
    "for file_name in fold_files:\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        fold_dict = pickle.load(f)\n",
    "    max_length = max(len(seq) for seq in fold_dict[\"data\"])\n",
    "    global_max_seq_length = max(global_max_seq_length, max_length)\n",
    "\n",
    "print(f\"\\nâœ… Global Max Sequence Length: {global_max_seq_length}\")\n",
    "\n",
    "# ===== Step 2: Standardize Features Using All Training Data ===== #\n",
    "scaler = StandardScaler()\n",
    "all_train_features = []\n",
    "\n",
    "for file_name in fold_files:\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        fold_dict = pickle.load(f)\n",
    "    columns = fold_dict[\"columns\"]\n",
    "    label_indices = [columns.index(col) for col in LABEL_COLUMNS]\n",
    "    feature_indices = [i for i in range(len(columns)) if i not in label_indices and columns[i] != \"participant_id\"]\n",
    "    X_raw = [np.array(seq[:, feature_indices], dtype=np.float32) for seq in fold_dict[\"data\"]]\n",
    "    X_flattened = np.vstack(X_raw)\n",
    "    all_train_features.append(X_flattened)\n",
    "\n",
    "scaler.fit(np.vstack(all_train_features))\n",
    "\n",
    "# Save Scaler\n",
    "with open(\"2Ddata_LSTM/scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"âœ… StandardScaler saved!\")\n",
    "\n",
    "# ===== Step 3: Preprocess the Test Set ===== #\n",
    "test_samples = 0\n",
    "test_data = []\n",
    "y_test = []\n",
    "\n",
    "for test_file in test_files:\n",
    "    with open(test_file, \"rb\") as f:\n",
    "        test_dict = pickle.load(f)\n",
    "    columns = test_dict[\"columns\"]\n",
    "    label_indices = [columns.index(col) for col in LABEL_COLUMNS]\n",
    "    feature_indices = [i for i in range(len(columns)) if i not in label_indices and columns[i] != \"participant_id\"]\n",
    "    X_test_raw = [np.array(seq[:, feature_indices], dtype=np.float32) for seq in test_dict[\"data\"]]\n",
    "    y_test_raw = [np.array(seq[:, label_indices], dtype=np.float32) for seq in test_dict[\"data\"]]\n",
    "    X_test_flat = np.vstack(X_test_raw)\n",
    "    X_test_scaled = scaler.transform(X_test_flat)\n",
    "    X_test_fixed = []\n",
    "    start = 0\n",
    "    for seq in X_test_raw:\n",
    "        length = len(seq)\n",
    "        X_test_fixed.append(X_test_scaled[start:start+length])\n",
    "        start += length\n",
    "    X_test_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        X_test_fixed, maxlen=global_max_seq_length, dtype=\"float32\", padding=\"post\"\n",
    "    )\n",
    "    test_data.append(X_test_padded)\n",
    "    y_test.extend([seq[-1] for seq in y_test_raw])\n",
    "    test_samples += len(X_test_padded)\n",
    "\n",
    "y_test_final = np.array(y_test, dtype=np.float32)\n",
    "print(f\"\\nâœ… Test set preprocessed! Total test samples: {test_samples}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52c45df6-9636-457d-a30d-4ccd551a38c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 13:28:00.356159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 530 MB memory:  -> device: 0, name: NVIDIA A10, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2025-02-28 13:28:00.358415: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 20011 MB memory:  -> device: 1, name: NVIDIA A10, pci bus id: 0000:ca:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 13:28:05.081690: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n",
      "2025-02-28 13:28:06.003180: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f5914ebd220 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-02-28 13:28:06.003208: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A10, Compute Capability 8.6\n",
      "2025-02-28 13:28:06.003214: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA A10, Compute Capability 8.6\n",
      "2025-02-28 13:28:06.009245: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-02-28 13:28:06.155736: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 8s 82ms/step - loss: 4.9099 - mae: 3.7881 - val_loss: 5.1597 - val_mae: 4.0446\n",
      "Epoch 2/50\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 4.6766 - mae: 3.5673 - val_loss: 4.9482 - val_mae: 3.8453\n",
      "Epoch 3/50\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 4.5085 - mae: 3.4109 - val_loss: 4.7680 - val_mae: 3.6763\n",
      "Epoch 4/50\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 4.4434 - mae: 3.3569 - val_loss: 4.6004 - val_mae: 3.5197\n",
      "Epoch 5/50\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 4.2690 - mae: 3.1931 - val_loss: 4.4321 - val_mae: 3.3617\n",
      "Epoch 6/50\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 4.1230 - mae: 3.0575 - val_loss: 4.2655 - val_mae: 3.2053\n",
      "Epoch 7/50\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 3.9394 - mae: 2.8838 - val_loss: 4.1191 - val_mae: 3.0686\n",
      "Epoch 8/50\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 3.8134 - mae: 2.7675 - val_loss: 3.9813 - val_mae: 2.9404\n",
      "Epoch 9/50\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 3.6765 - mae: 2.6400 - val_loss: 3.8429 - val_mae: 2.8114\n",
      "Epoch 10/50\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 3.5881 - mae: 2.5608 - val_loss: 3.7049 - val_mae: 2.6824\n",
      "Epoch 11/50\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 3.5051 - mae: 2.4867 - val_loss: 3.5827 - val_mae: 2.5691\n",
      "Epoch 12/50\n",
      "27/27 [==============================] - 1s 43ms/step - loss: 3.4320 - mae: 2.4224 - val_loss: 3.4590 - val_mae: 2.4541\n",
      "Epoch 13/50\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 3.3554 - mae: 2.3544 - val_loss: 3.3487 - val_mae: 2.3522\n",
      "Epoch 14/50\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 3.3632 - mae: 2.3707 - val_loss: 3.2615 - val_mae: 2.2734\n",
      "Epoch 15/50\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 3.1147 - mae: 2.1304 - val_loss: 3.1754 - val_mae: 2.1954\n",
      "Epoch 16/50\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 3.2027 - mae: 2.2265 - val_loss: 3.1038 - val_mae: 2.1319\n",
      "Epoch 17/50\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 3.1741 - mae: 2.2058 - val_loss: 3.0232 - val_mae: 2.0591\n",
      "Epoch 18/50\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 3.1100 - mae: 2.1495 - val_loss: 2.9658 - val_mae: 2.0094\n",
      "Epoch 19/50\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 3.1208 - mae: 2.1679 - val_loss: 2.9208 - val_mae: 1.9720\n",
      "Epoch 20/50\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 3.0041 - mae: 2.0587 - val_loss: 2.8926 - val_mae: 1.9512\n",
      "Epoch 21/50\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 2.9902 - mae: 2.0520 - val_loss: 2.8427 - val_mae: 1.9083\n",
      "Epoch 22/50\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 2.9825 - mae: 2.0513 - val_loss: 2.7954 - val_mae: 1.8680\n",
      "Epoch 23/50\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 3.0017 - mae: 2.0776 - val_loss: 2.7486 - val_mae: 1.8282\n",
      "Epoch 24/50\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 2.9634 - mae: 2.0461 - val_loss: 2.7210 - val_mae: 1.8074\n",
      "Epoch 25/50\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 2.8346 - mae: 1.9243 - val_loss: 2.7214 - val_mae: 1.8146\n",
      "Epoch 26/50\n",
      "27/27 [==============================] - 1s 31ms/step - loss: 2.8463 - mae: 1.9425 - val_loss: 2.6887 - val_mae: 1.7884\n",
      "Epoch 27/50\n",
      "27/27 [==============================] - 1s 43ms/step - loss: 2.8893 - mae: 1.9921 - val_loss: 2.6692 - val_mae: 1.7754\n",
      "Epoch 28/50\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 2.7980 - mae: 1.9071 - val_loss: 2.6679 - val_mae: 1.7805\n",
      "Epoch 29/50\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 2.8418 - mae: 1.9572 - val_loss: 2.6363 - val_mae: 1.7551\n",
      "Epoch 30/50\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 2.6860 - mae: 1.8075 - val_loss: 2.5947 - val_mae: 1.7193\n",
      "Epoch 31/50\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 2.7868 - mae: 1.9142 - val_loss: 2.5625 - val_mae: 1.6929\n",
      "Epoch 32/50\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 2.7228 - mae: 1.8561 - val_loss: 2.5610 - val_mae: 1.6974\n",
      "Epoch 33/50\n",
      "27/27 [==============================] - 1s 43ms/step - loss: 2.7120 - mae: 1.8511 - val_loss: 2.5443 - val_mae: 1.6864\n",
      "Epoch 34/50\n",
      "27/27 [==============================] - 1s 43ms/step - loss: 2.7762 - mae: 1.9209 - val_loss: 2.5341 - val_mae: 1.6819\n",
      "Epoch 35/50\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 2.6473 - mae: 1.7976 - val_loss: 2.5113 - val_mae: 1.6646\n",
      "Epoch 36/50\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 2.6259 - mae: 1.7819 - val_loss: 2.5209 - val_mae: 1.6797\n",
      "Epoch 37/50\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 2.6912 - mae: 1.8524 - val_loss: 2.4705 - val_mae: 1.6344\n",
      "Epoch 38/50\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 2.5868 - mae: 1.7532 - val_loss: 2.4594 - val_mae: 1.6287\n",
      "Epoch 39/50\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 2.6195 - mae: 1.7911 - val_loss: 2.4397 - val_mae: 1.6140\n",
      "Epoch 40/50\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 2.5351 - mae: 1.7118 - val_loss: 2.4067 - val_mae: 1.5861\n",
      "Epoch 41/50\n",
      "27/27 [==============================] - 1s 43ms/step - loss: 2.5740 - mae: 1.7558 - val_loss: 2.3990 - val_mae: 1.5835\n",
      "Epoch 42/50\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 2.5164 - mae: 1.7033 - val_loss: 2.3828 - val_mae: 1.5722\n",
      "Epoch 43/50\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 2.5243 - mae: 1.7161 - val_loss: 2.4047 - val_mae: 1.5992\n",
      "Epoch 44/50\n",
      "27/27 [==============================] - 1s 43ms/step - loss: 2.5170 - mae: 1.7137 - val_loss: 2.3848 - val_mae: 1.5841\n",
      "Epoch 45/50\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 2.5333 - mae: 1.7348 - val_loss: 2.3669 - val_mae: 1.5709\n",
      "Epoch 46/50\n",
      "27/27 [==============================] - 1s 44ms/step - loss: 2.5308 - mae: 1.7370 - val_loss: 2.3609 - val_mae: 1.5696\n",
      "Epoch 47/50\n",
      "27/27 [==============================] - 1s 43ms/step - loss: 2.4390 - mae: 1.6498 - val_loss: 2.3346 - val_mae: 1.5479\n",
      "Epoch 48/50\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 2.5292 - mae: 1.7446 - val_loss: 2.3355 - val_mae: 1.5532\n",
      "Epoch 49/50\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 2.4779 - mae: 1.6978 - val_loss: 2.3476 - val_mae: 1.5699\n",
      "Epoch 50/50\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 2.5103 - mae: 1.7346 - val_loss: 2.3522 - val_mae: 1.5788\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 2.0178 - mae: 1.2445\n",
      "\n",
      "âœ… Final Test Loss: 2.0178, Test MAE: 1.2445\n",
      "âœ… Final LSTM model saved!\n"
     ]
    }
   ],
   "source": [
    "# ===== Step 4: Define LSTM Model ===== #\n",
    "def build_model(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Masking(mask_value=0.0, input_shape=input_shape),\n",
    "        tf.keras.layers.GRU(64, return_sequences=False, kernel_regularizer=l2(0.005)),\n",
    "        tf.keras.layers.LayerNormalization(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(32, activation=\"relu\", kernel_regularizer=l2(0.005)),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Dense(len(LABEL_COLUMNS), activation=\"linear\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss=\"mae\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "# ===== Step 5: Train and Evaluate the Model ===== #\n",
    "model = build_model((global_max_seq_length, len(feature_indices)))\n",
    "model.fit(\n",
    "    np.vstack(test_data), y_test_final, epochs=50, batch_size=8, validation_split=0.2, verbose=1,\n",
    "    callbacks=[EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "# Evaluate on Test Set\n",
    "test_loss, test_mae = model.evaluate(np.vstack(test_data), y_test_final, verbose=1)\n",
    "print(f\"\\nâœ… Final Test Loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}\")\n",
    "\n",
    "# Save the Model\n",
    "model.save(\"2Ddata_LSTM/final_lstm_model.keras\")\n",
    "print(\"âœ… Final LSTM model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31fd783-eaca-494e-8a51-812663235646",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
