{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8a21c3-bdf9-47ea-8df4-b2b4fa980028",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "# -----------------\n",
    "# 1. Load Model and Data\n",
    "# -----------------\n",
    "# Updated file paths: \"scaled\" instead of \"pca\"\n",
    "model_path = \"folds_transformed_models/fold_4/scaled/MIL4 (sliderNeutralPos)/rf/rf_MIL4 (sliderNeutralPos).pkl\"\n",
    "data_path = \"folds_transformed/fold_4_train_scaled.csv\"\n",
    "\n",
    "# Load the pre-trained random forest classifier\n",
    "with open(model_path, \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Load the CSV data\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# -----------------\n",
    "# 2. Infer Target and Select Features\n",
    "# -----------------\n",
    "# Infer the target column from the model file name.\n",
    "model_file_name = os.path.basename(model_path)  # e.g., \"rf_MIL3 (sliderNeutralPos).pkl\"\n",
    "inferred_target = model_file_name.replace(\"rf_\", \"\").replace(\".pkl\", \"\")\n",
    "print(\"Inferred target column:\", inferred_target)\n",
    "\n",
    "# Drop non-feature columns: remove any column that has \"slider\" in its name, unless it's the target.\n",
    "feature_cols = [col for col in df.columns if (col == inferred_target or \"slider\" not in col.lower())]\n",
    "\n",
    "# From the features, remove the target column so that only predictors remain.\n",
    "X = df[feature_cols].drop(columns=[inferred_target])\n",
    "y = df[inferred_target]\n",
    "\n",
    "# Select a subset of samples for SHAP analysis to reduce computation time\n",
    "X_sample = X.sample(n=100, random_state=42)  # Adjust sample size as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2331d7-235a-4d41-9658-38b483282197",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------\n",
    "# 1. Load Model and Data\n",
    "# -----------------\n",
    "model_path = \"folds_transformed_models/fold_4/scaled/MIL4 (sliderNeutralPos)/rf/rf_MIL4 (sliderNeutralPos).pkl\"\n",
    "data_path = \"folds_transformed/fold_4_train_scaled.csv\"\n",
    "\n",
    "# Load the pre-trained classifier (Random Forest)\n",
    "with open(model_path, \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Load the CSV data\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# -----------------\n",
    "# 2. Infer Target and Select Features\n",
    "# -----------------\n",
    "# Extract target variable from the model filename\n",
    "model_file_name = os.path.basename(model_path)\n",
    "inferred_target = model_file_name.replace(\"rf_\", \"\").replace(\".pkl\", \"\")\n",
    "print(\"Inferred target column:\", inferred_target)\n",
    "\n",
    "# Select feature columns, ignoring non-relevant columns\n",
    "feature_cols = [col for col in df.columns if (col == inferred_target or \"slider\" not in col.lower())]\n",
    "X = df[feature_cols].drop(columns=[inferred_target])\n",
    "y = df[inferred_target]\n",
    "\n",
    "# -----------------\n",
    "# 3. SHAP Analysis for Multi-Class Classification\n",
    "# -----------------\n",
    "\n",
    "# Select a subset of 100 samples for analysis to reduce computation time\n",
    "X_sample = X.sample(n=100, random_state=42)\n",
    "\n",
    "# Ensure the model supports predict_proba (required for multi-class SHAP)\n",
    "if hasattr(model, \"predict_proba\"):\n",
    "    explainer = shap.Explainer(model.predict_proba, X_sample)\n",
    "else:\n",
    "    explainer = shap.Explainer(model, X_sample)\n",
    "\n",
    "# Compute SHAP values\n",
    "shap_values = explainer(X_sample)\n",
    "\n",
    "# Print SHAP values shape: (n_samples, n_features, n_classes)\n",
    "print(\"SHAP values shape:\", shap_values.values.shape)\n",
    "\n",
    "# -----------------\n",
    "# 4. Automatically Infer Available Classes\n",
    "# -----------------\n",
    "\n",
    "# Get the number of classes from SHAP values\n",
    "num_classes = shap_values.values.shape[2]  # Third axis represents classes\n",
    "\n",
    "# Print detected class indices\n",
    "available_classes = list(range(num_classes))  # Classes are indexed from 0 to (num_classes-1)\n",
    "print(f\"Detected available classes: {available_classes}\")\n",
    "\n",
    "# -----------------\n",
    "# 5. Multi-Class SHAP Summary in One Plot\n",
    "# -----------------\n",
    "\n",
    "feature_names = list(X_sample.columns)  # Ensure feature names are correctly passed as a list\n",
    "\n",
    "# Aggregate SHAP values across all classes (e.g., mean effect per feature)\n",
    "shap_values_mean = shap_values.values.mean(axis=2)  # Shape: (n_samples, n_features)\n",
    "\n",
    "# Plot the aggregated SHAP values\n",
    "shap.summary_plot(\n",
    "    shap_values_mean,\n",
    "    X_sample,\n",
    "    feature_names=feature_names\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ef2155-03b2-4629-9cfb-4543eab4bba3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# 4. LIME Analysis for Multi-Class Classification\n",
    "# -----------------\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# Initialize the LIME explainer\n",
    "lime_explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    training_data=X.values,\n",
    "    feature_names=X.columns.tolist(),\n",
    "    class_names=[str(cls) for cls in model.classes_],  # Ensure class names are string labels\n",
    "    mode='classification',\n",
    "    verbose=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Choose an instance index to explain\n",
    "i = 0  # Change this index to explain different samples\n",
    "instance = X.iloc[i].values\n",
    "\n",
    "# Get available classes (from model output)\n",
    "num_classes = len(model.classes_)  # Automatically determine number of classes\n",
    "print(f\"Available classes: {model.classes_}\")\n",
    "\n",
    "# Plot LIME explanations for each class\n",
    "for class_idx in range(num_classes):\n",
    "    print(f\"Explaining instance {i} for class {model.classes_[class_idx]}\")\n",
    "\n",
    "    # Explain instance for the specific class\n",
    "    exp = lime_explainer.explain_instance(\n",
    "        instance,\n",
    "        model.predict_proba,\n",
    "        num_features=10,  # Adjust number of features to display\n",
    "        labels=(class_idx,)  # Specify the class index\n",
    "    )\n",
    "\n",
    "    # Plot LIME explanation for this class\n",
    "    fig = exp.as_pyplot_figure(label=class_idx)\n",
    "    plt.title(f\"LIME Explanation for Instance {i} (Class {model.classes_[class_idx]})\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2c65c6-8132-4502-a553-a01e3ee7b6c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# 5. MAPIE for Classification Prediction Sets\n",
    "# -----------------\n",
    "from mapie.classification import MapieClassifier\n",
    "\n",
    "alpha = 0.05  # significance level for prediction sets\n",
    "mapie = MapieClassifier(estimator=model, cv=\"prefit\", method=\"score\")\n",
    "mapie.fit(X, y)  # calibrate MAPIE (this does not re-fit your model)\n",
    "\n",
    "# Obtain prediction sets for each instance in X\n",
    "y_pred, prediction_sets = mapie.predict(X, alpha=alpha)\n",
    "\n",
    "# Calculate the coverage: fraction of instances where the true label is in the prediction set.\n",
    "coverage = np.mean([y.iloc[i] in prediction_sets[i] for i in range(len(y))])\n",
    "print(f\"Prediction set coverage: {coverage:.3f}\")\n",
    "\n",
    "# Inspect the prediction sets for a few examples:\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i}: True label = {y.iloc[i]}, Prediction set = {prediction_sets[i]}\")\n",
    "\n",
    "# Since prediction sets are lists of class labels, you can visualize the distribution of their sizes.\n",
    "set_sizes = [len(pset) for pset in prediction_sets]\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(set_sizes, bins=np.arange(1, max(set_sizes)+2)-0.5, edgecolor='black')\n",
    "plt.xlabel(\"Prediction Set Size\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of MAPIE Prediction Set Sizes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef971ccc-7398-4aa2-b137-25ee0ae06eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
