{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67b79db-3470-46ef-a54d-33081358e6e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 1) IMPORTS & SETUP\n",
    "###############################################################################\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from tensorflow.keras.models import load_model\n",
    "from mapie.classification import MapieClassifier\n",
    "\n",
    "import joblib\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "###############################################################################\n",
    "# 2) WRAPPERS FOR KERAS & SKLEARN MODELS\n",
    "###############################################################################\n",
    "class KerasClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Wraps a pre-trained Keras model (.h5) so that it can be used\n",
    "    by MAPIE's MapieClassifier in \"prefit\" mode.\n",
    "\n",
    "    - .fit(X, y) loads the .h5 model (no re-training).\n",
    "    - .predict_proba(X) returns an (n_samples, n_classes) array.\n",
    "    - .predict(X, **kwargs) returns the predicted class index for each sample.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_path, n_classes=11):\n",
    "        self.model_path = model_path\n",
    "        self.model_ = None\n",
    "        self.classes_ = np.arange(n_classes)  # default 0..10\n",
    "    \n",
    "    def fit(self, X, y, **kwargs):\n",
    "        self.model_ = load_model(self.model_path)\n",
    "        # Optionally: self.classes_ = np.unique(y)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X, **kwargs):\n",
    "        return self.model_.predict(X)\n",
    "\n",
    "    def predict(self, X, **kwargs):\n",
    "        proba = self.predict_proba(X)\n",
    "        max_idx = np.argmax(proba, axis=1)\n",
    "        return self.classes_[max_idx]\n",
    "\n",
    "\n",
    "class SklearnClassifierWrapper(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Wraps a pre-trained scikit-learn classifier so that it can\n",
    "    ignore extra arguments like 'prediction_type' from MAPIE.\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.classes_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # 'Prefit' => no real training, but set self.classes_ if available\n",
    "        if hasattr(self.model, \"classes_\"):\n",
    "            self.classes_ = self.model.classes_\n",
    "        else:\n",
    "            self.classes_ = np.unique(y)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X, **kwargs):\n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "    def predict(self, X, **kwargs):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "###############################################################################\n",
    "# 3) LOAD & SPLIT DATA\n",
    "###############################################################################\n",
    "def load_and_split_data(csv_path, test_size=0.2, random_state=42):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Identify labels\n",
    "    label_cols = [c for c in df.columns if 'slider' in c.lower()]\n",
    "\n",
    "    # Identify features (example: columns starting with hrv|eda|acc|ibis)\n",
    "    feature_cols = df.filter(regex='^(hrv|eda|acc|num_ibis)').columns.tolist()\n",
    "\n",
    "    # Group-based split\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    participants = df['participant_id'].values\n",
    "\n",
    "    train_idx, test_idx = next(gss.split(df, groups=participants))\n",
    "\n",
    "    df_cal = df.iloc[train_idx].copy()  # calibration set\n",
    "    df_test = df.iloc[test_idx].copy()  # test set\n",
    "\n",
    "    X_cal = df_cal[feature_cols]\n",
    "    X_test = df_test[feature_cols]\n",
    "\n",
    "    return df_cal, df_test, X_cal, X_test, label_cols\n",
    "\n",
    "###############################################################################\n",
    "# 4) APPLY MAPIE WITH FALLBACK FOR DEGENERATE INTERVALS\n",
    "###############################################################################\n",
    "def apply_mapie_fallback(model, X_cal, y_cal, X_test, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Uses MAPIE in \"prefit\" mode to get classification intervals.\n",
    "    If degenerate single-class intervals appear, fallback to min/max probabilities.\n",
    "    Returns (y_pred, lower_bounds, upper_bounds).\n",
    "    \"\"\"\n",
    "    mapie_clf = MapieClassifier(\n",
    "        estimator=model,\n",
    "        method=\"score\",\n",
    "        cv=\"prefit\",  # do not retrain the underlying model\n",
    "    )\n",
    "    mapie_clf.fit(X_cal, y_cal)  # calibrate\n",
    "\n",
    "    # \"set\" => returns (y_pred, y_pred_sets)\n",
    "    y_pred, y_pis = mapie_clf.predict(X_test, alpha=[alpha], prediction_type=\"set\")\n",
    "\n",
    "    print(\"[MAPIE] y_pred shape:\", y_pred.shape)\n",
    "    print(\"[MAPIE] y_pis shape:\", y_pis.shape)\n",
    "\n",
    "    if y_pis.ndim == 3 and y_pis.shape[2] == 1:\n",
    "        # Degenerate single-class intervals\n",
    "        print(\"Degenerate intervals -> fallback to probabilities.\")\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_prob = model.predict_proba(X_test)\n",
    "            lower_bounds = y_prob.min(axis=1)\n",
    "            upper_bounds = y_prob.max(axis=1)\n",
    "        else:\n",
    "            lower_bounds = np.full(len(X_test), np.nan)\n",
    "            upper_bounds = np.full(len(X_test), np.nan)\n",
    "    else:\n",
    "        # typical multi-class intervals => shape: (n_samples, 1, n_classes)\n",
    "        classes_ = mapie_clf.classes_\n",
    "        class_idx = np.array([np.where(classes_ == c)[0][0] for c in y_pred])\n",
    "        lower_bounds = y_pis[np.arange(len(y_pis)), class_idx, 0]\n",
    "        upper_bounds = y_pis[np.arange(len(y_pis)), class_idx, 1]\n",
    "\n",
    "    return y_pred, lower_bounds, upper_bounds\n",
    "\n",
    "###############################################################################\n",
    "# 5) MAIN EVALUATION LOOP (LSTM)\n",
    "###############################################################################\n",
    "def evaluate_pretrained_models(csv_path, models_base_dir):\n",
    "    \"\"\"\n",
    "    1) Load & group-split data\n",
    "    2) For each label subfolder in 'models_base_dir', look for subfolder:\n",
    "         - 'lstm' with *.h5\n",
    "    3) Load the model, wrap it, apply MAPIE in prefit mode, save intervals.\n",
    "    \"\"\"\n",
    "    df_cal, df_test, X_cal, X_test, label_cols = load_and_split_data(csv_path)\n",
    "\n",
    "    # We'll handle only LSTM models\n",
    "    model_types = [\"lstm\"]\n",
    "    results = []\n",
    "\n",
    "    labels_found = [\n",
    "        d for d in os.listdir(models_base_dir)\n",
    "        if os.path.isdir(os.path.join(models_base_dir, d))\n",
    "    ]\n",
    "\n",
    "    for label_name in labels_found:\n",
    "        if label_name not in label_cols:\n",
    "            print(f\"[SKIP] {label_name} not found in CSV label columns.\")\n",
    "            continue\n",
    "\n",
    "        # define y_cal, y_test for this label\n",
    "        y_cal = df_cal[label_name].astype(int)\n",
    "        y_test = df_test[label_name].astype(int)\n",
    "\n",
    "        # For each model type, check subfolder\n",
    "        for mtype in model_types:\n",
    "            subfolder_path = os.path.join(models_base_dir, label_name, mtype)\n",
    "            if not os.path.isdir(subfolder_path):\n",
    "                print(f\"[SKIP] No '{mtype}' subfolder for label {label_name}\")\n",
    "                continue\n",
    "\n",
    "            # find model files (assuming LSTM models are saved as .h5 files)\n",
    "            model_files = [f for f in os.listdir(subfolder_path)\n",
    "                           if f.endswith(\".h5\")]\n",
    "\n",
    "            if not model_files:\n",
    "                print(f\"[SKIP] No {mtype} model file found in {subfolder_path}\")\n",
    "                continue\n",
    "\n",
    "            # take the first model file\n",
    "            model_file = model_files[0]\n",
    "            full_model_path = os.path.join(subfolder_path, model_file)\n",
    "            print(f\"\\n=== Processing Label: {label_name}, Model: {mtype}, File: {model_file} ===\")\n",
    "\n",
    "            # Load & wrap the LSTM model (using KerasClassifier)\n",
    "            wrapped_model = KerasClassifier(model_path=full_model_path, n_classes=11)\n",
    "            wrapped_model.fit(X_cal, y_cal)\n",
    "\n",
    "            # Apply MAPIE\n",
    "            y_pred, lower, upper = apply_mapie_fallback(wrapped_model, X_cal, y_cal, X_test, alpha=0.1)\n",
    "\n",
    "            # Save intervals\n",
    "            out_dir = os.path.join(\"results_mapie\", label_name)\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "            intervals_file = os.path.join(out_dir, f\"{mtype}_intervals_{label_name}.csv\")\n",
    "\n",
    "            intervals_df = pd.DataFrame({\n",
    "                \"Index\": np.arange(len(X_test)),\n",
    "                \"y_test\": y_test,\n",
    "                \"y_pred\": y_pred,\n",
    "                \"Lower_Bound\": lower,\n",
    "                \"Upper_Bound\": upper,\n",
    "            })\n",
    "            intervals_df.to_csv(intervals_file, index=False)\n",
    "            print(f\"[INFO] Intervals saved to {intervals_file}\")\n",
    "\n",
    "            results.append({\n",
    "                \"label\": label_name,\n",
    "                \"model_type\": mtype,\n",
    "                \"model_file\": model_file,\n",
    "                \"interval_csv\": intervals_file,\n",
    "            })\n",
    "\n",
    "    # Summarize all\n",
    "    df_results = pd.DataFrame(results)\n",
    "    summary_path = \"results_mapie/all_results.csv\"\n",
    "    os.makedirs(os.path.dirname(summary_path), exist_ok=True)\n",
    "    df_results.to_csv(summary_path, index=False)\n",
    "    print(f\"\\n=== All results saved to {summary_path} ===\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 6) RUN IT\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    # CSV path for your entire dataset\n",
    "    CSV_PATH = \"cleaned_dataset_0307.csv\"\n",
    "\n",
    "    # Base directory where your trained models are stored\n",
    "    MODELS_BASE_DIR = \"folds_original_models/fold_4/models\"\n",
    "\n",
    "    evaluate_pretrained_models(CSV_PATH, MODELS_BASE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25597637-90d5-48e9-868c-15a7153cc0eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 1) IMPORTS & SETUP\n",
    "###############################################################################\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from mapie.classification import MapieClassifier\n",
    "\n",
    "import joblib\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "###############################################################################\n",
    "# 2) WRAPPERS FOR MODELS\n",
    "###############################################################################\n",
    "class SklearnClassifierWrapper(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Wraps a pre-trained scikit-learn classifier so that it can\n",
    "    ignore extra arguments like 'prediction_type' from MAPIE.\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.classes_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # 'Prefit' => no re-training, but set self.classes_ if available\n",
    "        if hasattr(self.model, \"classes_\"):\n",
    "            self.classes_ = self.model.classes_\n",
    "        else:\n",
    "            self.classes_ = np.unique(y)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X, **kwargs):\n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "    def predict(self, X, **kwargs):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "###############################################################################\n",
    "# 3) LOAD & SPLIT DATA\n",
    "###############################################################################\n",
    "def load_and_split_data(csv_path, test_size=0.2, random_state=42):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Identify labels (e.g. columns containing the word \"slider\")\n",
    "    label_cols = [c for c in df.columns if 'slider' in c.lower()]\n",
    "\n",
    "    # Identify features (example: columns starting with hrv|eda|acc|num_ibis)\n",
    "    feature_cols = df.filter(regex='^(hrv|eda|acc|num_ibis)').columns.tolist()\n",
    "\n",
    "    # Group-based split based on participant_id\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    participants = df['participant_id'].values\n",
    "\n",
    "    train_idx, test_idx = next(gss.split(df, groups=participants))\n",
    "\n",
    "    df_cal = df.iloc[train_idx].copy()  # calibration set\n",
    "    df_test = df.iloc[test_idx].copy()   # test set\n",
    "\n",
    "    X_cal = df_cal[feature_cols]\n",
    "    X_test = df_test[feature_cols]\n",
    "\n",
    "    return df_cal, df_test, X_cal, X_test, label_cols\n",
    "\n",
    "###############################################################################\n",
    "# 4) APPLY MAPIE WITH FALLBACK FOR DEGENERATE INTERVALS\n",
    "###############################################################################\n",
    "def apply_mapie_fallback(model, X_cal, y_cal, X_test, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Uses MAPIE in \"prefit\" mode to get classification intervals.\n",
    "    If degenerate single-class intervals appear, fallback to min/max probabilities.\n",
    "    Returns (y_pred, lower_bounds, upper_bounds).\n",
    "    \"\"\"\n",
    "    mapie_clf = MapieClassifier(\n",
    "        estimator=model,\n",
    "        method=\"score\",\n",
    "        cv=\"prefit\",  # do not retrain the underlying model\n",
    "    )\n",
    "    mapie_clf.fit(X_cal, y_cal)  # calibrate\n",
    "\n",
    "    # \"set\" => returns (y_pred, y_pred_sets)\n",
    "    y_pred, y_pis = mapie_clf.predict(X_test, alpha=[alpha], prediction_type=\"set\")\n",
    "\n",
    "    print(\"[MAPIE] y_pred shape:\", y_pred.shape)\n",
    "    print(\"[MAPIE] y_pis shape:\", y_pis.shape)\n",
    "\n",
    "    if y_pis.ndim == 3 and y_pis.shape[2] == 1:\n",
    "        # Degenerate single-class intervals -> fallback to probabilities.\n",
    "        print(\"Degenerate intervals -> fallback to probabilities.\")\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_prob = model.predict_proba(X_test)\n",
    "            lower_bounds = y_prob.min(axis=1)\n",
    "            upper_bounds = y_prob.max(axis=1)\n",
    "        else:\n",
    "            lower_bounds = np.full(len(X_test), np.nan)\n",
    "            upper_bounds = np.full(len(X_test), np.nan)\n",
    "    else:\n",
    "        # Typical multi-class intervals => shape: (n_samples, 1, n_classes)\n",
    "        classes_ = mapie_clf.classes_\n",
    "        class_idx = np.array([np.where(classes_ == c)[0][0] for c in y_pred])\n",
    "        lower_bounds = y_pis[np.arange(len(y_pis)), class_idx, 0]\n",
    "        upper_bounds = y_pis[np.arange(len(y_pis)), class_idx, 1]\n",
    "\n",
    "    return y_pred, lower_bounds, upper_bounds\n",
    "\n",
    "###############################################################################\n",
    "# 5) MAIN EVALUATION LOOP (LGBM & XGBOOST)\n",
    "###############################################################################\n",
    "def evaluate_pretrained_models(csv_path, models_base_dir):\n",
    "    \"\"\"\n",
    "    1) Load & group-split data.\n",
    "    2) For each label subfolder in 'models_base_dir', look for subfolders:\n",
    "         - 'xgb' with *.pkl files.\n",
    "         - 'lgbm' with *.pkl files.\n",
    "    3) Load the model, wrap it, apply MAPIE in prefit mode, and save intervals.\n",
    "    \"\"\"\n",
    "    df_cal, df_test, X_cal, X_test, label_cols = load_and_split_data(csv_path)\n",
    "\n",
    "    # We'll handle only LGBM and XGBoost models.\n",
    "    model_types = [\"lgbm\", \"xgb\"]\n",
    "    results = []\n",
    "\n",
    "    labels_found = [\n",
    "        d for d in os.listdir(models_base_dir)\n",
    "        if os.path.isdir(os.path.join(models_base_dir, d))\n",
    "    ]\n",
    "\n",
    "    for label_name in labels_found:\n",
    "        if label_name not in label_cols:\n",
    "            print(f\"[SKIP] {label_name} not found in CSV label columns.\")\n",
    "            continue\n",
    "\n",
    "        # define y_cal, y_test for this label\n",
    "        y_cal = df_cal[label_name].astype(int)\n",
    "        y_test = df_test[label_name].astype(int)\n",
    "\n",
    "        # For each model type (lgbm and xgb), check for corresponding subfolder\n",
    "        for mtype in model_types:\n",
    "            subfolder_path = os.path.join(models_base_dir, label_name, mtype)\n",
    "            if not os.path.isdir(subfolder_path):\n",
    "                print(f\"[SKIP] No '{mtype}' subfolder for label {label_name}\")\n",
    "                continue\n",
    "\n",
    "            # find model files (assuming .pkl extension)\n",
    "            model_files = [f for f in os.listdir(subfolder_path)\n",
    "                           if f.endswith(\".pkl\")]\n",
    "\n",
    "            if not model_files:\n",
    "                print(f\"[SKIP] No {mtype} model file found in {subfolder_path}\")\n",
    "                continue\n",
    "\n",
    "            # take the first model file found\n",
    "            model_file = model_files[0]\n",
    "            full_model_path = os.path.join(subfolder_path, model_file)\n",
    "            print(f\"\\n=== Processing Label: {label_name}, Model: {mtype}, File: {model_file} ===\")\n",
    "\n",
    "            # Load the model via joblib and wrap it with SklearnClassifierWrapper\n",
    "            loaded_model = joblib.load(full_model_path)\n",
    "            wrapped_model = SklearnClassifierWrapper(loaded_model)\n",
    "            wrapped_model.fit(X_cal, y_cal)  # sets self.classes_\n",
    "\n",
    "            # Apply MAPIE\n",
    "            y_pred, lower, upper = apply_mapie_fallback(wrapped_model, X_cal, y_cal, X_test, alpha=0.1)\n",
    "\n",
    "            # Save intervals (results are saved in the same 'results_mapie' folder)\n",
    "            out_dir = os.path.join(\"results_mapie\", label_name)\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "            intervals_file = os.path.join(out_dir, f\"{mtype}_intervals_{label_name}.csv\")\n",
    "\n",
    "            intervals_df = pd.DataFrame({\n",
    "                \"Index\": np.arange(len(X_test)),\n",
    "                \"y_test\": y_test,\n",
    "                \"y_pred\": y_pred,\n",
    "                \"Lower_Bound\": lower,\n",
    "                \"Upper_Bound\": upper,\n",
    "            })\n",
    "            intervals_df.to_csv(intervals_file, index=False)\n",
    "            print(f\"[INFO] Intervals saved to {intervals_file}\")\n",
    "\n",
    "            results.append({\n",
    "                \"label\": label_name,\n",
    "                \"model_type\": mtype,\n",
    "                \"model_file\": model_file,\n",
    "                \"interval_csv\": intervals_file,\n",
    "            })\n",
    "\n",
    "    # Summarize all results\n",
    "    df_results = pd.DataFrame(results)\n",
    "    summary_path = \"results_mapie/all_results.csv\"\n",
    "    os.makedirs(os.path.dirname(summary_path), exist_ok=True)\n",
    "    df_results.to_csv(summary_path, index=False)\n",
    "    print(f\"\\n=== All results saved to {summary_path} ===\")\n",
    "\n",
    "###############################################################################\n",
    "# 6) RUN IT\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    # CSV path for your entire dataset\n",
    "    CSV_PATH = \"cleaned_dataset_0307.csv\"\n",
    "\n",
    "    # Base directory where your trained LGBM & XGBoost models are stored\n",
    "    MODELS_BASE_DIR = \"folds_original_LGBM_XGB_models/fold_4/models\"\n",
    "\n",
    "    evaluate_pretrained_models(CSV_PATH, MODELS_BASE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d586dafc-0f38-4082-a36c-8f1e0f6715a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
