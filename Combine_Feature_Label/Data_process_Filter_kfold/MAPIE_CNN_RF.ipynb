{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cf3236-3dcf-4888-a3f6-c56e2ce303cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 1) IMPORTS & SETUP\n",
    "###############################################################################\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from mapie.classification import MapieClassifier\n",
    "import joblib  # if you also want to load .pkl scikit-learn models\n",
    "\n",
    "###############################################################################\n",
    "# 2) KERAS WRAPPER FOR MAPIE (Classification)\n",
    "###############################################################################\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class KerasClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Wraps a pre-trained Keras model (.h5) so that it can be used\n",
    "    by MAPIE's MapieClassifier in \"prefit\" mode.\n",
    "\n",
    "    - .fit(X, y) loads the .h5 model and sets self.classes_ (no actual re-training).\n",
    "    - .predict_proba(X) returns an (n_samples, n_classes) array of probabilities.\n",
    "    - .predict(X, **kwargs) returns the predicted class index for each sample,\n",
    "      ignoring extra args like 'prediction_type'.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_path, n_classes=11):\n",
    "        self.model_path = model_path\n",
    "        self.model_ = None\n",
    "        self.classes_ = np.arange(n_classes)  # default 0..10\n",
    "    \n",
    "    def fit(self, X, y, **kwargs):\n",
    "        # Load the pre-trained CNN\n",
    "        self.model_ = load_model(self.model_path)\n",
    "        # If you want to set classes_ dynamically based on y:\n",
    "        # self.classes_ = np.unique(y)  # but if you always expect 0..10, keep as is\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X, **kwargs):\n",
    "        proba = self.model_.predict(X)\n",
    "        return proba\n",
    "\n",
    "    def predict(self, X, **kwargs):\n",
    "        proba = self.predict_proba(X)\n",
    "        max_idx = np.argmax(proba, axis=1)\n",
    "        return self.classes_[max_idx]\n",
    "\n",
    "###############################################################################\n",
    "# 3) LOAD & SPLIT DATA\n",
    "###############################################################################\n",
    "def load_and_split_data(csv_path, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    - Load combined_data_imputed.csv\n",
    "    - Group-split by 'participant_id' into calibration and test sets\n",
    "    - Return X_cal, y_cal, X_test, y_test\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Suppose each label is a column name with substring '(sliderNeutralPos)'\n",
    "    # or just 'slider'. We'll find them by your logic:\n",
    "    label_cols = [c for c in df.columns if 'slider' in c.lower()]\n",
    "\n",
    "    # Identify features, for instance, everything but participant_id and label columns\n",
    "    # Or filter by a pattern like you did with '^(hrv|eda|acc|ibis)'\n",
    "    # For simplicity, let's do the pattern approach:\n",
    "    feature_cols = df.filter(regex='^(hrv|eda|acc|ibis|num_ibis)').columns.tolist()\n",
    "\n",
    "    # GroupSplit\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    participants = df['participant_id'].values\n",
    "\n",
    "    train_idx, test_idx = next(gss.split(df, groups=participants))\n",
    "\n",
    "    df_cal = df.iloc[train_idx].copy()  # \"calibration\" set for MAPIE\n",
    "    df_test = df.iloc[test_idx].copy()  # \"test\" set\n",
    "\n",
    "    X_cal = df_cal[feature_cols]\n",
    "    X_test = df_test[feature_cols]\n",
    "\n",
    "    # We'll return the entire df_cal, df_test plus feature names so we can\n",
    "    # look up each label as needed.\n",
    "    return df_cal, df_test, X_cal, X_test, label_cols\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 4) APPLY MAPIE WITH FALLBACK FOR DEGENERATE INTERVALS\n",
    "###############################################################################\n",
    "def apply_mapie_fallback(model, X_cal, y_cal, X_test, alpha=0.1):\n",
    "    \"\"\"\n",
    "    - Uses MAPIE in prefit mode to get intervals (or sets) for classification.\n",
    "    - If degenerate single-class intervals appear, fallback to probability-based \"intervals.\"\n",
    "    - Returns (y_pred, lower_bounds, upper_bounds).\n",
    "    \"\"\"\n",
    "    mapie_clf = MapieClassifier(\n",
    "        estimator=model,\n",
    "        method=\"score\",\n",
    "        cv=\"prefit\",   # prefit => do not re-train underlying model\n",
    "    )\n",
    "    # Fit MAPIE (it won't re-train 'model', but it needs calibration data to measure errors)\n",
    "    mapie_clf.fit(X_cal, y_cal)\n",
    "\n",
    "    y_pred, y_pis = mapie_clf.predict(X_test, alpha=[alpha], prediction_type=\"set\")\n",
    "    # shape y_pred: (n_samples,)\n",
    "    # shape y_pis: (n_samples, 1, n_classes) in multi-class scenario\n",
    "\n",
    "    # Let's examine shape\n",
    "    print(\"[MAPIE] y_pred shape:\", y_pred.shape)\n",
    "    print(\"[MAPIE] y_pis shape:\", y_pis.shape)\n",
    "\n",
    "    # Fallback logic\n",
    "    if y_pis.ndim == 3 and y_pis.shape[2] == 1:\n",
    "        # degenerate single-class intervals\n",
    "        print(\"Degenerate intervals -> fallback to probabilities.\")\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_prob = model.predict_proba(X_test)\n",
    "            lower_bounds = y_prob.min(axis=1)\n",
    "            upper_bounds = y_prob.max(axis=1)\n",
    "        else:\n",
    "            lower_bounds = np.full(len(X_test), np.nan)\n",
    "            upper_bounds = np.full(len(X_test), np.nan)\n",
    "    else:\n",
    "        # normal multi-class intervals => extract intervals for predicted class\n",
    "        # Typically shape: (n_samples, 1, n_classes)\n",
    "        classes_ = mapie_clf.classes_\n",
    "        class_idx = np.array([np.where(classes_ == c)[0][0] for c in y_pred])\n",
    "        lower_bounds = y_pis[np.arange(len(y_pis)), class_idx, 0]\n",
    "        upper_bounds = y_pis[np.arange(len(y_pis)), class_idx, 1]\n",
    "\n",
    "    return y_pred, lower_bounds, upper_bounds\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 5) MAIN EVALUATION LOOP\n",
    "###############################################################################\n",
    "def evaluate_pretrained_models(csv_path, models_base_dir):\n",
    "    \"\"\"\n",
    "    1) Load & group-split data\n",
    "    2) For each label subfolder in 'models_base_dir', load the CNN .h5 model.\n",
    "    3) Wrap with KerasClassifier, apply MAPIE in prefit mode using fallback intervals.\n",
    "    4) Save intervals to CSV.\n",
    "    \"\"\"\n",
    "    df_cal, df_test, X_cal, X_test, label_cols = load_and_split_data(csv_path)\n",
    "\n",
    "    # For each label col, we assume there's a subfolder: models_base_dir/<label>/cnn/*.h5\n",
    "    # We'll just demonstrate for the 'cnn' model type. If you have 'lstm' or 'rf', do similarly.\n",
    "    results = []\n",
    "\n",
    "    labels_found = [\n",
    "        d for d in os.listdir(models_base_dir)\n",
    "        if os.path.isdir(os.path.join(models_base_dir, d))\n",
    "    ]\n",
    "\n",
    "    for label_name in labels_found:\n",
    "        # If this label doesn't appear in your data's label_cols, skip\n",
    "        if label_name not in label_cols:\n",
    "            print(f\"[SKIP] {label_name} not found in CSV label columns.\")\n",
    "            continue\n",
    "\n",
    "        y_cal = df_cal[label_name].astype(int)\n",
    "        y_test = df_test[label_name].astype(int)\n",
    "\n",
    "        # Build path to CNN folder\n",
    "        cnn_path = os.path.join(models_base_dir, label_name, \"cnn\")\n",
    "        if not os.path.isdir(cnn_path):\n",
    "            print(f\"[SKIP] No 'cnn' subfolder for label {label_name}\")\n",
    "            continue\n",
    "\n",
    "        # Find the .h5 model file\n",
    "        model_files = [\n",
    "            f for f in os.listdir(cnn_path)\n",
    "            if f.endswith(\".h5\") and os.path.isfile(os.path.join(cnn_path, f))\n",
    "        ]\n",
    "        if not model_files:\n",
    "            print(f\"[SKIP] No .h5 model file found in {cnn_path} for label {label_name}\")\n",
    "            continue\n",
    "\n",
    "        # Just take the first .h5 we find\n",
    "        h5_file = model_files[0]\n",
    "        full_h5_path = os.path.join(cnn_path, h5_file)\n",
    "\n",
    "        print(f\"\\n=== Processing Label: {label_name}, Model: cnn, File: {h5_file} ===\")\n",
    "        # Wrap in KerasClassifier\n",
    "        wrapped_cnn = KerasClassifier(model_path=full_h5_path, n_classes=11)\n",
    "        # 'Fit' the wrapper => loads the .h5\n",
    "        wrapped_cnn.fit(X_cal, y_cal)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Apply MAPIE\n",
    "        y_pred, lower, upper = apply_mapie_fallback(wrapped_cnn, X_cal, y_cal, X_test, alpha=0.1)\n",
    "        # Save intervals to a CSV\n",
    "        out_dir = os.path.join(\"results_mapie\", label_name)\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        intervals_file = os.path.join(out_dir, f\"cnn_intervals_{label_name}.csv\")\n",
    "\n",
    "        intervals_df = pd.DataFrame({\n",
    "            \"Index\": np.arange(len(X_test)),\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred,\n",
    "            \"Lower_Bound\": lower,\n",
    "            \"Upper_Bound\": upper,\n",
    "        })\n",
    "        intervals_df.to_csv(intervals_file, index=False)\n",
    "        print(f\"[INFO] Intervals saved to {intervals_file}\")\n",
    "\n",
    "        # Optionally store coverage or other metrics\n",
    "        # (You can define your custom coverage measure if you want)\n",
    "        results.append({\n",
    "            \"label\": label_name,\n",
    "            \"model_type\": \"cnn\",\n",
    "            \"model_file\": h5_file,\n",
    "            \"interval_csv\": intervals_file,\n",
    "        })\n",
    "\n",
    "    # Summarize\n",
    "    df_results = pd.DataFrame(results)\n",
    "    summary_path = \"results_mapie/all_results.csv\"\n",
    "    df_results.to_csv(summary_path, index=False)\n",
    "    print(f\"\\n=== All results saved to {summary_path} ===\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 6) RUN IT\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    # CSV path for your entire dataset\n",
    "    CSV_PATH = \"cleaned_dataset_0307.csv\"\n",
    "\n",
    "    # Base directory where your trained models are stored\n",
    "    MODELS_BASE_DIR = \"folds_original_models/fold_4/models\"\n",
    "\n",
    "    evaluate_pretrained_models(CSV_PATH, MODELS_BASE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295af7b0-ec20-448d-97f8-023ad9c04267",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f52a87-6810-42b5-b74e-04c4163f1893",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 1) IMPORTS & SETUP\n",
    "###############################################################################\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from tensorflow.keras.models import load_model\n",
    "from mapie.classification import MapieClassifier\n",
    "\n",
    "import joblib\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "###############################################################################\n",
    "# 2) WRAPPERS FOR KERAS & SKLEARN MODELS\n",
    "###############################################################################\n",
    "class KerasClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Wraps a pre-trained Keras model (.h5) so that it can be used\n",
    "    by MAPIE's MapieClassifier in \"prefit\" mode.\n",
    "\n",
    "    - .fit(X, y) loads the .h5 model (no re-training).\n",
    "    - .predict_proba(X) returns an (n_samples, n_classes) array.\n",
    "    - .predict(X, **kwargs) returns the predicted class index for each sample.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_path, n_classes=11):\n",
    "        self.model_path = model_path\n",
    "        self.model_ = None\n",
    "        self.classes_ = np.arange(n_classes)  # default 0..10\n",
    "    \n",
    "    def fit(self, X, y, **kwargs):\n",
    "        self.model_ = load_model(self.model_path)\n",
    "        # Optionally: self.classes_ = np.unique(y)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X, **kwargs):\n",
    "        return self.model_.predict(X)\n",
    "\n",
    "    def predict(self, X, **kwargs):\n",
    "        proba = self.predict_proba(X)\n",
    "        max_idx = np.argmax(proba, axis=1)\n",
    "        return self.classes_[max_idx]\n",
    "\n",
    "\n",
    "class SklearnClassifierWrapper(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Wraps a pre-trained scikit-learn classifier so that it can\n",
    "    ignore extra arguments like 'prediction_type' from MAPIE.\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.classes_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # 'Prefit' => no real training, but set self.classes_ if available\n",
    "        if hasattr(self.model, \"classes_\"):\n",
    "            self.classes_ = self.model.classes_\n",
    "        else:\n",
    "            self.classes_ = np.unique(y)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X, **kwargs):\n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "    def predict(self, X, **kwargs):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "###############################################################################\n",
    "# 3) LOAD & SPLIT DATA\n",
    "###############################################################################\n",
    "def load_and_split_data(csv_path, test_size=0.2, random_state=42):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Identify labels\n",
    "    label_cols = [c for c in df.columns if 'slider' in c.lower()]\n",
    "\n",
    "    # Identify features (example: columns starting with hrv|eda|acc|ibis)\n",
    "    feature_cols = df.filter(regex='^(hrv|eda|acc|num_ibis)').columns.tolist()\n",
    "\n",
    "    # Group-based split\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    participants = df['participant_id'].values\n",
    "\n",
    "    train_idx, test_idx = next(gss.split(df, groups=participants))\n",
    "\n",
    "    df_cal = df.iloc[train_idx].copy()  # calibration set\n",
    "    df_test = df.iloc[test_idx].copy()  # test set\n",
    "\n",
    "    X_cal = df_cal[feature_cols]\n",
    "    X_test = df_test[feature_cols]\n",
    "\n",
    "    return df_cal, df_test, X_cal, X_test, label_cols\n",
    "\n",
    "###############################################################################\n",
    "# 4) APPLY MAPIE WITH FALLBACK FOR DEGENERATE INTERVALS\n",
    "###############################################################################\n",
    "def apply_mapie_fallback(model, X_cal, y_cal, X_test, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Uses MAPIE in \"prefit\" mode to get classification intervals.\n",
    "    If degenerate single-class intervals appear, fallback to min/max probabilities.\n",
    "    Returns (y_pred, lower_bounds, upper_bounds).\n",
    "    \"\"\"\n",
    "    mapie_clf = MapieClassifier(\n",
    "        estimator=model,\n",
    "        method=\"score\",\n",
    "        cv=\"prefit\",  # do not retrain the underlying model\n",
    "    )\n",
    "    mapie_clf.fit(X_cal, y_cal)  # calibrate\n",
    "\n",
    "    # \"set\" => returns (y_pred, y_pred_sets)\n",
    "    y_pred, y_pis = mapie_clf.predict(X_test, alpha=[alpha], prediction_type=\"set\")\n",
    "\n",
    "    print(\"[MAPIE] y_pred shape:\", y_pred.shape)\n",
    "    print(\"[MAPIE] y_pis shape:\", y_pis.shape)\n",
    "\n",
    "    if y_pis.ndim == 3 and y_pis.shape[2] == 1:\n",
    "        # Degenerate single-class intervals\n",
    "        print(\"Degenerate intervals -> fallback to probabilities.\")\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_prob = model.predict_proba(X_test)\n",
    "            lower_bounds = y_prob.min(axis=1)\n",
    "            upper_bounds = y_prob.max(axis=1)\n",
    "        else:\n",
    "            lower_bounds = np.full(len(X_test), np.nan)\n",
    "            upper_bounds = np.full(len(X_test), np.nan)\n",
    "    else:\n",
    "        # typical multi-class intervals => shape: (n_samples, 1, n_classes)\n",
    "        classes_ = mapie_clf.classes_\n",
    "        class_idx = np.array([np.where(classes_ == c)[0][0] for c in y_pred])\n",
    "        lower_bounds = y_pis[np.arange(len(y_pis)), class_idx, 0]\n",
    "        upper_bounds = y_pis[np.arange(len(y_pis)), class_idx, 1]\n",
    "\n",
    "    return y_pred, lower_bounds, upper_bounds\n",
    "\n",
    "###############################################################################\n",
    "# 5) MAIN EVALUATION LOOP (CNN & RF)\n",
    "###############################################################################\n",
    "def evaluate_pretrained_models(csv_path, models_base_dir):\n",
    "    \"\"\"\n",
    "    1) Load & group-split data\n",
    "    2) For each label subfolder in 'models_base_dir', look for subfolders:\n",
    "         - 'cnn' with *.h5\n",
    "         - 'random_forest' with *.pkl\n",
    "    3) Load the model, wrap it, apply MAPIE in prefit mode, save intervals.\n",
    "    \"\"\"\n",
    "    df_cal, df_test, X_cal, X_test, label_cols = load_and_split_data(csv_path)\n",
    "\n",
    "    # We'll handle CNN and RandomForest\n",
    "    model_types = [\"cnn\", \"random_forest\"]\n",
    "    results = []\n",
    "\n",
    "    labels_found = [\n",
    "        d for d in os.listdir(models_base_dir)\n",
    "        if os.path.isdir(os.path.join(models_base_dir, d))\n",
    "    ]\n",
    "\n",
    "    for label_name in labels_found:\n",
    "        if label_name not in label_cols:\n",
    "            print(f\"[SKIP] {label_name} not found in CSV label columns.\")\n",
    "            continue\n",
    "\n",
    "        # define y_cal, y_test for this label\n",
    "        y_cal = df_cal[label_name].astype(int)\n",
    "        y_test = df_test[label_name].astype(int)\n",
    "\n",
    "        # For each model type, check subfolder\n",
    "        for mtype in model_types:\n",
    "            subfolder_path = os.path.join(models_base_dir, label_name, mtype)\n",
    "            if not os.path.isdir(subfolder_path):\n",
    "                print(f\"[SKIP] No '{mtype}' subfolder for label {label_name}\")\n",
    "                continue\n",
    "\n",
    "            # find model files\n",
    "            if mtype == \"cnn\":\n",
    "                model_files = [f for f in os.listdir(subfolder_path)\n",
    "                               if f.endswith(\".h5\")]\n",
    "            else:  # random_forest\n",
    "                model_files = [f for f in os.listdir(subfolder_path)\n",
    "                               if f.endswith(\".pkl\")]\n",
    "\n",
    "            if not model_files:\n",
    "                print(f\"[SKIP] No {mtype} model file found in {subfolder_path}\")\n",
    "                continue\n",
    "\n",
    "            # take the first model file\n",
    "            model_file = model_files[0]\n",
    "            full_model_path = os.path.join(subfolder_path, model_file)\n",
    "            print(f\"\\n=== Processing Label: {label_name}, Model: {mtype}, File: {model_file} ===\")\n",
    "\n",
    "            # Load & wrap the model\n",
    "            if mtype == \"cnn\":\n",
    "                # Keras\n",
    "                wrapped_model = KerasClassifier(model_path=full_model_path, n_classes=11)\n",
    "                wrapped_model.fit(X_cal, y_cal)\n",
    "            else:\n",
    "                # Random Forest\n",
    "                rf_model = joblib.load(full_model_path)\n",
    "                wrapped_model = SklearnClassifierWrapper(rf_model)\n",
    "                wrapped_model.fit(X_cal, y_cal)  # sets self.classes_\n",
    "\n",
    "            # Apply MAPIE\n",
    "            y_pred, lower, upper = apply_mapie_fallback(wrapped_model, X_cal, y_cal, X_test, alpha=0.1)\n",
    "\n",
    "            # Save intervals\n",
    "            out_dir = os.path.join(\"results_mapie\", label_name)\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "            intervals_file = os.path.join(out_dir, f\"{mtype}_intervals_{label_name}.csv\")\n",
    "\n",
    "            intervals_df = pd.DataFrame({\n",
    "                \"Index\": np.arange(len(X_test)),\n",
    "                \"y_test\": y_test,\n",
    "                \"y_pred\": y_pred,\n",
    "                \"Lower_Bound\": lower,\n",
    "                \"Upper_Bound\": upper,\n",
    "            })\n",
    "            intervals_df.to_csv(intervals_file, index=False)\n",
    "            print(f\"[INFO] Intervals saved to {intervals_file}\")\n",
    "\n",
    "            results.append({\n",
    "                \"label\": label_name,\n",
    "                \"model_type\": mtype,\n",
    "                \"model_file\": model_file,\n",
    "                \"interval_csv\": intervals_file,\n",
    "            })\n",
    "\n",
    "    # Summarize all\n",
    "    df_results = pd.DataFrame(results)\n",
    "    summary_path = \"results_mapie/all_results.csv\"\n",
    "    os.makedirs(os.path.dirname(summary_path), exist_ok=True)\n",
    "    df_results.to_csv(summary_path, index=False)\n",
    "    print(f\"\\n=== All results saved to {summary_path} ===\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 6) RUN IT\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    # CSV path for your entire dataset\n",
    "    CSV_PATH = \"cleaned_dataset_0307.csv\"\n",
    "\n",
    "    # Base directory where your trained models are stored\n",
    "    MODELS_BASE_DIR = \"folds_original_models/fold_4/models\"\n",
    "\n",
    "    evaluate_pretrained_models(CSV_PATH, MODELS_BASE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de7a06f-5802-4cc2-bd4d-b9b21c80f3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "###############################################################################\n",
    "# KERAS WRAPPER FOR DIRECT PREDICTION\n",
    "###############################################################################\n",
    "class KerasClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Wraps a pre-trained Keras model (.h5) so that it can be used\n",
    "    for direct prediction (both class labels and probabilities).\n",
    "    \"\"\"\n",
    "    def __init__(self, model_path, n_classes=11):\n",
    "        self.model_path = model_path\n",
    "        self.model_ = None\n",
    "        self.classes_ = np.arange(n_classes)  # e.g. classes 0, 1, ..., 10\n",
    "    \n",
    "    def fit(self, X, y, **kwargs):\n",
    "        # Load the pre-trained model (this is our \"fit\" step)\n",
    "        self.model_ = load_model(self.model_path)\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X, **kwargs):\n",
    "        # Returns predicted probabilities for each class\n",
    "        return self.model_.predict(X)\n",
    "    \n",
    "    def predict(self, X, **kwargs):\n",
    "        # Returns the class with highest predicted probability for each sample\n",
    "        proba = self.predict_proba(X)\n",
    "        max_idx = np.argmax(proba, axis=1)\n",
    "        return self.classes_[max_idx]\n",
    "\n",
    "###############################################################################\n",
    "# FUNCTION TO LOAD AND SPLIT DATA\n",
    "###############################################################################\n",
    "def load_and_split_data(csv_path, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Load the CSV data, split into calibration (train) and test sets using\n",
    "    GroupShuffleSplit based on 'participant_id', and select feature columns.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Define feature columns: here we select columns starting with specific patterns.\n",
    "    feature_cols = df.filter(regex='^(hrv|eda|acc|ibis|num_ibis)').columns.tolist()\n",
    "\n",
    "    # Group split based on 'participant_id'\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    participants = df['participant_id'].values\n",
    "    train_idx, test_idx = next(gss.split(df, groups=participants))\n",
    "\n",
    "    X_cal = df.iloc[train_idx][feature_cols]\n",
    "    X_test = df.iloc[test_idx][feature_cols]\n",
    "\n",
    "    return df, X_cal, X_test, train_idx, test_idx\n",
    "\n",
    "###############################################################################\n",
    "# MAIN SCRIPT: LOAD MODEL, PREDICT, AND SHOW RESULTS\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths to CSV and models directory (adjust as needed)\n",
    "    CSV_PATH = \"cleaned_dataset_0307.csv\"\n",
    "    MODELS_BASE_DIR = \"folds_original_models/fold_4/models\"\n",
    "\n",
    "    # Load data and get feature splits\n",
    "    df, X_cal, X_test, train_idx, test_idx = load_and_split_data(CSV_PATH)\n",
    "    \n",
    "    # Find label columns (those containing \"slider\" in their name)\n",
    "    label_cols = [c for c in df.columns if 'slider' in c.lower()]\n",
    "    if len(label_cols) == 0:\n",
    "        raise ValueError(\"No label columns found in the CSV.\")\n",
    "    \n",
    "    # For this example, use the first found label column\n",
    "    label_name = label_cols[0]\n",
    "    print(\"Using label column:\", label_name)\n",
    "    \n",
    "    # Get true labels for calibration and test sets\n",
    "    y_cal = df.iloc[train_idx][label_name].astype(int)\n",
    "    y_test = df.iloc[test_idx][label_name].astype(int)\n",
    "    \n",
    "    # Build the expected CNN model directory for the chosen label.\n",
    "    cnn_dir = os.path.join(MODELS_BASE_DIR, label_name, \"cnn\")\n",
    "    if not os.path.isdir(cnn_dir):\n",
    "        raise FileNotFoundError(f\"Directory {cnn_dir} does not exist.\")\n",
    "    \n",
    "    # Find the first .h5 model file in the CNN directory.\n",
    "    model_files = [f for f in os.listdir(cnn_dir) if f.endswith(\".h5\")]\n",
    "    if not model_files:\n",
    "        raise FileNotFoundError(\"No .h5 model found in \" + cnn_dir)\n",
    "    \n",
    "    h5_file = model_files[0]\n",
    "    model_path = os.path.join(cnn_dir, h5_file)\n",
    "    print(\"Loading model from:\", model_path)\n",
    "    \n",
    "    # Instantiate the KerasClassifier and load the pre-trained model\n",
    "    model_wrapper = KerasClassifier(model_path=model_path, n_classes=11)\n",
    "    model_wrapper.fit(X_cal, y_cal)\n",
    "    \n",
    "    # Direct prediction on the test set\n",
    "    y_pred = model_wrapper.predict(X_test)\n",
    "    y_proba = model_wrapper.predict_proba(X_test)\n",
    "    \n",
    "    # Display direct prediction results\n",
    "    print(\"\\n=== Direct Prediction Results ===\")\n",
    "    print(\"Predicted class indices for test set:\")\n",
    "    print(y_pred)\n",
    "    \n",
    "    print(\"\\nPredicted probabilities for the first 5 test samples:\")\n",
    "    print(y_proba[:5])\n",
    "    \n",
    "    # Optionally, compare true vs predicted labels for the test set\n",
    "    comparison = pd.DataFrame({\n",
    "        \"y_test\": y_test,\n",
    "        \"y_pred\": y_pred\n",
    "    })\n",
    "    print(\"\\nComparison of true and predicted labels (first 10 samples):\")\n",
    "    print(comparison.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb88026-e9e3-4830-a352-2dcbd72f0b7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "###############################################################################\n",
    "# KERAS WRAPPER FOR DIRECT PREDICTION\n",
    "###############################################################################\n",
    "class KerasClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Wraps a pre-trained Keras model (.h5) so that it can be used\n",
    "    for direct prediction (both class labels and probabilities).\n",
    "    \"\"\"\n",
    "    def __init__(self, model_path, n_classes=11):\n",
    "        self.model_path = model_path\n",
    "        self.model_ = None\n",
    "        self.classes_ = np.arange(n_classes)  # e.g. classes 0, 1, ..., 10\n",
    "    \n",
    "    def fit(self, X, y, **kwargs):\n",
    "        # Load the pre-trained model (this is our \"fit\" step)\n",
    "        self.model_ = load_model(self.model_path)\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X, **kwargs):\n",
    "        # Returns predicted probabilities for each class\n",
    "        return self.model_.predict(X)\n",
    "    \n",
    "    def predict(self, X, **kwargs):\n",
    "        # Returns the class with highest predicted probability for each sample\n",
    "        proba = self.predict_proba(X)\n",
    "        max_idx = np.argmax(proba, axis=1)\n",
    "        return self.classes_[max_idx]\n",
    "\n",
    "###############################################################################\n",
    "# FUNCTION TO LOAD AND SPLIT DATA\n",
    "###############################################################################\n",
    "def load_and_split_data(csv_path, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Load the CSV data, split into calibration (train) and test sets using\n",
    "    GroupShuffleSplit based on 'participant_id', and select feature columns.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Define feature columns: select columns starting with specific patterns.\n",
    "    feature_cols = df.filter(regex='^(hrv|eda|acc|ibis|num_ibis)').columns.tolist()\n",
    "\n",
    "    # Group split based on 'participant_id'\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    participants = df['participant_id'].values\n",
    "    train_idx, test_idx = next(gss.split(df, groups=participants))\n",
    "\n",
    "    X_cal = df.iloc[train_idx][feature_cols]\n",
    "    X_test = df.iloc[test_idx][feature_cols]\n",
    "\n",
    "    return df, X_cal, X_test, train_idx, test_idx\n",
    "\n",
    "###############################################################################\n",
    "# DEBUGGING FUNCTION (Adjusted for NumPy arrays)\n",
    "###############################################################################\n",
    "def debug_model(model_wrapper, X_cal, X_test, y_cal, y_test):\n",
    "    # Print shapes and basic statistics\n",
    "    print(\"=== Data Information ===\")\n",
    "    print(\"Calibration set shape:\", X_cal.shape)\n",
    "    print(\"Test set shape:\", X_test.shape)\n",
    "    print(\"\\nCalibration set (first 5 rows):\")\n",
    "    print(X_cal[:5])\n",
    "    print(\"\\nTest set (first 5 rows):\")\n",
    "    print(X_test[:5])\n",
    "\n",
    "    # Display the model summary\n",
    "    print(\"\\n=== Model Summary ===\")\n",
    "    model_wrapper.model_.summary()\n",
    "\n",
    "    # Predict on calibration and test sets\n",
    "    y_pred_cal = model_wrapper.predict(X_cal)\n",
    "    y_pred_test = model_wrapper.predict(X_test)\n",
    "\n",
    "    print(\"\\n=== Predictions on Calibration Set ===\")\n",
    "    df_cal_pred = pd.DataFrame({\"y_cal\": y_cal, \"y_pred_cal\": y_pred_cal})\n",
    "    print(df_cal_pred.head())\n",
    "\n",
    "    print(\"\\n=== Predictions on Test Set ===\")\n",
    "    df_test_pred = pd.DataFrame({\"y_test\": y_test, \"y_pred_test\": y_pred_test})\n",
    "    print(df_test_pred.head())\n",
    "\n",
    "    # Check if the predicted probabilities vary across samples\n",
    "    y_proba_test = model_wrapper.predict_proba(X_test)\n",
    "    print(\"\\n=== Predicted Probabilities (first 5 test samples) ===\")\n",
    "    print(y_proba_test[:5])\n",
    "    \n",
    "    # Print basic stats of predicted probabilities to check for constant outputs\n",
    "    print(\"\\n=== Statistics of Predicted Probabilities ===\")\n",
    "    print(\"Min values across samples:\", y_proba_test.min(axis=0))\n",
    "    print(\"Max values across samples:\", y_proba_test.max(axis=0))\n",
    "    print(\"Mean values across samples:\", y_proba_test.mean(axis=0))\n",
    "\n",
    "###############################################################################\n",
    "# MAIN SCRIPT: LOAD MODEL, PREDICT, AND DEBUG\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths to CSV and models directory (adjust as needed)\n",
    "    CSV_PATH = \"cleaned_dataset_0307.csv\"\n",
    "    MODELS_BASE_DIR = \"folds_original_models/fold_4/models\"\n",
    "\n",
    "    # Load data and get feature splits\n",
    "    df, X_cal_df, X_test_df, train_idx, test_idx = load_and_split_data(CSV_PATH)\n",
    "    \n",
    "    # Find label columns (those containing \"slider\" in their name)\n",
    "    label_cols = [c for c in df.columns if 'slider' in c.lower()]\n",
    "    if len(label_cols) == 0:\n",
    "        raise ValueError(\"No label columns found in the CSV.\")\n",
    "    \n",
    "    # For this example, use the first found label column\n",
    "    label_name = label_cols[0]\n",
    "    print(\"Using label column:\", label_name)\n",
    "    \n",
    "    # Get true labels for calibration and test sets\n",
    "    y_cal = df.iloc[train_idx][label_name].astype(int)\n",
    "    y_test = df.iloc[test_idx][label_name].astype(int)\n",
    "    \n",
    "    # Build the expected CNN model directory for the chosen label.\n",
    "    cnn_dir = os.path.join(MODELS_BASE_DIR, label_name, \"cnn\")\n",
    "    if not os.path.isdir(cnn_dir):\n",
    "        raise FileNotFoundError(f\"Directory {cnn_dir} does not exist.\")\n",
    "    \n",
    "    # Find the first .h5 model file in the CNN directory.\n",
    "    model_files = [f for f in os.listdir(cnn_dir) if f.endswith(\".h5\")]\n",
    "    if not model_files:\n",
    "        raise FileNotFoundError(\"No .h5 model found in \" + cnn_dir)\n",
    "    \n",
    "    h5_file = model_files[0]\n",
    "    model_path = os.path.join(cnn_dir, h5_file)\n",
    "    print(\"Loading model from:\", model_path)\n",
    "    \n",
    "    # Instantiate the KerasClassifier and load the pre-trained model\n",
    "    model_wrapper = KerasClassifier(model_path=model_path, n_classes=11)\n",
    "    model_wrapper.fit(X_cal_df, y_cal)\n",
    "\n",
    "    # --- IMPORTANT: Reshape input data to add a channel dimension ---\n",
    "    # Convert DataFrames to NumPy arrays and reshape:\n",
    "    print(\"\\nBefore reshape, X_test shape:\", X_test_df.shape)\n",
    "    X_cal = X_cal_df.to_numpy().reshape(-1, X_cal_df.shape[1], 1)\n",
    "    X_test = X_test_df.to_numpy().reshape(-1, X_test_df.shape[1], 1)\n",
    "    print(\"After reshape, X_test shape:\", X_test.shape)\n",
    "    \n",
    "    # Direct prediction on the reshaped test set\n",
    "    y_pred = model_wrapper.predict(X_test)\n",
    "    y_proba = model_wrapper.predict_proba(X_test)\n",
    "    \n",
    "    # Display direct prediction results\n",
    "    print(\"\\n=== Direct Prediction Results ===\")\n",
    "    print(\"Predicted class indices for test set:\")\n",
    "    print(y_pred)\n",
    "    \n",
    "    print(\"\\nPredicted probabilities for the first 5 test samples:\")\n",
    "    print(y_proba[:5])\n",
    "    \n",
    "    # Compare true vs predicted labels for the test set\n",
    "    print(\"\\nComparison of true and predicted labels (first 10 samples):\")\n",
    "    comparison = pd.DataFrame({\n",
    "        \"y_test\": y_test,\n",
    "        \"y_pred\": y_pred\n",
    "    })\n",
    "    print(comparison.head(10))\n",
    "    \n",
    "    # Run additional debugging outputs on the reshaped data\n",
    "    debug_model(model_wrapper, X_cal, X_test, y_cal, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015c2566-f1ba-4858-bb17-07c404dac504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
